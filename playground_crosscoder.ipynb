{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/BenWilop/WSG_games/blob/main/playground_WSG_games.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "V887fWJPBYKB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /homes/55/bwilop/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbenwilop\u001b[0m (\u001b[33mbenwilop-rwth-aachen-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv(os.path.join(\"/homes/55/bwilop/wsg/private/\", \"vscode-ssh.env\"))\n",
    "api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "wandb.login(key=api_key)\n",
    "WANDB_ENTITIY = \"benwilop-rwth-aachen-university\"\n",
    "\n",
    "data_folder = \"/homes/55/bwilop/wsg/data/\"\n",
    "experiment_folder = \"/homes/55/bwilop/wsg/experiments/\"\n",
    "crosscoder_folder = experiment_folder + \"tictactoe/crosscoder/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uV6cMqvSvaBY",
    "outputId": "ec9e34fb-4176-487f-e718-9b1a44238dfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "import torch as t\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import datetime\n",
    "\n",
    "# from jaxtyping import Float\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wsg_games.tictactoe.evals import *\n",
    "from wsg_games.tictactoe.data import *\n",
    "from wsg_games.tictactoe.game import *\n",
    "\n",
    "from wsg_games.tictactoe.analysis.analyse_data import *\n",
    "from wsg_games.tictactoe.analysis.visualize_game import *\n",
    "\n",
    "from wsg_games.tictactoe.train.create_models import *\n",
    "from wsg_games.tictactoe.train.save_load_models import *\n",
    "from wsg_games.tictactoe.train.train import *\n",
    "from wsg_games.tictactoe.train.finetune import *\n",
    "from wsg_games.tictactoe.train.pretrain import *\n",
    "\n",
    "DEVICE = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-WZi6lEq1sA"
   },
   "source": [
    "# Load Data & Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FseGnVqe_00c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment_folder:  /homes/55/bwilop/wsg/experiments/\n",
      "project_name:  tictactoe/tictactoe_pretraining5\n",
      "/homes/55/bwilop/wsg/experiments/tictactoe/tictactoe_pretraining5\n",
      "Loading model from /homes/55/bwilop/wsg/experiments/tictactoe/tictactoe_pretraining5/experiment_2_small_weak_2025-05-16-16-35_ayyrg2xq.pkl\n",
      "Moving model to device:  cuda\n",
      "experiment_folder:  /homes/55/bwilop/wsg/experiments/\n",
      "project_name:  tictactoe/tictactoe_pretraining5\n",
      "/homes/55/bwilop/wsg/experiments/tictactoe/tictactoe_pretraining5\n",
      "Loading model from /homes/55/bwilop/wsg/experiments/tictactoe/tictactoe_pretraining5/experiment_2_medium_weak_2025-05-16-16-35_eif67e03.pkl\n",
      "Moving model to device:  cuda\n",
      "experiment_folder:  /homes/55/bwilop/wsg/experiments/\n",
      "project_name:  tictactoe/tictactoe_pretraining5\n",
      "/homes/55/bwilop/wsg/experiments/tictactoe/tictactoe_pretraining5\n",
      "Loading model from /homes/55/bwilop/wsg/experiments/tictactoe/tictactoe_pretraining5/experiment_2_medium_strong_2025-05-16-16-44_omyjjb73.pkl\n",
      "Moving model to device:  cuda\n",
      "Moving model to device:  cuda\n",
      "weak_model\n",
      "weak_loss:  0.9690045714378357\n",
      "strong_loss:  1.2434145212173462\n",
      "strong_baseline_model\n",
      "weak_loss:  0.7446461915969849\n",
      "strong_loss:  1.4311984777450562\n",
      "strong_model\n",
      "weak_loss:  1.4627606868743896\n",
      "strong_loss:  0.7166216373443604\n",
      "finetuned_model\n",
      "weak_loss:  0.8355621099472046\n",
      "strong_loss:  1.0068204402923584\n",
      "Performance Gap Recovered (PGR):  0.5947736901349031\n"
     ]
    }
   ],
   "source": [
    "project_name_pretrain = \"tictactoe/tictactoe_pretraining5\"\n",
    "project_name_finetune = \"tictactoe/tictactoe_finetuning5\"\n",
    "weak_model_size = \"small\"\n",
    "strong_model_size = \"medium\"\n",
    "index = 2\n",
    "\n",
    "# Load data\n",
    "(\n",
    "    tictactoe_train_data,\n",
    "    tictactoe_weak_finetune_data,\n",
    "    tictactoe_val_data,\n",
    "    tictactoe_test_data,\n",
    ") = load_split_data(data_folder + \"tictactoe/\", device=DEVICE, index=index)\n",
    "\n",
    "# Load models\n",
    "weak_model = load_model(\n",
    "    project_name_pretrain,\n",
    "    weak_model_size,\n",
    "    Goal.WEAK_GOAL,\n",
    "    experiment_folder,\n",
    "    device=DEVICE,\n",
    "    index=index,\n",
    ")\n",
    "strong_baseline_model = load_model(\n",
    "    project_name_pretrain,\n",
    "    strong_model_size,\n",
    "    Goal.WEAK_GOAL,\n",
    "    experiment_folder,\n",
    "    device=DEVICE,\n",
    "    index=index,\n",
    ")\n",
    "strong_model = load_model(\n",
    "    project_name_pretrain,\n",
    "    strong_model_size,\n",
    "    Goal.STRONG_GOAL,\n",
    "    experiment_folder,\n",
    "    device=DEVICE,\n",
    "    index=index,\n",
    ")\n",
    "finetuned_model = load_finetuned_model(\n",
    "    project_name_finetune,\n",
    "    weak_model_size,\n",
    "    strong_model_size,\n",
    "    experiment_folder,\n",
    "    DEVICE,\n",
    "    index,\n",
    ")\n",
    "\n",
    "# Print evaluations\n",
    "(\n",
    "    weak_loss,\n",
    "    _,\n",
    ") = quick_evaluation(\"weak_model\", weak_model, tictactoe_test_data)\n",
    "strong_baseline_loss, _ = quick_evaluation(\n",
    "    \"strong_baseline_model\", strong_baseline_model, tictactoe_test_data\n",
    ")\n",
    "quick_evaluation(\"strong_model\", strong_model, tictactoe_test_data)\n",
    "weak_finetuned_loss, _ = quick_evaluation(\n",
    "    \"finetuned_model\", finetuned_model, tictactoe_test_data\n",
    ")\n",
    "print(\n",
    "    \"Performance Gap Recovered (PGR): \",\n",
    "    (weak_loss - weak_finetuned_loss) / (weak_loss - strong_baseline_loss),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dictionary_learning.dictionary_learning import CrossCoder\n",
    "from dictionary_learning.dictionary_learning.trainers.crosscoder import (\n",
    "    CrossCoderTrainer,\n",
    ")\n",
    "from dictionary_learning.dictionary_learning.training import trainSAE\n",
    "from dictionary_learning.dictionary_learning.cache import (\n",
    "    PairedActivationCache,\n",
    "    ActivationCache,\n",
    "    ActivationShard,\n",
    ")\n",
    "import transformer_lens.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(\n",
    "    model,\n",
    "    tokenized_games: Float[Tensor, \"n_games game_length\"],\n",
    "    layer_i: int,\n",
    ") -> t.Tensor:\n",
    "    activation_hook_name = utils.get_act_name(\"resid_post\", layer_i)\n",
    "    model.eval()\n",
    "    _, cache = model.run_with_cache(tokenized_games)\n",
    "    layer_activations = cache[activation_hook_name]\n",
    "    return layer_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.1.hook_resid_post\n"
     ]
    }
   ],
   "source": [
    "activation_hook_name = utils.get_act_name(\"resid_post\", 1)\n",
    "print(activation_hook_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@t.no_grad()\n",
    "def create_data_shards(\n",
    "    games_data: Float[Tensor, \"n_games game_length\"],\n",
    "    model: HookedTransformer,\n",
    "    store_dir: str,\n",
    "    batch_size: int = 64,\n",
    "    shard_size: int = 10**6,\n",
    "    max_total_tokens: int = 10**8,\n",
    "    overwrite: bool = False,\n",
    ") -> None:\n",
    "    dataloader = DataLoader(games_data, batch_size=batch_size)\n",
    "    io: str = \"out\"\n",
    "    submodule_names = [f\"layer_{layer_i}\" for layer_i in range(model.cfg.n_layers)]\n",
    "\n",
    "    activation_cache = [[] for _ in submodule_names]\n",
    "    store_dirs = [\n",
    "        os.path.join(store_dir, f\"{submodule_names[layer_i]}_{io}\")\n",
    "        for layer_i in range(len(submodule_names))\n",
    "    ]\n",
    "    for store_dir in store_dirs:\n",
    "        os.makedirs(store_dir, exist_ok=True)\n",
    "    total_size = 0\n",
    "    current_size = 0\n",
    "    shard_count = 0\n",
    "\n",
    "    # Check if shards already exist\n",
    "    if os.path.exists(os.path.join(store_dirs[0], \"shard_0.memmap\")):\n",
    "        print(f\"Shards already exist in {store_dir}\")\n",
    "        if not overwrite:\n",
    "            print(\"Set overwrite=True to overwrite existing shards.\")\n",
    "            return\n",
    "        else:\n",
    "            print(\"Overwriting existing shards...\")\n",
    "\n",
    "    print(\"Collecting activations...\")\n",
    "    for games in tqdm(dataloader, desc=\"Collecting activations\"):\n",
    "        for layer_i in range(len(submodule_names)):\n",
    "            local_activations = rearrange(\n",
    "                get_activations(model, games, layer_i)\n",
    "            )  # (B x T) x D\n",
    "            activation_cache[layer_i].append(local_activations.cpu())\n",
    "\n",
    "        current_size += activation_cache[0][-1].shape[0]\n",
    "        if current_size > shard_size:\n",
    "            print(f\"Storing shard {shard_count}...\", flush=True)\n",
    "            ActivationCache.collate_store_shards(\n",
    "                store_dirs,\n",
    "                shard_count,\n",
    "                activation_cache,\n",
    "                submodule_names,\n",
    "                shuffle_shards=True,\n",
    "                io=io,\n",
    "                multiprocessing=False,\n",
    "            )\n",
    "            shard_count += 1\n",
    "            total_size += current_size\n",
    "            current_size = 0\n",
    "            activation_cache = [[] for _ in submodule_names]\n",
    "\n",
    "        if total_size > max_total_tokens:\n",
    "            print(\"Max total tokens reached. Stopping collection.\")\n",
    "            break\n",
    "\n",
    "    if current_size > 0:\n",
    "        ActivationCache.collate_store_shards(\n",
    "            store_dirs,\n",
    "            shard_count,\n",
    "            activation_cache,\n",
    "            submodule_names,\n",
    "            shuffle_shards=True,\n",
    "            io=io,\n",
    "            multiprocessing=False,\n",
    "        )\n",
    "\n",
    "    # store configs\n",
    "    for i, store_dir in enumerate(store_dirs):\n",
    "        with open(os.path.join(store_dir, \"config.json\"), \"w\") as f:\n",
    "            json.dump(\n",
    "                {\n",
    "                    \"batch_size\": batch_size,\n",
    "                    \"context_len\": -1,\n",
    "                    \"shard_size\": shard_size,\n",
    "                    \"d_model\": model.cfg.d_model,\n",
    "                    \"shuffle_shards\": True,\n",
    "                    \"io\": io,\n",
    "                    \"total_size\": total_size,\n",
    "                    \"shard_count\": shard_count,\n",
    "                    \"store_tokens\": False,\n",
    "                },\n",
    "                f,\n",
    "            )\n",
    "    ActivationCache.cleanup_multiprocessing()\n",
    "    print(f\"Finished collecting activations. Total size: {total_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations_path(\n",
    "    model_goal: Goal | None,\n",
    "    weak_model_size: str | None,\n",
    "    model_size: str,\n",
    "    index: int,\n",
    "    crosscoder_folder: str,\n",
    "    train_val: str,\n",
    ") -> str:\n",
    "    assert model_goal is None or weak_model_size is None\n",
    "    assert model_goal is not None or weak_model_size is not None\n",
    "    if weak_model_size:\n",
    "        postfix = \"finetuned_through_\" + weak_model_size\n",
    "    elif model_goal in [Goal.WEAK_GOAL, Goal.STRONG_GOAL]:\n",
    "        postfix = str(model_goal)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid activations model goal: {model_goal}\")\n",
    "    return os.path.join(\n",
    "        crosscoder_folder, \"activations\", f\"{index}_{model_size}_{postfix}_\" + train_val\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_activations(\n",
    "    model_goal: Goal | None,\n",
    "    project_name_pretrain: str | None,\n",
    "    weak_model_size: str | None,\n",
    "    project_name_finetune: str | None,\n",
    "    model_size: str,\n",
    "    index: int,\n",
    "    crosscoder_folder: str,\n",
    "    tictactoe_test_data: Float[Tensor, \"n_games game_length\"],\n",
    "    tictactoe_val_data: Float[Tensor, \"n_games game_length\"],\n",
    "    experiment_folder: str,\n",
    ") -> None:\n",
    "    # Either finetuned or pretrained\n",
    "    bool_finetuned_model = (\n",
    "        project_name_finetune is not None and weak_model_size is not None\n",
    "    )\n",
    "    bool_pretrained_model = project_name_pretrain is not None and model_goal is not None\n",
    "    assert int(bool_finetuned_model) + int(bool_pretrained_model) == 1, (\n",
    "        f\"Finetuned XOR pretrained model must be provided.\"\n",
    "    )\n",
    "\n",
    "    # Models\n",
    "    if bool_finetuned_model:\n",
    "        model = load_finetuned_model(\n",
    "            project_name_finetune,\n",
    "            weak_model_size,\n",
    "            model_size,\n",
    "            experiment_folder,\n",
    "            DEVICE,\n",
    "            index,\n",
    "        )\n",
    "    else:\n",
    "        model = load_model(\n",
    "            project_name_pretrain,\n",
    "            model_size,\n",
    "            model_goal,\n",
    "            experiment_folder,\n",
    "            device=DEVICE,\n",
    "            index=index,\n",
    "        )\n",
    "\n",
    "    # Run\n",
    "    for train_val in [\"train\", \"val\"]:\n",
    "        if train_val == \"train\":\n",
    "            games_data = tictactoe_test_data\n",
    "        elif train_val == \"val\":\n",
    "            games_data = tictactoe_val_data\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid train_val: {train_val}\")\n",
    "\n",
    "        activations_path = get_activations_path(\n",
    "            model_goal, weak_model_size, model_size, index, crosscoder_folder, train_val\n",
    "        )\n",
    "        create_data_shards(\n",
    "            games_data,\n",
    "            model,\n",
    "            store_dir=activations_path,\n",
    "            batch_size=64,\n",
    "            shard_size=10**5,\n",
    "            max_total_tokens=10**10,\n",
    "            overwrite=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment_folder:  /homes/55/bwilop/wsg/experiments/\n",
      "project_name:  tictactoe/tictactoe_pretraining5\n",
      "/homes/55/bwilop/wsg/experiments/tictactoe/tictactoe_pretraining5\n",
      "Loading model from /homes/55/bwilop/wsg/experiments/tictactoe/tictactoe_pretraining5/experiment_2_medium_strong_2025-05-16-16-44_omyjjb73.pkl\n",
      "Moving model to device:  cuda\n",
      "Shards already exist in /homes/55/bwilop/wsg/experiments/tictactoe/crosscoder/activations/2_medium_strong_train/layer_3_out\n",
      "Set overwrite=True to overwrite existing shards.\n",
      "Shards already exist in /homes/55/bwilop/wsg/experiments/tictactoe/crosscoder/activations/2_medium_strong_val/layer_3_out\n",
      "Set overwrite=True to overwrite existing shards.\n",
      "Moving model to device:  cuda\n",
      "Shards already exist in /homes/55/bwilop/wsg/experiments/tictactoe/crosscoder/activations/2_medium_finetuned_through_small_train/layer_3_out\n",
      "Set overwrite=True to overwrite existing shards.\n",
      "Shards already exist in /homes/55/bwilop/wsg/experiments/tictactoe/crosscoder/activations/2_medium_finetuned_through_small_val/layer_3_out\n",
      "Set overwrite=True to overwrite existing shards.\n"
     ]
    }
   ],
   "source": [
    "# Strong\n",
    "compute_activations(\n",
    "    Goal.STRONG_GOAL,\n",
    "    project_name_pretrain,\n",
    "    None,\n",
    "    None,\n",
    "    strong_model_size,\n",
    "    index,\n",
    "    crosscoder_folder,\n",
    "    tictactoe_test_data.games_data,\n",
    "    tictactoe_val_data.games_data,\n",
    "    experiment_folder,\n",
    ")\n",
    "\n",
    "# Finetuned\n",
    "compute_activations(\n",
    "    None,\n",
    "    None,\n",
    "    weak_model_size,\n",
    "    project_name_finetune,\n",
    "    strong_model_size,\n",
    "    index,\n",
    "    crosscoder_folder,\n",
    "    tictactoe_test_data.games_data,\n",
    "    tictactoe_val_data.games_data,\n",
    "    experiment_folder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_cfg_cross_coder():\n",
    "    training_cfg_cross_coder = {\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"max_steps\": 1000,\n",
    "        \"validate_every_n_steps\": 100,\n",
    "        \"batch_size\": 64,\n",
    "        \"expansion_factor\": 32,\n",
    "        \"resample_steps\": None,  # int | None\n",
    "        \"mu\": 1e-1,\n",
    "    }\n",
    "    return training_cfg_cross_coder\n",
    "\n",
    "\n",
    "# run_name\n",
    "# wandb-entity\n",
    "# disable-wandb\n",
    "# K=\n",
    "# n_workers\n",
    "# compile = False\n",
    "\n",
    "\n",
    "def train_crosscoder(\n",
    "    model_1_name: str,\n",
    "    model_2_name: str,\n",
    "    index: int,\n",
    "    train_activations_stor_dir_model_1: str,\n",
    "    val_activations_stor_dir_model_1: str,\n",
    "    train_activations_stor_dir_model_2: str,\n",
    "    val_activations_stor_dir_model_2: str,\n",
    "    layer: int,\n",
    "    training_cfg_cross_coder: dict,\n",
    "    wandb_entitiy: str,\n",
    ") -> None:\n",
    "    # Data (not loaded in memory yet)\n",
    "    train_dataset = PairedActivationCache(\n",
    "        train_activations_stor_dir_model_1,\n",
    "        train_activations_stor_dir_model_2,\n",
    "    )\n",
    "    dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=training_cfg_cross_coder[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    print(f\"Training on {len(train_dataset)} token activations.\")\n",
    "    val_dataset = PairedActivationCache(\n",
    "        val_activations_stor_dir_model_1,\n",
    "        val_activations_stor_dir_model_2,\n",
    "    )\n",
    "    validation_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=1000,\n",
    "        shuffle=False,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    print(f\"Validating on {len(val_dataset)} token activations.\")\n",
    "\n",
    "    # Training config\n",
    "    activation_dim = train_dataset[0].shape[1]\n",
    "    dictionary_size = training_cfg_cross_coder[\"expansion_factor\"] * activation_dim\n",
    "    print(f\"Activation dim: {activation_dim}\")\n",
    "    print(f\"Dictionary size: {dictionary_size}\")\n",
    "    mu = training_cfg_cross_coder[\"mu\"]\n",
    "    lr = training_cfg_cross_coder[\"learning_rate\"]\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "    experiment_name = f\"experiment_{index}_{model_1_name}_{model_2_name}_{timestamp}\"\n",
    "    trainer_cfg = {\n",
    "        \"trainer\": CrossCoderTrainer,\n",
    "        \"dict_class\": CrossCoder,\n",
    "        \"activation_dim\": activation_dim,\n",
    "        \"dict_size\": dictionary_size,\n",
    "        \"lr\": lr,\n",
    "        \"resample_steps\": training_cfg_cross_coder[\"resample_steps\"],\n",
    "        \"device\": str(DEVICE),\n",
    "        \"warmup_steps\": 1000,\n",
    "        \"layer\": layer,\n",
    "        \"lm_name\": experiment_name,\n",
    "        \"compile\": True,\n",
    "        \"wandb_name\": experiment_name + f\"L{layer}-mu{mu:.1e}-lr{lr:.0e}\",\n",
    "        \"l1_penalty\": mu,\n",
    "        \"dict_class_kwargs\": {\n",
    "            \"same_init_for_all_layers\": True,\n",
    "            \"norm_init_scale\": 0.005,\n",
    "            \"init_with_transpose\": True,\n",
    "            \"encoder_layers\": None,\n",
    "        },\n",
    "        \"pretrained_ae\": None,\n",
    "    }\n",
    "\n",
    "    # train the sparse autoencoder (SAE)\n",
    "    wandb.finish()\n",
    "    ae = trainSAE(\n",
    "        data=dataloader,\n",
    "        trainer_config=trainer_cfg,\n",
    "        validate_every_n_steps=training_cfg_cross_coder[\"validate_every_n_steps\"],\n",
    "        validation_data=validation_dataloader,\n",
    "        use_wandb=True,\n",
    "        wandb_entity=wandb_entitiy,\n",
    "        wandb_project=\"crosscoder\",\n",
    "        log_steps=50,\n",
    "        save_dir=\"checkpoints\",\n",
    "        steps=training_cfg_cross_coder[\"max_steps\"],\n",
    "        save_steps=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 200704 token activations.\n",
      "Validating on 100352 token activations.\n",
      "Activation dim: 32\n",
      "Dictionary size: 1024\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">experiment_2_strong_model_finetuned_model_2025-05-23-00-35L3-mu1.0e-01-lr1e-03</strong> at: <a href='https://wandb.ai/benwilop-rwth-aachen-university/crosscoder/runs/qwwutnik' target=\"_blank\">https://wandb.ai/benwilop-rwth-aachen-university/crosscoder/runs/qwwutnik</a><br> View project at: <a href='https://wandb.ai/benwilop-rwth-aachen-university/crosscoder' target=\"_blank\">https://wandb.ai/benwilop-rwth-aachen-university/crosscoder</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250523_003503-qwwutnik/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/homes/55/bwilop/wsg/WSG_games/wandb/run-20250523_003719-5osb580j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/benwilop-rwth-aachen-university/crosscoder/runs/5osb580j' target=\"_blank\">experiment_2_strong_model_finetuned_model_2025-05-23-00-37L3-mu1.0e-01-lr1e-03</a></strong> to <a href='https://wandb.ai/benwilop-rwth-aachen-university/crosscoder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/benwilop-rwth-aachen-university/crosscoder' target=\"_blank\">https://wandb.ai/benwilop-rwth-aachen-university/crosscoder</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/benwilop-rwth-aachen-university/crosscoder/runs/5osb580j' target=\"_blank\">https://wandb.ai/benwilop-rwth-aachen-university/crosscoder/runs/5osb580j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/55/bwilop/wsg/WSG_games/dictionary_learning/dictionary_learning/training.py:239: UserWarning: Error saving config: 'DataLoader' object has no attribute 'config'\n",
      "  warn(f\"Error saving config: {e}\")\n",
      "  0%|          | 1/1000 [00:06<1:43:31,  6.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating at step 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:08<00:00, 11.61it/s]\n",
      "  0%|          | 4/1000 [00:14<58:39,  3.53s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating at step 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:07<00:00, 13.28it/s]\n",
      "  1%|          | 7/1000 [00:22<49:40,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating at step 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:08<00:00, 12.54it/s]\n",
      "  1%|          | 10/1000 [00:30<47:18,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating at step 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:07<00:00, 12.78it/s]\n",
      "  1%|▏         | 13/1000 [00:38<45:44,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating at step 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:07<00:00, 12.95it/s]\n",
      "  2%|▏         | 16/1000 [00:46<44:36,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating at step 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:07<00:00, 13.22it/s]\n",
      "  2%|▏         | 19/1000 [00:54<43:33,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating at step 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:07<00:00, 13.12it/s]\n",
      "  2%|▏         | 22/1000 [01:01<42:56,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating at step 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 39/101 [00:02<00:04, 13.28it/s]\n",
      "  2%|▏         | 24/1000 [01:04<43:55,  2.70s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m train_activations_stor_dir_model_2 \u001b[38;5;241m=\u001b[39m get_activations_path(\u001b[38;5;28;01mNone\u001b[39;00m, weak_model_size, strong_model_size, index, crosscoder_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m training_cfg_cross_coder \u001b[38;5;241m=\u001b[39m get_training_cfg_cross_coder()\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrain_crosscoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_1_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmodel_2_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtrain_activations_stor_dir_model_1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/layer_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlayer\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_out\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                \u001b[49m\u001b[43mval_activations_stor_dir_model_1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/layer_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlayer\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_out\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtrain_activations_stor_dir_model_2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/layer_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlayer\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_out\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                \u001b[49m\u001b[43mval_activations_stor_dir_model_2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/layer_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlayer\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_out\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtraining_cfg_cross_coder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                \u001b[49m\u001b[43mWANDB_ENTITIY\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 92\u001b[0m, in \u001b[0;36mtrain_crosscoder\u001b[0;34m(model_1_name, model_2_name, index, train_activations_stor_dir_model_1, val_activations_stor_dir_model_1, train_activations_stor_dir_model_2, val_activations_stor_dir_model_2, layer, training_cfg_cross_coder, wandb_entitiy)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# train the sparse autoencoder (SAE)\u001b[39;00m\n\u001b[1;32m     91\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n\u001b[0;32m---> 92\u001b[0m ae \u001b[38;5;241m=\u001b[39m \u001b[43mtrainSAE\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_every_n_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_cfg_cross_coder\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidate_every_n_steps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_entity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwandb_entitiy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_project\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcrosscoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_cfg_cross_coder\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_steps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/wsg/WSG_games/dictionary_learning/dictionary_learning/training.py:286\u001b[0m, in \u001b[0;36mtrainSAE\u001b[0;34m(data, trainer_config, use_wandb, wandb_entity, wandb_project, steps, save_steps, save_dir, log_steps, activations_split_by_head, validate_every_n_steps, validation_data, transcoder, run_cfg, end_of_step_logging_fn, save_last_eval, start_of_training_eval, dtype)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    281\u001b[0m     validate_every_n_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;241m%\u001b[39m validate_every_n_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (start_of_training_eval \u001b[38;5;129;01mor\u001b[39;00m step \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    284\u001b[0m ):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidating at step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 286\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m         os\u001b[38;5;241m.\u001b[39mmakedirs(save_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/wsg/WSG_games/uv_venv2/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/wsg/WSG_games/dictionary_learning/dictionary_learning/training.py:116\u001b[0m, in \u001b[0;36mrun_validation\u001b[0;34m(trainer, validation_data, step, dtype)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(trainer, CrossCoderTrainer) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    113\u001b[0m     trainer, BatchTopKCrossCoderTrainer\n\u001b[1;32m    114\u001b[0m ):\n\u001b[1;32m    115\u001b[0m     frac_variance_explained_per_layer \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m val_step, act \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(validation_data, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(validation_data))):\n\u001b[1;32m    117\u001b[0m     act \u001b[38;5;241m=\u001b[39m act\u001b[38;5;241m.\u001b[39mto(trainer\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mto(dtype)\n\u001b[1;32m    118\u001b[0m     stats \u001b[38;5;241m=\u001b[39m get_stats(trainer, act, deads_sum\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/wsg/WSG_games/uv_venv2/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/wsg/WSG_games/uv_venv2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m~/wsg/WSG_games/uv_venv2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1491\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data, worker_id)\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1491\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1494\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/wsg/WSG_games/uv_venv2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1443\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1443\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1444\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1445\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/wsg/WSG_games/uv_venv2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1284\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1285\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1286\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1287\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1289\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._post_run_cell_hook of <wandb.sdk.wandb_init._WandbInit object at 0x7fcb16faabf0>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fce15d0ba00, execution_count=13 error_before_exec=None error_in_exec= info=<ExecutionInfo object at 7fce15d0ace0, raw_cell=\"layer = 3\n",
      "\n",
      "model_1_name = \"strong_model\"\n",
      "model_2_n..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2Btorrnode4/homes/55/bwilop/wsg/WSG_games/playground_crosscoder.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/wsg/WSG_games/uv_venv2/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:614\u001b[0m, in \u001b[0;36m_WandbInit._post_run_cell_hook\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresuming backend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 614\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/wsg/WSG_games/uv_venv2/lib/python3.10/site-packages/wandb/sdk/interface/interface.py:778\u001b[0m, in \u001b[0;36mInterfaceBase.publish_resume\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    777\u001b[0m     resume \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mResumeRequest()\n\u001b[0;32m--> 778\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/wsg/WSG_games/uv_venv2/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py:293\u001b[0m, in \u001b[0;36mInterfaceShared._publish_resume\u001b[0;34m(self, resume)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb\u001b[38;5;241m.\u001b[39mResumeRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(resume\u001b[38;5;241m=\u001b[39mresume)\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/wsg/WSG_games/uv_venv2/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py:39\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign(record)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/wsg/WSG_games/uv_venv2/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:174\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    172\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrequest_id \u001b[38;5;241m=\u001b[39m record\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mmailbox_slot\n\u001b[1;32m    173\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_publish\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/wsg/WSG_games/uv_venv2/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:154\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: spb\u001b[38;5;241m.\u001b[39mServerRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/wsg/WSG_games/uv_venv2/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:151\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    149\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m), raw_size)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/wsg/WSG_games/uv_venv2/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:130\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    128\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "layer = 3\n",
    "\n",
    "model_1_name = \"strong_model\"\n",
    "model_2_name = \"finetuned_model\"\n",
    "\n",
    "train_activations_stor_dir_model_1 = get_activations_path(\n",
    "    Goal.STRONG_GOAL, None, strong_model_size, index, crosscoder_folder, \"train\"\n",
    ")\n",
    "val_activations_stor_dir_model_1 = get_activations_path(\n",
    "    Goal.STRONG_GOAL, None, strong_model_size, index, crosscoder_folder, \"val\"\n",
    ")\n",
    "val_activations_stor_dir_model_2 = get_activations_path(\n",
    "    None, weak_model_size, strong_model_size, index, crosscoder_folder, \"val\"\n",
    ")\n",
    "train_activations_stor_dir_model_2 = get_activations_path(\n",
    "    None, weak_model_size, strong_model_size, index, crosscoder_folder, \"train\"\n",
    ")\n",
    "\n",
    "training_cfg_cross_coder = get_training_cfg_cross_coder()\n",
    "train_crosscoder(\n",
    "    model_1_name,\n",
    "    model_2_name,\n",
    "    index,\n",
    "    train_activations_stor_dir_model_1 + f\"/layer_{layer}_out\",\n",
    "    val_activations_stor_dir_model_1 + f\"/layer_{layer}_out\",\n",
    "    train_activations_stor_dir_model_2 + f\"/layer_{layer}_out\",\n",
    "    val_activations_stor_dir_model_2 + f\"/layer_{layer}_out\",\n",
    "    layer,\n",
    "    training_cfg_cross_coder,\n",
    "    WANDB_ENTITIY,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (uv_venv2)",
   "language": "python",
   "name": "uv_venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
