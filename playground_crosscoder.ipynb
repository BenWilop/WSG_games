{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/BenWilop/WSG_games/blob/main/playground_WSG_games.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "V887fWJPBYKB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /homes/55/bwilop/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbenwilop\u001b[0m (\u001b[33mbenwilop-rwth-aachen-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv(os.path.join(\"/homes/55/bwilop/wsg/private/\", \"vscode-ssh.env\"))\n",
    "api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "wandb.login(key=api_key)\n",
    "WANDB_ENTITIY = \"benwilop-rwth-aachen-university\"\n",
    "\n",
    "data_folder = \"/homes/55/bwilop/wsg/data/\"\n",
    "experiment_folder = \"/homes/55/bwilop/wsg/experiments/\"\n",
    "crosscoder_folder = experiment_folder + \"tictactoe/crosscoder/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uV6cMqvSvaBY",
    "outputId": "ec9e34fb-4176-487f-e718-9b1a44238dfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "import torch as t\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import datetime\n",
    "\n",
    "# from jaxtyping import Float\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wsg_games.tictactoe.evals import *\n",
    "from wsg_games.tictactoe.data import *\n",
    "from wsg_games.tictactoe.game import *\n",
    "\n",
    "from wsg_games.tictactoe.analysis.analyse_data import *\n",
    "from wsg_games.tictactoe.analysis.visualize_game import *\n",
    "\n",
    "from wsg_games.tictactoe.train.create_models import *\n",
    "from wsg_games.tictactoe.train.save_load_models import *\n",
    "from wsg_games.tictactoe.train.train import *\n",
    "from wsg_games.tictactoe.train.finetune import *\n",
    "from wsg_games.tictactoe.train.pretrain import *\n",
    "\n",
    "DEVICE = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-WZi6lEq1sA"
   },
   "source": [
    "# Load Data & Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FseGnVqe_00c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment_folder:  /homes/55/bwilop/wsg/experiments/\n",
      "project_name:  tictactoe/tictactoe_pretraining5\n",
      "/homes/55/bwilop/wsg/experiments/tictactoe/tictactoe_pretraining5\n",
      "Loading model from /homes/55/bwilop/wsg/experiments/tictactoe/tictactoe_pretraining5/experiment_2_small_weak_2025-05-16-16-35_ayyrg2xq.pkl\n",
      "Moving model to device:  cuda\n",
      "experiment_folder:  /homes/55/bwilop/wsg/experiments/\n",
      "project_name:  tictactoe/tictactoe_pretraining5\n",
      "/homes/55/bwilop/wsg/experiments/tictactoe/tictactoe_pretraining5\n",
      "Loading model from /homes/55/bwilop/wsg/experiments/tictactoe/tictactoe_pretraining5/experiment_2_medium_weak_2025-05-16-16-35_eif67e03.pkl\n",
      "Moving model to device:  cuda\n",
      "experiment_folder:  /homes/55/bwilop/wsg/experiments/\n",
      "project_name:  tictactoe/tictactoe_pretraining5\n",
      "/homes/55/bwilop/wsg/experiments/tictactoe/tictactoe_pretraining5\n",
      "Loading model from /homes/55/bwilop/wsg/experiments/tictactoe/tictactoe_pretraining5/experiment_2_medium_strong_2025-05-16-16-44_omyjjb73.pkl\n",
      "Moving model to device:  cuda\n",
      "Moving model to device:  cuda\n",
      "weak_model\n",
      "weak_loss:  0.971375584602356\n",
      "strong_loss:  1.2469927072525024\n",
      "strong_baseline_model\n",
      "weak_loss:  0.7467523217201233\n",
      "strong_loss:  1.4348102807998657\n",
      "strong_model\n",
      "weak_loss:  1.4630937576293945\n",
      "strong_loss:  0.7160466909408569\n",
      "finetuned_model\n",
      "weak_loss:  0.8342999815940857\n",
      "strong_loss:  1.004762053489685\n",
      "Performance Gap Recovered (PGR):  0.6102466915020168\n"
     ]
    }
   ],
   "source": [
    "project_name_pretrain = \"tictactoe/tictactoe_pretraining5\"\n",
    "project_name_finetune = \"tictactoe/tictactoe_finetuning5\"\n",
    "weak_model_size = \"small\"\n",
    "strong_model_size = \"medium\"\n",
    "index = 2\n",
    "\n",
    "# Load data\n",
    "(\n",
    "    tictactoe_train_data,\n",
    "    tictactoe_weak_finetune_data,\n",
    "    tictactoe_val_data,\n",
    "    tictactoe_test_data,\n",
    ") = load_split_data(data_folder + \"tictactoe/\", device=DEVICE, index=index)\n",
    "\n",
    "# Load models\n",
    "weak_model = load_model(\n",
    "    project_name_pretrain,\n",
    "    weak_model_size,\n",
    "    Goal.WEAK_GOAL,\n",
    "    experiment_folder,\n",
    "    device=DEVICE,\n",
    "    index=index,\n",
    ")\n",
    "strong_baseline_model = load_model(\n",
    "    project_name_pretrain,\n",
    "    strong_model_size,\n",
    "    Goal.WEAK_GOAL,\n",
    "    experiment_folder,\n",
    "    device=DEVICE,\n",
    "    index=index,\n",
    ")\n",
    "strong_model = load_model(\n",
    "    project_name_pretrain,\n",
    "    strong_model_size,\n",
    "    Goal.STRONG_GOAL,\n",
    "    experiment_folder,\n",
    "    device=DEVICE,\n",
    "    index=index,\n",
    ")\n",
    "finetuned_model = load_finetuned_model(\n",
    "    project_name_finetune,\n",
    "    weak_model_size,\n",
    "    strong_model_size,\n",
    "    experiment_folder,\n",
    "    DEVICE,\n",
    "    index,\n",
    ")\n",
    "\n",
    "# Print evaluations\n",
    "(\n",
    "    weak_loss,\n",
    "    _,\n",
    ") = quick_evaluation(\"weak_model\", weak_model, tictactoe_test_data)\n",
    "strong_baseline_loss, _ = quick_evaluation(\n",
    "    \"strong_baseline_model\", strong_baseline_model, tictactoe_test_data\n",
    ")\n",
    "quick_evaluation(\"strong_model\", strong_model, tictactoe_test_data)\n",
    "weak_finetuned_loss, _ = quick_evaluation(\n",
    "    \"finetuned_model\", finetuned_model, tictactoe_test_data\n",
    ")\n",
    "print(\n",
    "    \"Performance Gap Recovered (PGR): \",\n",
    "    (weak_loss - weak_finetuned_loss) / (weak_loss - strong_baseline_loss),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dictionary_learning.dictionary_learning import CrossCoder\n",
    "from dictionary_learning.dictionary_learning.dictionary import BatchTopKCrossCoder\n",
    "from dictionary_learning.dictionary_learning.trainers.crosscoder import (\n",
    "    BatchTopKCrossCoderTrainer,\n",
    ")\n",
    "from dictionary_learning.dictionary_learning.training import trainSAE\n",
    "from dictionary_learning.dictionary_learning.cache import (\n",
    "    PairedActivationCache,\n",
    "    ActivationCache,\n",
    "    ActivationShard,\n",
    ")\n",
    "import transformer_lens.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(\n",
    "    model,\n",
    "    tokenized_games: Float[Tensor, \"n_games game_length\"],\n",
    "    layer_i: int,\n",
    ") -> Float[Tensor, \"n_games game_length d_model\"]:\n",
    "    activation_hook_name = utils.get_act_name(\"resid_post\", layer_i)\n",
    "    model.eval()\n",
    "    _, cache = model.run_with_cache(tokenized_games)\n",
    "    layer_activations = cache[activation_hook_name]\n",
    "    return layer_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.1.hook_resid_post\n"
     ]
    }
   ],
   "source": [
    "activation_hook_name = utils.get_act_name(\"resid_post\", 1)\n",
    "print(activation_hook_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@t.no_grad()\n",
    "def create_data_shards(\n",
    "    games_data: Float[Tensor, \"n_games game_length\"],\n",
    "    model: HookedTransformer,\n",
    "    store_dir: str,\n",
    "    batch_size: int = 64,\n",
    "    shard_size: int = 10**6,\n",
    "    max_total_tokens: int = 10**8,\n",
    "    overwrite: bool = False,\n",
    ") -> None:\n",
    "    dataloader = DataLoader(games_data, batch_size=batch_size)\n",
    "    io: str = \"out\"\n",
    "    submodule_names = [f\"layer_{layer_i}\" for layer_i in range(model.cfg.n_layers)]\n",
    "\n",
    "    activation_cache = [[] for _ in submodule_names]\n",
    "    store_dirs = [\n",
    "        os.path.join(store_dir, f\"{submodule_names[layer_i]}_{io}\")\n",
    "        for layer_i in range(len(submodule_names))\n",
    "    ]\n",
    "    for store_dir in store_dirs:\n",
    "        os.makedirs(store_dir, exist_ok=True)\n",
    "    total_size = 0\n",
    "    current_size = 0\n",
    "    shard_count = 0\n",
    "\n",
    "    # Check if shards already exist\n",
    "    if os.path.exists(os.path.join(store_dirs[0], \"shard_0.memmap\")):\n",
    "        print(f\"Shards already exist in {store_dir}\")\n",
    "        if not overwrite:\n",
    "            print(\"Set overwrite=True to overwrite existing shards.\")\n",
    "            return\n",
    "        else:\n",
    "            print(\"Overwriting existing shards...\")\n",
    "\n",
    "    print(\"Collecting activations...\")\n",
    "    for games in tqdm(dataloader, desc=\"Collecting activations\"):\n",
    "        for layer_i in range(len(submodule_names)):\n",
    "            local_activations = rearrange(\n",
    "                get_activations(model, games, layer_i)\n",
    "            )  # (B x T) x D\n",
    "            activation_cache[layer_i].append(local_activations.cpu())\n",
    "\n",
    "        current_size += activation_cache[0][-1].shape[0]\n",
    "        if current_size > shard_size:\n",
    "            print(f\"Storing shard {shard_count}...\", flush=True)\n",
    "            ActivationCache.collate_store_shards(\n",
    "                store_dirs,\n",
    "                shard_count,\n",
    "                activation_cache,\n",
    "                submodule_names,\n",
    "                shuffle_shards=True,\n",
    "                io=io,\n",
    "                multiprocessing=False,\n",
    "            )\n",
    "            shard_count += 1\n",
    "            total_size += current_size\n",
    "            current_size = 0\n",
    "            activation_cache = [[] for _ in submodule_names]\n",
    "\n",
    "        if total_size > max_total_tokens:\n",
    "            print(\"Max total tokens reached. Stopping collection.\")\n",
    "            break\n",
    "\n",
    "    if current_size > 0:\n",
    "        ActivationCache.collate_store_shards(\n",
    "            store_dirs,\n",
    "            shard_count,\n",
    "            activation_cache,\n",
    "            submodule_names,\n",
    "            shuffle_shards=True,\n",
    "            io=io,\n",
    "            multiprocessing=False,\n",
    "        )\n",
    "\n",
    "    # store configs\n",
    "    for i, store_dir in enumerate(store_dirs):\n",
    "        with open(os.path.join(store_dir, \"config.json\"), \"w\") as f:\n",
    "            json.dump(\n",
    "                {\n",
    "                    \"batch_size\": batch_size,\n",
    "                    \"context_len\": -1,\n",
    "                    \"shard_size\": shard_size,\n",
    "                    \"d_model\": model.cfg.d_model,\n",
    "                    \"shuffle_shards\": True,\n",
    "                    \"io\": io,\n",
    "                    \"total_size\": total_size,\n",
    "                    \"shard_count\": shard_count,\n",
    "                    \"store_tokens\": False,\n",
    "                },\n",
    "                f,\n",
    "            )\n",
    "    ActivationCache.cleanup_multiprocessing()\n",
    "    print(f\"Finished collecting activations. Total size: {total_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations_path(\n",
    "    model_goal: Goal | None,\n",
    "    weak_model_size: str | None,\n",
    "    model_size: str,\n",
    "    index: int,\n",
    "    crosscoder_folder: str,\n",
    "    train_val: str,\n",
    ") -> str:\n",
    "    assert model_goal is None or weak_model_size is None\n",
    "    assert model_goal is not None or weak_model_size is not None\n",
    "    if weak_model_size:\n",
    "        postfix = \"finetuned_through_\" + weak_model_size\n",
    "    elif model_goal in [Goal.WEAK_GOAL, Goal.STRONG_GOAL]:\n",
    "        postfix = str(model_goal)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid activations model goal: {model_goal}\")\n",
    "    return os.path.join(\n",
    "        crosscoder_folder, \"activations\", f\"{index}_{model_size}_{postfix}_\" + train_val\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_activations(\n",
    "    model_goal: Goal | None,\n",
    "    project_name_pretrain: str | None,\n",
    "    weak_model_size: str | None,\n",
    "    project_name_finetune: str | None,\n",
    "    model_size: str,\n",
    "    index: int,\n",
    "    crosscoder_folder: str,\n",
    "    tictactoe_test_data: Float[Tensor, \"n_games game_length\"],\n",
    "    tictactoe_val_data: Float[Tensor, \"n_games game_length\"],\n",
    "    experiment_folder: str,\n",
    ") -> None:\n",
    "    # Either finetuned or pretrained\n",
    "    bool_finetuned_model = (\n",
    "        project_name_finetune is not None and weak_model_size is not None\n",
    "    )\n",
    "    bool_pretrained_model = project_name_pretrain is not None and model_goal is not None\n",
    "    assert int(bool_finetuned_model) + int(bool_pretrained_model) == 1, (\n",
    "        f\"Finetuned XOR pretrained model must be provided.\"\n",
    "    )\n",
    "\n",
    "    # Models\n",
    "    if bool_finetuned_model:\n",
    "        model = load_finetuned_model(\n",
    "            project_name_finetune,\n",
    "            weak_model_size,\n",
    "            model_size,\n",
    "            experiment_folder,\n",
    "            DEVICE,\n",
    "            index,\n",
    "        )\n",
    "    else:\n",
    "        model = load_model(\n",
    "            project_name_pretrain,\n",
    "            model_size,\n",
    "            model_goal,\n",
    "            experiment_folder,\n",
    "            device=DEVICE,\n",
    "            index=index,\n",
    "        )\n",
    "\n",
    "    # Run\n",
    "    for train_val in [\"train\", \"val\"]:\n",
    "        if train_val == \"train\":\n",
    "            games_data = tictactoe_test_data\n",
    "        elif train_val == \"val\":\n",
    "            games_data = tictactoe_val_data\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid train_val: {train_val}\")\n",
    "\n",
    "        activations_path = get_activations_path(\n",
    "            model_goal, weak_model_size, model_size, index, crosscoder_folder, train_val\n",
    "        )\n",
    "        create_data_shards(\n",
    "            games_data,\n",
    "            model,\n",
    "            store_dir=activations_path,\n",
    "            batch_size=64,\n",
    "            shard_size=10**5,\n",
    "            max_total_tokens=10**10,\n",
    "            overwrite=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment_folder:  /homes/55/bwilop/wsg/experiments/\n",
      "project_name:  tictactoe/tictactoe_pretraining5\n",
      "/homes/55/bwilop/wsg/experiments/tictactoe/tictactoe_pretraining5\n",
      "Loading model from /homes/55/bwilop/wsg/experiments/tictactoe/tictactoe_pretraining5/experiment_2_medium_strong_2025-05-16-16-44_omyjjb73.pkl\n",
      "Moving model to device:  cuda\n",
      "Shards already exist in /homes/55/bwilop/wsg/experiments/tictactoe/crosscoder/activations/2_medium_strong_train/layer_3_out\n",
      "Set overwrite=True to overwrite existing shards.\n",
      "Shards already exist in /homes/55/bwilop/wsg/experiments/tictactoe/crosscoder/activations/2_medium_strong_val/layer_3_out\n",
      "Set overwrite=True to overwrite existing shards.\n",
      "Moving model to device:  cuda\n",
      "Shards already exist in /homes/55/bwilop/wsg/experiments/tictactoe/crosscoder/activations/2_medium_finetuned_through_small_train/layer_3_out\n",
      "Set overwrite=True to overwrite existing shards.\n",
      "Shards already exist in /homes/55/bwilop/wsg/experiments/tictactoe/crosscoder/activations/2_medium_finetuned_through_small_val/layer_3_out\n",
      "Set overwrite=True to overwrite existing shards.\n"
     ]
    }
   ],
   "source": [
    "# Strong\n",
    "compute_activations(\n",
    "    Goal.STRONG_GOAL,\n",
    "    project_name_pretrain,\n",
    "    None,\n",
    "    None,\n",
    "    strong_model_size,\n",
    "    index,\n",
    "    crosscoder_folder,\n",
    "    tictactoe_test_data.games_data,\n",
    "    tictactoe_val_data.games_data,\n",
    "    experiment_folder,\n",
    ")\n",
    "\n",
    "# Finetuned\n",
    "compute_activations(\n",
    "    None,\n",
    "    None,\n",
    "    weak_model_size,\n",
    "    project_name_finetune,\n",
    "    strong_model_size,\n",
    "    index,\n",
    "    crosscoder_folder,\n",
    "    tictactoe_test_data.games_data,\n",
    "    tictactoe_val_data.games_data,\n",
    "    experiment_folder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_epoch_dataloader_iterator(dataloader: DataLoader, total_steps_to_yield: int):\n",
    "    \"\"\"\n",
    "    A generator that yields batches from a DataLoader repeatedly until\n",
    "    total_steps_to_yield is reached. Re-shuffles if dataloader.shuffle=True.\n",
    "    \"\"\"\n",
    "    # Edge cases\n",
    "    if total_steps_to_yield == 0:  # No steps\n",
    "        return\n",
    "    try:\n",
    "        if len(dataloader) == 0 and total_steps_to_yield > 0:  # Empty dataloader\n",
    "            print(\n",
    "                \"Warning: DataLoader is empty, but total_steps_to_yield > 0. No steps will run.\"\n",
    "            )\n",
    "            return\n",
    "    except TypeError:  # no __len__\n",
    "        pass\n",
    "\n",
    "    steps_yielded = 0\n",
    "    while steps_yielded < total_steps_to_yield:\n",
    "        num_batches_this_epoch = 0\n",
    "        for batch in dataloader:  # DataLoader shuffles here if its shuffle=True\n",
    "            if steps_yielded >= total_steps_to_yield:\n",
    "                return\n",
    "            yield batch\n",
    "            steps_yielded += 1\n",
    "            num_batches_this_epoch += 1\n",
    "\n",
    "        # Safeguard, if the dataloader gets empty for any reason, it would be an infinite loop otherwise\n",
    "        if num_batches_this_epoch == 0 and steps_yielded < total_steps_to_yield:\n",
    "            print(\"Warning: DataLoader became empty before all steps were yielded.\")\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_cfg_cross_coder():\n",
    "    training_cfg_cross_coder = {\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"max_steps\": 20000,\n",
    "        \"validate_every_n_steps\": 1000,\n",
    "        \"batch_size\": 64,\n",
    "        \"expansion_factor\": 32,\n",
    "        \"k\": 10,\n",
    "    }\n",
    "    return training_cfg_cross_coder\n",
    "\n",
    "\n",
    "def train_crosscoder(\n",
    "    model_1_name: str,\n",
    "    model_2_name: str,\n",
    "    index: int,\n",
    "    train_activations_stor_dir_model_1: str,\n",
    "    val_activations_stor_dir_model_1: str,\n",
    "    train_activations_stor_dir_model_2: str,\n",
    "    val_activations_stor_dir_model_2: str,\n",
    "    layer: int,\n",
    "    training_cfg_cross_coder: dict,\n",
    "    wandb_entity: str,\n",
    "    device: t.device = DEVICE,\n",
    ") -> None:\n",
    "    # Save arguments\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "    experiment_name = f\"experiment_{index}_{model_1_name}_{model_2_name}_{timestamp}\"\n",
    "    save_dir = crosscoder_folder + \"checkpoints/\" + experiment_name\n",
    "    train_crosscoder_args = {\n",
    "        \"model_1_name\": model_1_name,\n",
    "        \"model_2_name\": model_2_name,\n",
    "        \"index\": index,\n",
    "        \"layer\": layer,\n",
    "        \"training_cfg_cross_coder\": training_cfg_cross_coder,\n",
    "        \"data_path\": {\n",
    "            \"train_activations_stor_dir_model_1\": train_activations_stor_dir_model_1,\n",
    "            \"val_activations_stor_dir_model_1\": val_activations_stor_dir_model_1,\n",
    "            \"train_activations_stor_dir_model_2\": train_activations_stor_dir_model_2,\n",
    "            \"val_activations_stor_dir_model_2\": val_activations_stor_dir_model_2,\n",
    "        },\n",
    "    }\n",
    "    save_dir = os.path.join(crosscoder_folder, \"checkpoints\", experiment_name)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    with open(\n",
    "        os.path.join(save_dir, \"train_crosscoder_args.json\"), \"w\", encoding=\"utf-8\"\n",
    "    ) as json_file:\n",
    "        json.dump(train_crosscoder_args, json_file)\n",
    "\n",
    "    # Data (not loaded in memory yet)\n",
    "    train_dataset = PairedActivationCache(\n",
    "        train_activations_stor_dir_model_1,\n",
    "        train_activations_stor_dir_model_2,\n",
    "    )\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=training_cfg_cross_coder[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    print(f\"Training on {len(train_dataset)} token activations.\")\n",
    "    val_dataset = PairedActivationCache(\n",
    "        val_activations_stor_dir_model_1,\n",
    "        val_activations_stor_dir_model_2,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=1000,\n",
    "        shuffle=False,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    print(f\"Validating on {len(val_dataset)} token activations.\")\n",
    "\n",
    "    # Training config\n",
    "    activation_dim = train_dataset[0].shape[1]\n",
    "    dictionary_size = training_cfg_cross_coder[\"expansion_factor\"] * activation_dim\n",
    "    print(f\"Activation dim: {activation_dim}\")\n",
    "    print(f\"Dictionary size: {dictionary_size}\")\n",
    "    k = training_cfg_cross_coder[\"k\"]\n",
    "    lr = training_cfg_cross_coder[\"learning_rate\"]\n",
    "    max_steps = training_cfg_cross_coder[\"max_steps\"]\n",
    "\n",
    "    # Top level of trainer_cfg: BatchTopKCrossCoderTrainer\n",
    "    # dict_class_kwargs: CrossCoder\n",
    "    trainer_cfg = {\n",
    "        \"trainer\": BatchTopKCrossCoderTrainer,\n",
    "        \"activation_dim\": activation_dim,\n",
    "        \"dict_size\": dictionary_size,\n",
    "        \"lr\": lr,\n",
    "        \"device\": str(device),\n",
    "        \"wandb_name\": experiment_name + f\"L{layer}-k{k:.1e}-lr{lr:.0e}\",  #\n",
    "        \"steps\": max_steps,  # Also used below, I think because it is needed here for learning rate and in trainSAE just for loop termination\n",
    "        \"k\": k,  # 'k' as a top-level argument for the trainer\n",
    "        \"layer\": layer,  # Only for logging\n",
    "        \"lm_name\": experiment_name,  # Only for logging\n",
    "        # \"dict_class_kwargs\": {\n",
    "        # },\n",
    "    }\n",
    "    # train the sparse autoencoder (SAE)\n",
    "    wandb.finish()\n",
    "    multi_epoch_train_dataloader = multi_epoch_dataloader_iterator(\n",
    "        train_dataloader, max_steps\n",
    "    )\n",
    "    trainSAE(\n",
    "        data=multi_epoch_train_dataloader,\n",
    "        trainer_config=trainer_cfg,\n",
    "        validate_every_n_steps=training_cfg_cross_coder[\"validate_every_n_steps\"],\n",
    "        validation_data=val_dataloader,\n",
    "        use_wandb=True,\n",
    "        wandb_entity=wandb_entity,\n",
    "        wandb_project=\"crosscoder\",\n",
    "        log_steps=50,\n",
    "        save_dir=save_dir,\n",
    "        steps=max_steps,\n",
    "        save_steps=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 200704 token activations.\n",
      "Validating on 100352 token activations.\n",
      "Activation dim: 32\n",
      "Dictionary size: 1024\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/homes/55/bwilop/wsg/WSG_games/wandb/run-20250524_112124-squbveie</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/benwilop-rwth-aachen-university/crosscoder/runs/squbveie' target=\"_blank\">experiment_2_strong_model_finetuned_model_2025-05-24-11-21L3-k1.0e+01-lr1e-03</a></strong> to <a href='https://wandb.ai/benwilop-rwth-aachen-university/crosscoder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/benwilop-rwth-aachen-university/crosscoder' target=\"_blank\">https://wandb.ai/benwilop-rwth-aachen-university/crosscoder</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/benwilop-rwth-aachen-university/crosscoder/runs/squbveie' target=\"_blank\">https://wandb.ai/benwilop-rwth-aachen-university/crosscoder/runs/squbveie</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/55/bwilop/wsg/WSG_games/dictionary_learning/dictionary_learning/training.py:239: UserWarning: Error saving config: 'generator' object has no attribute 'config'\n",
      "  warn(f\"Error saving config: {e}\")\n",
      "  5%|▍         | 992/20000 [00:05<01:42, 184.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating at step 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:07<00:00, 14.32it/s]\n",
      " 10%|▉         | 1985/20000 [00:17<01:34, 190.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating at step 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:07<00:00, 14.38it/s]\n",
      " 15%|█▍        | 2991/20000 [00:30<01:29, 190.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating at step 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:07<00:00, 14.29it/s]\n",
      " 20%|█▉        | 3995/20000 [00:43<01:45, 152.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating at step 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:07<00:00, 14.30it/s]\n",
      " 25%|██▍       | 4997/20000 [00:55<01:40, 149.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating at step 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:07<00:00, 14.31it/s]\n",
      " 30%|██▉       | 5989/20000 [01:08<01:33, 149.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating at step 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:07<00:00, 14.42it/s]\n",
      " 35%|███▍      | 6995/20000 [01:21<01:07, 191.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating at step 7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:07<00:00, 14.25it/s]\n",
      " 40%|███▉      | 7994/20000 [01:33<01:02, 192.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating at step 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:07<00:00, 14.23it/s]\n",
      " 45%|████▍     | 8980/20000 [01:46<00:55, 198.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating at step 9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:07<00:00, 14.30it/s]\n",
      " 50%|████▉     | 9999/20000 [01:58<00:52, 191.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating at step 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:07<00:00, 14.18it/s]\n",
      " 55%|█████▍    | 10994/20000 [02:12<00:52, 171.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating at step 11000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:07<00:00, 14.28it/s]\n",
      " 60%|█████▉    | 11995/20000 [02:24<00:41, 190.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating at step 12000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:06<00:00, 14.61it/s]\n",
      " 65%|██████▍   | 12998/20000 [02:36<00:36, 189.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating at step 13000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:07<00:00, 13.79it/s]\n",
      " 70%|██████▉   | 13986/20000 [02:49<00:32, 187.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating at step 14000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:06<00:00, 14.62it/s]\n",
      " 75%|███████▍  | 14996/20000 [03:01<00:25, 197.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating at step 15000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:06<00:00, 15.12it/s]\n",
      " 80%|███████▉  | 15987/20000 [03:13<00:20, 191.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating at step 16000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:07<00:00, 14.29it/s]\n",
      " 85%|████████▍ | 16983/20000 [03:25<00:15, 190.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating at step 17000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:09<00:00, 10.70it/s]\n",
      " 90%|████████▉ | 17992/20000 [03:40<00:10, 192.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating at step 18000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:07<00:00, 14.30it/s]\n",
      " 95%|█████████▌| 19000/20000 [03:53<00:05, 187.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating at step 19000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:06<00:00, 14.92it/s]\n",
      "100%|██████████| 20000/20000 [04:05<00:00, 81.49it/s] \n",
      "100%|██████████| 101/101 [00:06<00:00, 14.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▃▃▄▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>train/auxk_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/cl0_frac_variance_explained</td><td>▁▂▆▇████████████████████████████████████</td></tr><tr><td>train/cl1_frac_variance_explained</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>train/effective_l0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/frac_deads</td><td>▁▇▇█▇██▇█▇█▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▇▇▆▇▆▇▆▇▆</td></tr><tr><td>train/frac_variance_explained</td><td>▁███████████████████████████████████████</td></tr><tr><td>train/l0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/l2_loss</td><td>█▄▃▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▂▁▂▁▁▂▂▂▁▁▂▁▁▂▁▁▂▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/mse_loss</td><td>█▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/pre_norm_auxk_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/running_deads</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/sparsity_weight</td><td>█▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/threshold</td><td>▁▁▁██▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>trainthres/auxk_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainthres/cl0_frac_variance_explained</td><td>▁███████████████████████████████████████</td></tr><tr><td>trainthres/cl1_frac_variance_explained</td><td>▁███████████████████████████████████████</td></tr><tr><td>trainthres/effective_l0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainthres/frac_deads</td><td>▁▁▂█████████████████████████████████████</td></tr><tr><td>trainthres/frac_variance_explained</td><td>▁███████████████████████████████████████</td></tr><tr><td>trainthres/l0</td><td>█▇▇▆▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainthres/l2_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainthres/loss</td><td>█▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainthres/mse_loss</td><td>█▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainthres/pre_norm_auxk_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainthres/running_deads</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainthres/sparsity_weight</td><td>█▇▆▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainthres/threshold</td><td>▁▁▁▁▁█████▇▇▇▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>val/cl0_frac_variance_explained</td><td>▁▆▇▇████████████████</td></tr><tr><td>val/cl1_frac_variance_explained</td><td>▁▇█████████████████▇</td></tr><tr><td>val/frac_deads</td><td>▁▂▃▃▄▅▆▆▇▇▇▇████████</td></tr><tr><td>val/frac_variance_explained</td><td>▁▇▇█████████████████</td></tr><tr><td>val/l0</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/num_specific_latents_l0</td><td>▁▁▂▄▅▅▆▆▆▇▇▇▇▇▇▇████</td></tr><tr><td>val/num_specific_latents_l1</td><td>▁▁▂▄▅▅▆▆▆▇▇▇▇███████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>19999</td></tr><tr><td>train/auxk_loss</td><td>0</td></tr><tr><td>train/cl0_frac_variance_explained</td><td>0.95246</td></tr><tr><td>train/cl1_frac_variance_explained</td><td>0.95206</td></tr><tr><td>train/effective_l0</td><td>10</td></tr><tr><td>train/frac_deads</td><td>0.88867</td></tr><tr><td>train/frac_variance_explained</td><td>0.95228</td></tr><tr><td>train/l0</td><td>10</td></tr><tr><td>train/l2_loss</td><td>12.39712</td></tr><tr><td>train/loss</td><td>12.39712</td></tr><tr><td>train/mse_loss</td><td>181.28909</td></tr><tr><td>train/pre_norm_auxk_loss</td><td>-1</td></tr><tr><td>train/running_deads</td><td>0</td></tr><tr><td>train/sparsity_weight</td><td>1.3581</td></tr><tr><td>train/threshold</td><td>6.84287</td></tr><tr><td>trainthres/auxk_loss</td><td>0</td></tr><tr><td>trainthres/cl0_frac_variance_explained</td><td>0.95226</td></tr><tr><td>trainthres/cl1_frac_variance_explained</td><td>0.95317</td></tr><tr><td>trainthres/effective_l0</td><td>10</td></tr><tr><td>trainthres/frac_deads</td><td>0.88672</td></tr><tr><td>trainthres/frac_variance_explained</td><td>0.95267</td></tr><tr><td>trainthres/l0</td><td>10.125</td></tr><tr><td>trainthres/l2_loss</td><td>12.34741</td></tr><tr><td>trainthres/loss</td><td>12.34741</td></tr><tr><td>trainthres/mse_loss</td><td>179.88159</td></tr><tr><td>trainthres/pre_norm_auxk_loss</td><td>-1</td></tr><tr><td>trainthres/running_deads</td><td>0</td></tr><tr><td>trainthres/sparsity_weight</td><td>1.3581</td></tr><tr><td>trainthres/threshold</td><td>6.84287</td></tr><tr><td>val/cl0_frac_variance_explained</td><td>0.93873</td></tr><tr><td>val/cl1_frac_variance_explained</td><td>0.87815</td></tr><tr><td>val/frac_deads</td><td>0.48145</td></tr><tr><td>val/frac_variance_explained</td><td>0.91229</td></tr><tr><td>val/l0</td><td>12.14142</td></tr><tr><td>val/num_specific_latents_l0</td><td>84</td></tr><tr><td>val/num_specific_latents_l1</td><td>121</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">experiment_2_strong_model_finetuned_model_2025-05-24-11-21L3-k1.0e+01-lr1e-03</strong> at: <a href='https://wandb.ai/benwilop-rwth-aachen-university/crosscoder/runs/squbveie' target=\"_blank\">https://wandb.ai/benwilop-rwth-aachen-university/crosscoder/runs/squbveie</a><br> View project at: <a href='https://wandb.ai/benwilop-rwth-aachen-university/crosscoder' target=\"_blank\">https://wandb.ai/benwilop-rwth-aachen-university/crosscoder</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250524_112124-squbveie/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer = 3\n",
    "\n",
    "model_1_name = \"strong_model\"\n",
    "model_2_name = \"finetuned_model\"\n",
    "\n",
    "train_activations_stor_dir_model_1 = get_activations_path(\n",
    "    Goal.STRONG_GOAL, None, strong_model_size, index, crosscoder_folder, \"train\"\n",
    ")\n",
    "val_activations_stor_dir_model_1 = get_activations_path(\n",
    "    Goal.STRONG_GOAL, None, strong_model_size, index, crosscoder_folder, \"val\"\n",
    ")\n",
    "train_activations_stor_dir_model_2 = get_activations_path(\n",
    "    None, weak_model_size, strong_model_size, index, crosscoder_folder, \"train\"\n",
    ")\n",
    "val_activations_stor_dir_model_2 = get_activations_path(\n",
    "    None, weak_model_size, strong_model_size, index, crosscoder_folder, \"val\"\n",
    ")\n",
    "training_cfg_cross_coder = get_training_cfg_cross_coder()\n",
    "train_crosscoder(\n",
    "    model_1_name,\n",
    "    model_2_name,\n",
    "    index,\n",
    "    train_activations_stor_dir_model_1 + f\"/layer_{layer}_out\",\n",
    "    val_activations_stor_dir_model_1 + f\"/layer_{layer}_out\",\n",
    "    train_activations_stor_dir_model_2 + f\"/layer_{layer}_out\",\n",
    "    val_activations_stor_dir_model_2 + f\"/layer_{layer}_out\",\n",
    "    layer,\n",
    "    training_cfg_cross_coder,\n",
    "    WANDB_ENTITIY,\n",
    "    DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4123618715.py, line 216)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 216\u001b[0;36m\u001b[0m\n\u001b[0;31m    crosscoder_metrics = CrosscoderMetrics(save_dir, , DEVICE)\u001b[0m\n\u001b[0m                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class CrosscoderMetrics:\n",
    "    save_dir: str\n",
    "    config: dict\n",
    "    crosscoder: BatchTopKCrossCoder\n",
    "    delta_norms: Float[Tensor, \"n_activations\"]\n",
    "    beta_reconstruction_model_1: Float[Tensor, \"n_activations\"]\n",
    "    beta_reconstruction_model_2: Float[Tensor, \"n_activations\"]\n",
    "    beta_error_model_1: Float[Tensor, \"n_activations\"]\n",
    "    beta_error_model_2: Float[Tensor, \"n_activations\"]\n",
    "    nu_reconstruction: Float[Tensor, \"n_activations\"]\n",
    "    nu_error: Float[Tensor, \"n_activations\"]\n",
    "\n",
    "    def __init__(\n",
    "        self, save_dir: str, val_dataloader: DataLoader, device: t.device\n",
    "    ) -> None:\n",
    "        self.save_dir = save_dir\n",
    "        self.config = self.load_config(self.save_dir)\n",
    "        self.crosscoder = self.load_model(self.save_dir, device)\n",
    "        self.delta_norms = self.compute_delta_norms()\n",
    "        self.beta_reconstruction_model_1, self.beta_error_model_1 = self.compute_beta(\n",
    "            model_i=0, val_dataloader=val_dataloader, device=device\n",
    "        )\n",
    "        self.beta_reconstruction_model_2, self.beta_error_model_2 = self.compute_beta(\n",
    "            model_i=1, val_dataloader=val_dataloader, device=device\n",
    "        )\n",
    "        self.nu_reconstruction = (\n",
    "            self.beta_reconstruction_model_1 / self.beta_reconstruction_model_2\n",
    "        )\n",
    "        self.nu_error = self.beta_error_model_1 / self.beta_error_model_2\n",
    "        self.plot_delta_norms()\n",
    "        self.plot_betas()\n",
    "        self.plot_nu()\n",
    "\n",
    "    # Save + Load\n",
    "    def save(self, save_dir: str) -> None:\n",
    "        path = os.path.join(save_dir, \"crosscoder_metric.pt\")\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    def _load_helper(path: str):\n",
    "        if os.path.exists(path):\n",
    "            with open(path, \"rb\") as f:\n",
    "                data = pickle.load(f)\n",
    "            return data\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"File {path} not found.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load(save_dir: str):  # -> CrosscoderMetrics\n",
    "        path = os.path.join(save_dir, \"crosscoder_metric.pt\")\n",
    "        return CrosscoderMetrics._load_helper(path)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_config(save_dir: str) -> dict:\n",
    "        path = os.path.join(save_dir, \"config.json\")\n",
    "        if os.path.exists(path):\n",
    "            with open(path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "            return data.get(\"trainer\", {})\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"File {path} not found.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(save_dir: str, device) -> CrossCoder:\n",
    "        path = os.path.join(save_dir, \"model_final.pt\")\n",
    "        if os.path.exists(path):\n",
    "            return BatchTopKCrossCoder.from_pretrained(path, device=device)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"File {path} not found.\")\n",
    "\n",
    "    def compute_delta_norms(\n",
    "        self, epsilon: float = 0.0001\n",
    "    ) -> Float[Tensor, \"n_activations\"]:\n",
    "        d_model_1_vectors = self.crosscoder.decoder.weight[0]\n",
    "        d_model_2_vectors = self.crosscoder.decoder.weight[1]\n",
    "        norm_sq_model1 = t.sum(d_model_1_vectors**2, dim=-1)\n",
    "        norm_sq_model2 = t.sum(d_model_2_vectors**2, dim=-1)\n",
    "        max_norms_sq = t.maximum(norm_sq_model1, norm_sq_model2)\n",
    "\n",
    "        delta_norms = 0.5 * (\n",
    "            (norm_sq_model2 - norm_sq_model1) / (max_norms_sq + epsilon) + 1.0\n",
    "        )\n",
    "        return delta_norms.cpu()\n",
    "\n",
    "    def compute_beta(\n",
    "        self, model_i: int, val_dataloader: DataLoader, device: t.device\n",
    "    ) -> tuple[Float[Tensor, \"n_activations\"], Float[Tensor, \"n_activations\"]]:\n",
    "        \"\"\"\n",
    "        beta = (d.T @ (Y.T @ F)) / (||d||^2 * ||f||^2)\n",
    "        \"\"\"\n",
    "        D = self.crosscoder.decoder.weight[model_i]\n",
    "        D_chat_norms_sq = D.square().sum(dim=1)  # ||D_j||_2, [dictionary_size]\n",
    "\n",
    "        # Accumulators for sums over all samples, for each latent j.\n",
    "        # numer_r[j] = sum_samples_i ( F[i,j] * <d_j, Y_reconstruction_target[i,:]> )\n",
    "        # numer_e[j] = sum_samples_i ( F[i,j] * <d_j, Y_error_target[i,:]> )\n",
    "        # sum_F_sq[j] = sum_samples_i ( F[i,j]^2 ) (this is ||f_j||^2 from the formula)\n",
    "        dict_size = D.shape[0]\n",
    "        numer_r = t.zeros(dict_size, device=device, dtype=t.float32)  # [dict_size]\n",
    "        numer_e = t.zeros_like(numer_r)  # [dict_size]\n",
    "        sum_F_sq = t.zeros_like(numer_r)  # [dict_size]\n",
    "\n",
    "        self.crosscoder.eval()\n",
    "        with t.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                activations = get_activations(\n",
    "                    self.crosscoder, batch, layer_i=self.crosscoder.layer\n",
    "                ).to(device)  # [batch_size, activation_dim]\n",
    "\n",
    "                # F_batch: sparse feature activations from sae.encode.\n",
    "                F_batch = self.crosscoder.encode(activations)  # [batch_size, dict_size]\n",
    "                sum_F_sq += F_batch.square().sum(dim=0)  # [dict_size]\n",
    "\n",
    "                # Determine target Y for reconstruction (Y_r) and error (Y_e)\n",
    "                # Y_r, Y_e have shape (batch_size, activation_dim)\n",
    "                reconstructions = self.crosscoder(activations)\n",
    "                Y_r = reconstructions\n",
    "                Y_e = activations - Y_r\n",
    "\n",
    "                # Accumulate numerators: sum_i F[i,j] * <d_j, Y_target[i,:]>\n",
    "                # (Y_target @ D.T) gives <Y_target[i,:], d_j> for each sample i and latent j  # [batch_size, dict_size]\n",
    "                numer_r += (F_batch * (Y_r @ D.T)).sum(dim=0)  # [dict_size]\n",
    "                numer_e += (F_batch * (Y_e @ D.T)).sum(dim=0)  # [dict_size]\n",
    "\n",
    "        # Denominator for beta_j = ||f_j||^2 * ||d_j||^2)\n",
    "        denominators = sum_F_sq * D_chat_norms_sq  # [dict_size]\n",
    "\n",
    "        beta_r = t.nan_to_num(numer_r / denominators, nan=1.0, posinf=1.0, neginf=1.0)\n",
    "        beta_e = t.nan_to_num(numer_e / denominators, nan=1.0, posinf=1.0, neginf=1.0)\n",
    "\n",
    "        return beta_r, beta_e\n",
    "\n",
    "    def plot_delta_norms(\n",
    "        self, threshold_only: float = 0.1, threshold_shared: float = 0.1\n",
    "    ):  # -> matplotlib plot that can be easily added to another plot as a subplot\n",
    "        # Thresholds\n",
    "        threshold_model_1_only = threshold_only\n",
    "        threshold_model_2_only = 1 - threshold_only\n",
    "        threshold_shared_lower = 0.5 - threshold_shared\n",
    "        threshold_shared_upper = 0.5 + threshold_shared\n",
    "        assert (\n",
    "            threshold_model_1_only\n",
    "            < threshold_shared_lower\n",
    "            < threshold_shared_upper\n",
    "            < threshold_model_2_only\n",
    "        ), (\n",
    "            f\"Invalid thresholds: {threshold_model_1_only}, {threshold_shared_lower}, {threshold_shared_upper}, {threshold_model_2_only}\"\n",
    "        )\n",
    "\n",
    "        # Data\n",
    "        data_to_plot = self.delta_norms.detach().numpy()\n",
    "        bins = np.linspace(0, 1, 101)  # 100 bins from 0 to 1, [0, 0.01, 0.02, ..., 1.0]\n",
    "\n",
    "        # Plot\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        fig.suptitle(\n",
    "            f\"Histogram of Delta Norms (Features: {len(data_to_plot)})\", fontsize=16\n",
    "        )\n",
    "        titles = [\"Linear Y-Axis\", \"Logarithmic Y-Axis\"]\n",
    "        y_scales = [\"linear\", \"log\"]\n",
    "        for i, ax in enumerate(axes):\n",
    "            counts, _, patches = ax.hist(\n",
    "                data_to_plot, bins=bins, edgecolor=\"black\", alpha=0.7\n",
    "            )\n",
    "            bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "            for count, patch, bin_center in zip(counts, patches, bin_centers):\n",
    "                if bin_center <= threshold_model_1_only:\n",
    "                    patch.set_facecolor(\"green\")\n",
    "                elif bin_center >= threshold_model_2_only:\n",
    "                    patch.set_facecolor(\"lightblue\")\n",
    "                elif threshold_shared_lower <= bin_center <= threshold_shared_upper:\n",
    "                    patch.set_facecolor(\"orange\")\n",
    "                else:\n",
    "                    patch.set_facecolor(\"grey\")\n",
    "\n",
    "            ax.set_title(titles[i])\n",
    "            ax.set_xlabel(\"Delta Norm Value\")\n",
    "            ax.set_ylabel(\"Number of Features\")\n",
    "            ax.set_yscale(y_scales[i])\n",
    "            if y_scales[i] == \"log\":\n",
    "                ax.set_ylim(bottom=0.1)  # Avoid log(0)\n",
    "            ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "        # Save plot\n",
    "        filename = f\"delta_norms.png\"\n",
    "        if self.save_dir:\n",
    "            os.makedirs(self.save_dir, exist_ok=True)\n",
    "            save_path = os.path.join(self.save_dir, filename)\n",
    "            try:\n",
    "                fig.savefig(save_path)\n",
    "                print(f\"Delta norms plot saved to {save_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving plot: {e}\")\n",
    "\n",
    "        return fig, axes\n",
    "\n",
    "    def plot_betas(self):\n",
    "        pass\n",
    "        # Ignore for now\n",
    "        # Please plot a\n",
    "\n",
    "    def plot_nu(self):\n",
    "        pass\n",
    "        # Ignore for now\n",
    "\n",
    "\n",
    "save_dir = \"/homes/55/bwilop/wsg/experiments/tictactoe/crosscoder/checkpoints/experiment_2_strong_model_finetuned_model_2025-05-24-11-21\"\n",
    "val_dataloader = DataLoader(\n",
    "    tictactoe_val_data.games_data,\n",
    "    batch_size=1000,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n",
    "crosscoder_metrics = CrosscoderMetrics(save_dir, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (uv_venv2)",
   "language": "python",
   "name": "uv_venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
