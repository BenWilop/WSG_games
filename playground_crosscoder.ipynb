{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/BenWilop/WSG_games/blob/main/playground_WSG_games.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "V887fWJPBYKB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /homes/55/bwilop/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbenwilop\u001b[0m (\u001b[33mbenwilop-rwth-aachen-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv(os.path.join(\"/homes/55/bwilop/wsg/private/\", \"vscode-ssh.env\"))\n",
    "api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "wandb.login(key=api_key)\n",
    "WANDB_ENTITIY = \"benwilop-rwth-aachen-university\"\n",
    "\n",
    "data_folder = \"/homes/55/bwilop/wsg/data/\"\n",
    "experiment_folder = \"/homes/55/bwilop/wsg/experiments/\"\n",
    "crosscoder_folder = experiment_folder + \"tictactoe/crosscoder/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uV6cMqvSvaBY",
    "outputId": "ec9e34fb-4176-487f-e718-9b1a44238dfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "import torch as t\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import datetime\n",
    "from typing import Iterator, Any\n",
    "\n",
    "from jaxtyping import Float\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wsg_games.tictactoe.evals import *\n",
    "from wsg_games.tictactoe.data import *\n",
    "from wsg_games.tictactoe.game import *\n",
    "\n",
    "from wsg_games.tictactoe.analysis.analyse_data import *\n",
    "from wsg_games.tictactoe.analysis.visualize_game import *\n",
    "\n",
    "from wsg_games.tictactoe.train.create_models import *\n",
    "from wsg_games.tictactoe.train.save_load_models import *\n",
    "from wsg_games.tictactoe.train.train import *\n",
    "from wsg_games.tictactoe.train.finetune import *\n",
    "from wsg_games.tictactoe.train.pretrain import *\n",
    "\n",
    "DEVICE = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-WZi6lEq1sA"
   },
   "source": [
    "# Load Data & Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FseGnVqe_00c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment_folder:  /homes/55/bwilop/wsg/experiments/\n",
      "project_name:  tictactoe/tictactoe_pretraining5\n",
      "/homes/55/bwilop/wsg/experiments/tictactoe/tictactoe_pretraining5\n",
      "Loading model from /homes/55/bwilop/wsg/experiments/tictactoe/tictactoe_pretraining5/experiment_2_small_weak_2025-05-16-16-35_ayyrg2xq.pkl\n",
      "Moving model to device:  cuda\n",
      "experiment_folder:  /homes/55/bwilop/wsg/experiments/\n",
      "project_name:  tictactoe/tictactoe_pretraining5\n",
      "/homes/55/bwilop/wsg/experiments/tictactoe/tictactoe_pretraining5\n",
      "Loading model from /homes/55/bwilop/wsg/experiments/tictactoe/tictactoe_pretraining5/experiment_2_medium_weak_2025-05-16-16-35_eif67e03.pkl\n",
      "Moving model to device:  cuda\n",
      "experiment_folder:  /homes/55/bwilop/wsg/experiments/\n",
      "project_name:  tictactoe/tictactoe_pretraining5\n",
      "/homes/55/bwilop/wsg/experiments/tictactoe/tictactoe_pretraining5\n",
      "Loading model from /homes/55/bwilop/wsg/experiments/tictactoe/tictactoe_pretraining5/experiment_2_medium_strong_2025-05-16-16-44_omyjjb73.pkl\n",
      "Moving model to device:  cuda\n",
      "Moving model to device:  cuda\n",
      "weak_model\n",
      "weak_loss:  0.937859058380127\n",
      "strong_loss:  1.1051303148269653\n",
      "strong_baseline_model\n",
      "weak_loss:  0.7346481680870056\n",
      "strong_loss:  1.2439697980880737\n",
      "strong_model\n",
      "weak_loss:  1.2865269184112549\n",
      "strong_loss:  0.6951013207435608\n",
      "finetuned_model\n",
      "weak_loss:  0.7902370691299438\n",
      "strong_loss:  0.914488673210144\n",
      "Performance Gap Recovered (PGR):  0.7264472343841707\n"
     ]
    }
   ],
   "source": [
    "project_name_pretrain = \"tictactoe/tictactoe_pretraining5\"\n",
    "project_name_finetune = \"tictactoe/tictactoe_finetuning5\"\n",
    "weak_model_size = \"small\"\n",
    "strong_model_size = \"medium\"\n",
    "index = 2\n",
    "\n",
    "# Load data\n",
    "(\n",
    "    tictactoe_train_data,\n",
    "    tictactoe_weak_finetune_data,\n",
    "    tictactoe_val_data,\n",
    "    tictactoe_test_data,\n",
    ") = load_split_data(data_folder + \"tictactoe/\", device=DEVICE, index=index)\n",
    "\n",
    "# Load models\n",
    "weak_model = load_model(\n",
    "    project_name_pretrain,\n",
    "    weak_model_size,\n",
    "    Goal.WEAK_GOAL,\n",
    "    experiment_folder,\n",
    "    device=DEVICE,\n",
    "    index=index,\n",
    ")\n",
    "strong_baseline_model = load_model(\n",
    "    project_name_pretrain,\n",
    "    strong_model_size,\n",
    "    Goal.WEAK_GOAL,\n",
    "    experiment_folder,\n",
    "    device=DEVICE,\n",
    "    index=index,\n",
    ")\n",
    "strong_model = load_model(\n",
    "    project_name_pretrain,\n",
    "    strong_model_size,\n",
    "    Goal.STRONG_GOAL,\n",
    "    experiment_folder,\n",
    "    device=DEVICE,\n",
    "    index=index,\n",
    ")\n",
    "finetuned_model = load_finetuned_model(\n",
    "    project_name_finetune,\n",
    "    weak_model_size,\n",
    "    strong_model_size,\n",
    "    experiment_folder,\n",
    "    DEVICE,\n",
    "    index,\n",
    ")\n",
    "\n",
    "# Print evaluations\n",
    "(\n",
    "    weak_loss,\n",
    "    _,\n",
    ") = quick_evaluation(\"weak_model\", weak_model, tictactoe_test_data)\n",
    "strong_baseline_loss, _ = quick_evaluation(\n",
    "    \"strong_baseline_model\", strong_baseline_model, tictactoe_test_data\n",
    ")\n",
    "quick_evaluation(\"strong_model\", strong_model, tictactoe_test_data)\n",
    "weak_finetuned_loss, _ = quick_evaluation(\n",
    "    \"finetuned_model\", finetuned_model, tictactoe_test_data\n",
    ")\n",
    "print(\n",
    "    \"Performance Gap Recovered (PGR): \",\n",
    "    (weak_loss - weak_finetuned_loss) / (weak_loss - strong_baseline_loss),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dictionary_learning.dictionary_learning import CrossCoder\n",
    "from dictionary_learning.dictionary_learning.dictionary import BatchTopKCrossCoder\n",
    "from dictionary_learning.dictionary_learning.trainers.crosscoder import (\n",
    "    BatchTopKCrossCoderTrainer,\n",
    ")\n",
    "from dictionary_learning.dictionary_learning.training import trainSAE\n",
    "from dictionary_learning.dictionary_learning.cache import *\n",
    "import transformer_lens.utils as utils\n",
    "\n",
    "from wsg_games.tictactoe.crosscoder.collect_activations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment_folder:  /homes/55/bwilop/wsg/experiments/\n",
      "project_name:  tictactoe/tictactoe_pretraining5\n",
      "/homes/55/bwilop/wsg/experiments/tictactoe/tictactoe_pretraining5\n",
      "Loading model from /homes/55/bwilop/wsg/experiments/tictactoe/tictactoe_pretraining5/experiment_2_medium_strong_2025-05-16-16-44_omyjjb73.pkl\n",
      "Moving model to device:  cuda\n",
      "Shards already exist in /homes/55/bwilop/wsg/experiments/tictactoe/crosscoder/activations/2_medium_strong_train/layer_3_out\n",
      "Instead, you can set overwrite=True to overwrite existing shards.\n",
      "Shards already exist in /homes/55/bwilop/wsg/experiments/tictactoe/crosscoder/activations/2_medium_strong_val/layer_3_out\n",
      "Instead, you can set overwrite=True to overwrite existing shards.\n",
      "Moving model to device:  cuda\n",
      "Shards already exist in /homes/55/bwilop/wsg/experiments/tictactoe/crosscoder/activations/2_medium_finetuned_through_small_train/layer_3_out\n",
      "Instead, you can set overwrite=True to overwrite existing shards.\n",
      "Shards already exist in /homes/55/bwilop/wsg/experiments/tictactoe/crosscoder/activations/2_medium_finetuned_through_small_val/layer_3_out\n",
      "Instead, you can set overwrite=True to overwrite existing shards.\n",
      "strong_train_activations_path /homes/55/bwilop/wsg/experiments/tictactoe/crosscoder/activations/2_medium_strong_train\n",
      "strong_val_activations_path: /homes/55/bwilop/wsg/experiments/tictactoe/crosscoder/activations/2_medium_strong_val\n",
      "finetuned_train_activations_path: /homes/55/bwilop/wsg/experiments/tictactoe/crosscoder/activations/2_medium_finetuned_through_small_train\n",
      "finetuned_val_activations_path: /homes/55/bwilop/wsg/experiments/tictactoe/crosscoder/activations/2_medium_finetuned_through_small_val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of wsg_games.tictactoe.train.train failed: Traceback (most recent call last):\n",
      "  File \"/homes/55/bwilop/wsg/WSG_games/uv_venv2/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/homes/55/bwilop/wsg/WSG_games/uv_venv2/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/usr/local/lib/python3.10/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/homes/55/bwilop/wsg/WSG_games/wsg_games/tictactoe/train/train.py\", line 18, in <module>\n",
      "    def rearrange(tensor: t.Tensor, game: Game = Game.TIC) -> t.Tensor:\n",
      "  File \"/usr/local/lib/python3.10/enum.py\", line 437, in __getattr__\n",
      "    raise AttributeError(name) from None\n",
      "AttributeError: TIC\n",
      "]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Verify\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m---> 41\u001b[0m     \u001b[43mvalidate_activations\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrong_train_activations_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/layer_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlayer\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_out\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfinetuned_train_activations_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/layer_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlayer\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_out\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     validate_activations([\n\u001b[1;32m     46\u001b[0m         strong_val_activations_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/layer_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_out\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     47\u001b[0m         finetuned_val_activations_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/layer_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_out\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     48\u001b[0m     ])\n",
      "File \u001b[0;32m~/wsg/WSG_games/wsg_games/tictactoe/crosscoder/collect_activations.py:239\u001b[0m, in \u001b[0;36mvalidate_activations\u001b[0;34m(store_dirs)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvalidate_activations\u001b[39m(store_dirs: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m    Assertions to verify that the activations of list_of_paths fit together,\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m    i.e. all were trained on same tokens and shapes are correct.\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m     activation_cache_tuples \u001b[38;5;241m=\u001b[39m \u001b[43mActivationCacheTuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore_dirs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     stacked_tokens \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    241\u001b[0m         activation_cache_tuples\u001b[38;5;241m.\u001b[39mtokens\n\u001b[1;32m    242\u001b[0m     )  \u001b[38;5;66;03m# [len(store_dir), n_games, game_length]\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     token_0 \u001b[38;5;241m=\u001b[39m stacked_tokens[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/wsg/WSG_games/dictionary_learning/dictionary_learning/cache.py:392\u001b[0m, in \u001b[0;36mActivationCacheTuple.__init__\u001b[0;34m(self, *store_dirs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mstore_dirs: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_caches \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    393\u001b[0m         ActivationCache(store_dir) \u001b[38;5;28;01mfor\u001b[39;00m store_dir \u001b[38;5;129;01min\u001b[39;00m store_dirs\n\u001b[1;32m    394\u001b[0m     ]\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_caches) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_caches)):\n",
      "File \u001b[0;32m~/wsg/WSG_games/dictionary_learning/dictionary_learning/cache.py:393\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mstore_dirs: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_caches \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 393\u001b[0m         \u001b[43mActivationCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m store_dir \u001b[38;5;129;01min\u001b[39;00m store_dirs\n\u001b[1;32m    394\u001b[0m     ]\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_caches) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_caches)):\n",
      "File \u001b[0;32m~/wsg/WSG_games/dictionary_learning/dictionary_learning/cache.py:79\u001b[0m, in \u001b[0;36mActivationCache.__init__\u001b[0;34m(self, store_dir)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, store_dir: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore_dir \u001b[38;5;241m=\u001b[39m store_dir\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshards \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     81\u001b[0m         ActivationShard(store_dir, i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshard_count\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     82\u001b[0m     ]\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_range_to_shard_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcumsum([\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m [s\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshards])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/posixpath.py:76\u001b[0m, in \u001b[0;36mjoin\u001b[0;34m(a, *p)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mjoin\u001b[39m(a, \u001b[38;5;241m*\u001b[39mp):\n\u001b[1;32m     72\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Join two or more pathname components, inserting '/' as needed.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    If any component is an absolute path, all previous path components\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    will be discarded.  An empty last part will result in a path that\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    ends with a separator.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     sep \u001b[38;5;241m=\u001b[39m _get_sep(a)\n\u001b[1;32m     78\u001b[0m     path \u001b[38;5;241m=\u001b[39m a\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not list"
     ]
    }
   ],
   "source": [
    "# Strong\n",
    "strong_train_activations_path, strong_val_activations_path = compute_activations(\n",
    "    2,\n",
    "    Goal.STRONG_GOAL,\n",
    "    project_name_pretrain,\n",
    "    None,\n",
    "    None,\n",
    "    strong_model_size,\n",
    "    index,\n",
    "    crosscoder_folder,\n",
    "    tictactoe_test_data.games_data,\n",
    "    tictactoe_val_data.games_data,\n",
    "    experiment_folder,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "# Finetuned\n",
    "finetuned_train_activations_path, finetuned_val_activations_path = compute_activations(\n",
    "    2,\n",
    "    None,\n",
    "    None,\n",
    "    weak_model_size,\n",
    "    project_name_finetune,\n",
    "    strong_model_size,\n",
    "    index,\n",
    "    crosscoder_folder,\n",
    "    tictactoe_test_data.games_data,\n",
    "    tictactoe_val_data.games_data,\n",
    "    experiment_folder,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "print(\"strong_train_activations_path\", strong_train_activations_path)\n",
    "print(\"strong_val_activations_path:\", strong_val_activations_path)\n",
    "print(\"finetuned_train_activations_path:\", finetuned_train_activations_path)\n",
    "print(\"finetuned_val_activations_path:\", finetuned_val_activations_path)\n",
    "\n",
    "# Verify\n",
    "\n",
    "for layer in range(5):\n",
    "    validate_activations(\n",
    "        [\n",
    "            strong_train_activations_path + f\"/layer_{layer}_out\",\n",
    "            finetuned_train_activations_path + f\"/layer_{layer}_out\",\n",
    "        ]\n",
    "    )\n",
    "    validate_activations(\n",
    "        [\n",
    "            strong_val_activations_path + f\"/layer_{layer}_out\",\n",
    "            finetuned_val_activations_path + f\"/layer_{layer}_out\",\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_epoch_dataloader_iterator(\n",
    "    dataloader: DataLoader, total_steps_to_yield: int\n",
    ") -> Iterator[Any]:\n",
    "    \"\"\"\n",
    "    A generator that yields batches from a DataLoader repeatedly until\n",
    "    total_steps_to_yield is reached. Re-shuffles if dataloader.shuffle=True.\n",
    "    \"\"\"\n",
    "    # Edge cases\n",
    "    if total_steps_to_yield == 0:  # No steps\n",
    "        return\n",
    "    try:\n",
    "        if len(dataloader) == 0 and total_steps_to_yield > 0:  # Empty dataloader\n",
    "            print(\n",
    "                \"Warning: DataLoader is empty, but total_steps_to_yield > 0. No steps will run.\"\n",
    "            )\n",
    "            return\n",
    "    except TypeError:  # no __len__\n",
    "        pass\n",
    "\n",
    "    steps_yielded = 0\n",
    "    while steps_yielded < total_steps_to_yield:\n",
    "        num_batches_this_epoch = 0\n",
    "        for batch in dataloader:  # DataLoader shuffles here if its shuffle=True\n",
    "            if steps_yielded >= total_steps_to_yield:\n",
    "                return\n",
    "            yield batch\n",
    "            steps_yielded += 1\n",
    "            num_batches_this_epoch += 1\n",
    "\n",
    "        # Safeguard, if the dataloader gets empty for any reason, it would be an infinite loop otherwise\n",
    "        if num_batches_this_epoch == 0 and steps_yielded < total_steps_to_yield:\n",
    "            print(\"Warning: DataLoader became empty before all steps were yielded.\")\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_cfg_cross_coder():\n",
    "    training_cfg_cross_coder = {\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"max_steps\": 20000,\n",
    "        \"validate_every_n_steps\": 1000,\n",
    "        \"batch_size\": 64,\n",
    "        \"expansion_factor\": 32,\n",
    "        \"k\": 10,\n",
    "    }\n",
    "    return training_cfg_cross_coder\n",
    "\n",
    "\n",
    "def train_crosscoder(\n",
    "    model_0_name: str,\n",
    "    model_1_name: str,\n",
    "    index: int,\n",
    "    train_activations_stor_dir_model_0: str,\n",
    "    val_activations_stor_dir_model_0: str,\n",
    "    train_activations_stor_dir_model_1: str,\n",
    "    val_activations_stor_dir_model_1: str,\n",
    "    layer: int,\n",
    "    training_cfg_cross_coder: dict,\n",
    "    wandb_entity: str,\n",
    "    device: t.device = DEVICE,\n",
    ") -> None:\n",
    "    # Save arguments\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "    experiment_name = f\"experiment_{index}_{model_0_name}_{model_1_name}_{timestamp}\"\n",
    "    save_dir = crosscoder_folder + \"checkpoints/\" + experiment_name\n",
    "    train_crosscoder_args = {\n",
    "        \"model_0_name\": model_0_name,\n",
    "        \"model_1_name\": model_1_name,\n",
    "        \"index\": index,\n",
    "        \"layer\": layer,\n",
    "        \"training_cfg_cross_coder\": training_cfg_cross_coder,\n",
    "        \"data_path\": {\n",
    "            \"train_activations_stor_dir_model_0\": train_activations_stor_dir_model_0,\n",
    "            \"val_activations_stor_dir_model_0\": val_activations_stor_dir_model_0,\n",
    "            \"train_activations_stor_dir_model_1\": train_activations_stor_dir_model_1,\n",
    "            \"val_activations_stor_dir_model_1\": val_activations_stor_dir_model_1,\n",
    "        },\n",
    "    }\n",
    "    save_dir = os.path.join(crosscoder_folder, \"checkpoints\", experiment_name)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    with open(\n",
    "        os.path.join(save_dir, \"train_crosscoder_args.json\"), \"w\", encoding=\"utf-8\"\n",
    "    ) as json_file:\n",
    "        json.dump(train_crosscoder_args, json_file)\n",
    "\n",
    "    # Data (not loaded in memory yet)\n",
    "    train_dataset = PairedActivationCache(\n",
    "        train_activations_stor_dir_model_0,\n",
    "        train_activations_stor_dir_model_1,\n",
    "    )\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=training_cfg_cross_coder[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    print(f\"Training on {len(train_dataset)} token activations.\")\n",
    "    val_dataset = PairedActivationCache(\n",
    "        val_activations_stor_dir_model_0,\n",
    "        val_activations_stor_dir_model_1,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=1000,\n",
    "        shuffle=False,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    print(f\"Validating on {len(val_dataset)} token activations.\")\n",
    "\n",
    "    # Training config\n",
    "    activation_dim = train_dataset[0].shape[1]\n",
    "    dictionary_size = training_cfg_cross_coder[\"expansion_factor\"] * activation_dim\n",
    "    print(f\"Activation dim: {activation_dim}\")\n",
    "    print(f\"Dictionary size: {dictionary_size}\")\n",
    "    k = training_cfg_cross_coder[\"k\"]\n",
    "    lr = training_cfg_cross_coder[\"learning_rate\"]\n",
    "    max_steps = training_cfg_cross_coder[\"max_steps\"]\n",
    "\n",
    "    # Top level of trainer_cfg: BatchTopKCrossCoderTrainer\n",
    "    # dict_class_kwargs: CrossCoder\n",
    "    trainer_cfg = {\n",
    "        \"trainer\": BatchTopKCrossCoderTrainer,\n",
    "        \"activation_dim\": activation_dim,\n",
    "        \"dict_size\": dictionary_size,\n",
    "        \"lr\": lr,\n",
    "        \"device\": str(device),\n",
    "        \"wandb_name\": experiment_name + f\"L{layer}-k{k:.1e}-lr{lr:.0e}\",  #\n",
    "        \"steps\": max_steps,  # Also used below, I think because it is needed here for learning rate and in trainSAE just for loop termination\n",
    "        \"k\": k,  # 'k' as a top-level argument for the trainer\n",
    "        \"layer\": layer,  # Only for logging\n",
    "        \"lm_name\": experiment_name,  # Only for logging\n",
    "        # \"dict_class_kwargs\": {\n",
    "        # },\n",
    "    }\n",
    "    # train the sparse autoencoder (SAE)\n",
    "    wandb.finish()\n",
    "    multi_epoch_train_dataloader = multi_epoch_dataloader_iterator(\n",
    "        train_dataloader, max_steps\n",
    "    )\n",
    "    trainSAE(\n",
    "        data=multi_epoch_train_dataloader,\n",
    "        trainer_config=trainer_cfg,\n",
    "        validate_every_n_steps=training_cfg_cross_coder[\"validate_every_n_steps\"],\n",
    "        validation_data=val_dataloader,\n",
    "        use_wandb=True,\n",
    "        wandb_entity=wandb_entity,\n",
    "        wandb_project=\"crosscoder\",\n",
    "        log_steps=50,\n",
    "        save_dir=save_dir,\n",
    "        steps=max_steps,\n",
    "        save_steps=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_activations_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model_0_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrong_model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m model_1_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinetuned_model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m train_activations_stor_dir_model_0 \u001b[38;5;241m=\u001b[39m \u001b[43mget_activations_path\u001b[49m(\n\u001b[1;32m      7\u001b[0m     Goal\u001b[38;5;241m.\u001b[39mSTRONG_GOAL, \u001b[38;5;28;01mNone\u001b[39;00m, strong_model_size, index, crosscoder_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m val_activations_stor_dir_model_0 \u001b[38;5;241m=\u001b[39m get_activations_path(\n\u001b[1;32m     10\u001b[0m     Goal\u001b[38;5;241m.\u001b[39mSTRONG_GOAL, \u001b[38;5;28;01mNone\u001b[39;00m, strong_model_size, index, crosscoder_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m train_activations_stor_dir_model_1 \u001b[38;5;241m=\u001b[39m get_activations_path(\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m, weak_model_size, strong_model_size, index, crosscoder_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_activations_path' is not defined"
     ]
    }
   ],
   "source": [
    "layer = 3\n",
    "\n",
    "model_0_name = \"strong_model\"\n",
    "model_1_name = \"finetuned_model\"\n",
    "\n",
    "train_activations_stor_dir_model_0 = get_activations_path(\n",
    "    Goal.STRONG_GOAL, None, strong_model_size, index, crosscoder_folder, \"train\"\n",
    ")\n",
    "val_activations_stor_dir_model_0 = get_activations_path(\n",
    "    Goal.STRONG_GOAL, None, strong_model_size, index, crosscoder_folder, \"val\"\n",
    ")\n",
    "train_activations_stor_dir_model_1 = get_activations_path(\n",
    "    None, weak_model_size, strong_model_size, index, crosscoder_folder, \"train\"\n",
    ")\n",
    "val_activations_stor_dir_model_1 = get_activations_path(\n",
    "    None, weak_model_size, strong_model_size, index, crosscoder_folder, \"val\"\n",
    ")\n",
    "training_cfg_cross_coder = get_training_cfg_cross_coder()\n",
    "# train_crosscoder(\n",
    "#     model_0_name,\n",
    "#     model_1_name,\n",
    "#     index,\n",
    "#     train_activations_stor_dir_model_0 + f\"/layer_{layer}_out\",\n",
    "#     val_activations_stor_dir_model_0 + f\"/layer_{layer}_out\",\n",
    "#     train_activations_stor_dir_model_1 + f\"/layer_{layer}_out\",\n",
    "#     val_activations_stor_dir_model_1 + f\"/layer_{layer}_out\",\n",
    "#     layer,\n",
    "#     training_cfg_cross_coder,\n",
    "#     WANDB_ENTITIY,\n",
    "#     DEVICE,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BatchTopKCrossCoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mCrosscoderMetrics\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     save_dir: \u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m      4\u001b[0m     device: t\u001b[38;5;241m.\u001b[39mdevice\n",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m, in \u001b[0;36mCrosscoderMetrics\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m train_crosscoder_args: \u001b[38;5;28mdict\u001b[39m\n\u001b[1;32m      6\u001b[0m config: \u001b[38;5;28mdict\u001b[39m\n\u001b[0;32m----> 7\u001b[0m crosscoder: \u001b[43mBatchTopKCrossCoder\u001b[49m\n\u001b[1;32m      8\u001b[0m delta_norms: Float[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_features\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      9\u001b[0m beta_reconstruction_model_0: Float[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_features\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BatchTopKCrossCoder' is not defined"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class CrosscoderMetrics:\n",
    "    save_dir: str\n",
    "    device: t.device\n",
    "    train_crosscoder_args: dict\n",
    "    config: dict\n",
    "    crosscoder: BatchTopKCrossCoder\n",
    "    delta_norms: Float[Tensor, \"n_features\"]\n",
    "    beta_reconstruction_model_0: Float[Tensor, \"n_features\"]\n",
    "    beta_reconstruction_model_1: Float[Tensor, \"n_features\"]\n",
    "    beta_error_model_0: Float[Tensor, \"n_features\"]\n",
    "    beta_error_model_1: Float[Tensor, \"n_features\"]\n",
    "    nu_reconstruction: Float[Tensor, \"n_features\"]\n",
    "    nu_error: Float[Tensor, \"n_features\"]\n",
    "    top_n_activations: Float[Tensor, \"n_features top_n\"]\n",
    "\n",
    "    def __init__(self, save_dir: str, device: t.device) -> None:\n",
    "        self.save_dir = save_dir\n",
    "        self.device = device\n",
    "        self.train_crosscoder_args = self.load_train_crosscoder_args(save_dir)\n",
    "        self.config = self.load_config(self.save_dir)\n",
    "        self.crosscoder = self.load_model(self.save_dir, device)\n",
    "        self.delta_norms = self.compute_delta_norms()\n",
    "        self.beta_reconstruction_model_0, self.beta_error_model_0 = self.compute_beta(\n",
    "            model_i=0, device=self.device\n",
    "        )\n",
    "        self.beta_reconstruction_model_1, self.beta_error_model_1 = self.compute_beta(\n",
    "            model_i=1, device=self.device\n",
    "        )\n",
    "        self.nu_reconstruction = (\n",
    "            self.beta_reconstruction_model_0 / self.beta_reconstruction_model_1\n",
    "        )\n",
    "        self.nu_error = self.beta_error_model_0 / self.beta_error_model_1\n",
    "        self.top_n_activations = self.compute_top_n_activations(top_n=9)\n",
    "        self.plot_delta_norms()\n",
    "        self.plot_betas()\n",
    "        self.plot_nu()\n",
    "\n",
    "    # Save + Load\n",
    "    def save(self, save_dir: str) -> None:\n",
    "        path = os.path.join(save_dir, \"crosscoder_metric.pt\")\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    def _load_helper(path: str):\n",
    "        if os.path.exists(path):\n",
    "            with open(path, \"rb\") as f:\n",
    "                data = pickle.load(f)\n",
    "            return data\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"File {path} not found.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load(save_dir: str):  # -> CrosscoderMetrics\n",
    "        path = os.path.join(save_dir, \"crosscoder_metric.pt\")\n",
    "        return CrosscoderMetrics._load_helper(path)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_train_crosscoder_args(save_dir: str) -> dict:\n",
    "        path = os.path.join(save_dir, \"train_crosscoder_args.json\")\n",
    "        if os.path.exists(path):\n",
    "            with open(path, \"r\") as f:\n",
    "                return json.load(f)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"File {path} not found.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load_config(save_dir: str) -> dict:\n",
    "        path = os.path.join(save_dir, \"config.json\")\n",
    "        if os.path.exists(path):\n",
    "            with open(path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "            return data.get(\"trainer\", {})\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"File {path} not found.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(save_dir: str, device) -> CrossCoder:\n",
    "        path = os.path.join(save_dir, \"model_final.pt\")\n",
    "        if os.path.exists(path):\n",
    "            return BatchTopKCrossCoder.from_pretrained(path, device=device)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"File {path} not found.\")\n",
    "\n",
    "    def compute_delta_norms(\n",
    "        self, epsilon: float = 0.0001\n",
    "    ) -> Float[Tensor, \"n_activations\"]:\n",
    "        d_model_0_vectors = self.crosscoder.decoder.weight[0]\n",
    "        d_model_1_vectors = self.crosscoder.decoder.weight[1]\n",
    "        norm_sq_model1 = t.sum(d_model_0_vectors**2, dim=-1)\n",
    "        norm_sq_model2 = t.sum(d_model_1_vectors**2, dim=-1)\n",
    "        max_norms_sq = t.maximum(norm_sq_model1, norm_sq_model2)\n",
    "\n",
    "        delta_norms = 0.5 * (\n",
    "            (norm_sq_model2 - norm_sq_model1) / (max_norms_sq + epsilon) + 1.0\n",
    "        )\n",
    "        return delta_norms.cpu()\n",
    "\n",
    "    def _activations_both_generator(self, tqdm_desc) -> Iterator[t.Tensor]:\n",
    "        val_dataset = PairedActivationCache(\n",
    "            self.train_crosscoder_args[\"data_path\"][\"val_activations_stor_dir_model_0\"],\n",
    "            self.train_crosscoder_args[\"data_path\"][\"val_activations_stor_dir_model_1\"],\n",
    "        )\n",
    "        val_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=1000,\n",
    "            shuffle=False,\n",
    "            num_workers=1,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        for activations_both in tqdm(val_dataloader, desc=tqdm_desc):\n",
    "            # Move activations_both to device\n",
    "            activations_model_0_dev = activations_both[0].to(self.device)\n",
    "            activations_model_1_dev = activations_both[1].to(self.device)\n",
    "            activations_both = t.stack(\n",
    "                [activations_model_0_dev, activations_model_1_dev], dim=1\n",
    "            )\n",
    "            yield activations_both\n",
    "\n",
    "    def compute_beta(\n",
    "        self, model_i: int, device: t.device\n",
    "    ) -> tuple[Float[Tensor, \"n_features\"], Float[Tensor, \"n_features\"]]:\n",
    "        \"\"\"\n",
    "        beta = (d.T @ (Y.T @ F)) / (||d||^2 * ||f||^2)\n",
    "        \"\"\"\n",
    "        D = self.crosscoder.decoder.weight[model_i]\n",
    "        D_chat_norms_sq = D.square().sum(dim=1)  # ||D_j||_2, [dictionary_size]\n",
    "\n",
    "        # Accumulators for sums over all samples, for each latent j.\n",
    "        # numer_r[j] = sum_samples_i ( F[i,j] * <d_j, Y_reconstruction_target[i,:]> )\n",
    "        # numer_e[j] = sum_samples_i ( F[i,j] * <d_j, Y_error_target[i,:]> )\n",
    "        # sum_F_sq[j] = sum_samples_i ( F[i,j]^2 ) (this is ||f_j||^2 from the formula)\n",
    "        dict_size = D.shape[0]\n",
    "        numer_r = t.zeros(dict_size, device=device, dtype=t.float32)  # [dict_size]\n",
    "        numer_e = t.zeros_like(numer_r)  # [dict_size]\n",
    "        sum_F_sq = t.zeros_like(numer_r)  # [dict_size]\n",
    "\n",
    "        self.crosscoder.eval()\n",
    "        with t.no_grad():\n",
    "            for activations_both in self._activations_both_generator(\n",
    "                f\"Computing beta of model {model_i + 1}\"\n",
    "            ):\n",
    "                reconstructions_both, F_batch = self.crosscoder.forward(\n",
    "                    activations_both, output_features=True\n",
    "                )\n",
    "                activations = activations_both[\n",
    "                    :, model_i, :\n",
    "                ]  # [batch_size, activation_dim]\n",
    "                reconstructions = reconstructions_both[\n",
    "                    :, model_i, :\n",
    "                ]  # [batch_size, activation_dim]\n",
    "\n",
    "                # F_batch: sparse feature activations from sae.encode.\n",
    "                sum_F_sq += F_batch.square().sum(\n",
    "                    dim=0\n",
    "                )  # [dict_size] (F_batch is [batch_size, dict_size])\n",
    "\n",
    "                # Determine target Y for reconstruction (Y_r) and error (Y_e)\n",
    "                # Y_r, Y_e have shape (batch_size, activation_dim)\n",
    "                Y_r = reconstructions\n",
    "                Y_e = activations - Y_r\n",
    "\n",
    "                # Accumulate numerators: sum_i F[i,j] * <d_j, Y_target[i,:]>\n",
    "                # (Y_target @ D.T) gives <Y_target[i,:], d_j> for each sample i and latent j  # [batch_size, dict_size]\n",
    "                numer_r += (F_batch * (Y_r @ D.T)).sum(dim=0)  # [dict_size]\n",
    "                numer_e += (F_batch * (Y_e @ D.T)).sum(dim=0)  # [dict_size]\n",
    "\n",
    "        # Denominator for beta_j = ||f_j||^2 * ||d_j||^2)\n",
    "        denominators = sum_F_sq * D_chat_norms_sq  # [dict_size]\n",
    "\n",
    "        beta_r = t.nan_to_num(numer_r / denominators, nan=1.0, posinf=1.0, neginf=1.0)\n",
    "        beta_e = t.nan_to_num(numer_e / denominators, nan=1.0, posinf=1.0, neginf=1.0)\n",
    "\n",
    "        return beta_r, beta_e\n",
    "\n",
    "    def compute_top_n_activations(top_n: int) -> Float[Tensor, \"n_features top_n\"]:\n",
    "        \"\"\"\n",
    "        Creates tensor that maps from feature j to the indicies of the top n activations of that feature.\n",
    "        The indices are for # TODO\n",
    "        \"\"\"\n",
    "\n",
    "    def get_delta_thresholds(\n",
    "        self, threshold_only: float = 0.1, threshold_shared: float = 0.1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Returns the thresholds for delta norms based on the provided parameters.\n",
    "        \"\"\"\n",
    "        threshold_model_0_only = threshold_only\n",
    "        threshold_model_1_only = 1 - threshold_only\n",
    "        threshold_shared_lower = 0.5 - threshold_shared\n",
    "        threshold_shared_upper = 0.5 + threshold_shared\n",
    "        assert (\n",
    "            threshold_model_0_only\n",
    "            < threshold_shared_lower\n",
    "            < threshold_shared_upper\n",
    "            < threshold_model_1_only\n",
    "        ), (\n",
    "            f\"Invalid thresholds: {threshold_model_0_only}, {threshold_shared_lower}, {threshold_shared_upper}, {threshold_model_1_only}\"\n",
    "        )\n",
    "        return (\n",
    "            threshold_model_0_only,\n",
    "            threshold_model_1_only,\n",
    "            threshold_shared_lower,\n",
    "            threshold_shared_upper,\n",
    "        )\n",
    "\n",
    "    def plot_delta_norms(self) -> tuple[plt.Figure, list[plt.Axes]]:\n",
    "        \"\"\"\n",
    "        Plots a histogram of delta norms with two subplots: one with linear y-axis and one with logarithmic y-axis.\n",
    "        Linear same as in https://arxiv.org/pdf/2504.02922\n",
    "        \"\"\"\n",
    "        (\n",
    "            threshold_model_0_only,\n",
    "            threshold_model_1_only,\n",
    "            threshold_shared_lower,\n",
    "            threshold_shared_upper,\n",
    "        ) = self.get_delta_thresholds()\n",
    "\n",
    "        # Data\n",
    "        data_to_plot = self.delta_norms.detach().numpy()\n",
    "        bins = np.linspace(0, 1, 101)  # 100 bins from 0 to 1, [0, 0.01, 0.02, ..., 1.0]\n",
    "\n",
    "        # Plot\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        fig.suptitle(\n",
    "            f\"Histogram of Delta Norms (Features: {len(data_to_plot)})\", fontsize=16\n",
    "        )\n",
    "        titles = [\"Linear Y-Axis\", \"Logarithmic Y-Axis\"]\n",
    "        y_scales = [\"linear\", \"log\"]\n",
    "        for i, ax in enumerate(axes):\n",
    "            counts, _, patches = ax.hist(\n",
    "                data_to_plot, bins=bins, edgecolor=\"black\", alpha=0.7\n",
    "            )\n",
    "            bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "            for count, patch, bin_center in zip(counts, patches, bin_centers):\n",
    "                if bin_center <= threshold_model_0_only:\n",
    "                    patch.set_facecolor(\"green\")\n",
    "                elif bin_center >= threshold_model_1_only:\n",
    "                    patch.set_facecolor(\"lightblue\")\n",
    "                elif threshold_shared_lower <= bin_center <= threshold_shared_upper:\n",
    "                    patch.set_facecolor(\"orange\")\n",
    "                else:\n",
    "                    patch.set_facecolor(\"grey\")\n",
    "\n",
    "            ax.set_title(titles[i])\n",
    "            ax.set_xlabel(\"Delta Norm Value\")\n",
    "            ax.set_ylabel(\"Number of Features\")\n",
    "            ax.set_yscale(y_scales[i])\n",
    "            if y_scales[i] == \"log\":\n",
    "                ax.set_ylim(bottom=0.1)  # Avoid log(0)\n",
    "            ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "        # Save plot\n",
    "        filename = f\"delta_norms.png\"\n",
    "        if self.save_dir:\n",
    "            os.makedirs(self.save_dir, exist_ok=True)\n",
    "            save_path = os.path.join(self.save_dir, filename)\n",
    "            try:\n",
    "                fig.savefig(save_path)\n",
    "                print(f\"Delta norms plot saved to {save_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving plot: {e}\")\n",
    "\n",
    "        return fig, axes\n",
    "\n",
    "    def plot_betas(self) -> tuple[plt.Figure, list[plt.Axes]]:\n",
    "        \"\"\"Plots a 2x2 grid of histograms for beta values.\"\"\"\n",
    "        beta_data = {\n",
    "            r\"$\\beta^r$ Model 0 (Reconstruction)\": self.beta_reconstruction_model_0.cpu()\n",
    "            .detach()\n",
    "            .numpy(),\n",
    "            r\"$\\beta^\\epsilon$ Model 0 (Error)\": self.beta_error_model_0.cpu()\n",
    "            .detach()\n",
    "            .numpy(),\n",
    "            r\"$\\beta^r$ Model 1 (Reconstruction)\": self.beta_reconstruction_model_1.cpu()\n",
    "            .detach()\n",
    "            .numpy(),\n",
    "            r\"$\\beta^\\epsilon$ Model 1 (Error)\": self.beta_error_model_1.cpu()\n",
    "            .detach()\n",
    "            .numpy(),\n",
    "        }\n",
    "\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle(\"Histograms of Beta Values\", fontsize=16)\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        for i, (title, data) in enumerate(beta_data.items()):\n",
    "            ax = axes[i]\n",
    "            if len(data) > 0:\n",
    "                ax.hist(data, bins=50, edgecolor=\"black\", alpha=0.7)\n",
    "                ax.set_title(title)\n",
    "                ax.set_xlabel(\"Beta Value\")\n",
    "                ax.set_ylabel(\"Number of Features\")\n",
    "                ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "                # ax.set_yscale('log')\n",
    "                ax.set_ylim(bottom=0.1)\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\")\n",
    "                ax.set_title(title)\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        if self.save_dir:\n",
    "            filename = \"betas_histogram.png\"\n",
    "            os.makedirs(self.save_dir, exist_ok=True)\n",
    "            save_path = os.path.join(self.save_dir, filename)\n",
    "            try:\n",
    "                fig.savefig(save_path)\n",
    "                print(f\"Betas plot saved to {save_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving betas plot: {e}\")\n",
    "        # plt.show() # Uncomment to display\n",
    "        return fig, axes\n",
    "\n",
    "    def plot_nu(self):\n",
    "        \"\"\"\n",
    "        Plots nu_error vs nu_reconstruction, colored by delta_norm categories,\n",
    "        Same as in https://arxiv.org/pdf/2504.02922\n",
    "        \"\"\"\n",
    "        # Data\n",
    "        nu_e_data = self.nu_error.cpu().detach().numpy()\n",
    "        nu_r_data = self.nu_reconstruction.cpu().detach().numpy()\n",
    "        delta_norms_data = self.delta_norms.cpu().detach().numpy()\n",
    "\n",
    "        # Categories for each feature $j$\n",
    "        (\n",
    "            threshold_model_0_only,\n",
    "            threshold_model_1_only,\n",
    "            threshold_shared_lower,\n",
    "            threshold_shared_upper,\n",
    "        ) = self.get_delta_thresholds()\n",
    "        categories = []\n",
    "        for dn in delta_norms_data:\n",
    "            if dn <= threshold_model_0_only:\n",
    "                categories.append(\"Model 0 Only\")\n",
    "            elif dn >= threshold_model_1_only:\n",
    "                categories.append(\"Model 1 Only\")\n",
    "            elif threshold_shared_lower <= dn <= threshold_shared_upper:\n",
    "                categories.append(\"Shared\")\n",
    "            else:\n",
    "                categories.append(\"Other\")\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                r\"$\\nu^\\epsilon$ (Error)\": nu_e_data,\n",
    "                r\"$\\nu^r$ (Reconstruction)\": nu_r_data,\n",
    "                \"Category\": categories,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Plot\n",
    "        palette = {\n",
    "            \"Model 0 Only\": \"cornflowerblue\",\n",
    "            \"Shared\": \"grey\",\n",
    "            \"Model 1 Only\": \"darkorange\",\n",
    "            \"Other\": \"lightgrey\",\n",
    "        }\n",
    "        categories_to_display = [\"Model 0 Only\", \"Model 1 Only\"]\n",
    "        df_plot = df[\n",
    "            df[\"Category\"].isin(categories_to_display)\n",
    "        ].copy()  # Use .copy() to avoid SettingWithCopyWarning\n",
    "        if df_plot.empty:\n",
    "            print(\"No data to plot for the selected categories in plot_nu.\")\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.text(\n",
    "                0.5, 0.5, \"No data for selected categories\", ha=\"center\", va=\"center\"\n",
    "            )\n",
    "            if self.save_dir:\n",
    "                plt.savefig(os.path.join(self.save_dir, \"nu_plot_nodata.png\"))\n",
    "            return fig  # Return an empty figure or handle appropriately\n",
    "\n",
    "        g = sns.JointGrid(\n",
    "            data=df_plot,\n",
    "            x=r\"$\\nu^\\epsilon$ (Error)\",\n",
    "            y=r\"$\\nu^r$ (Reconstruction)\",\n",
    "            hue=\"Category\",  # Hue is set on the grid\n",
    "            height=7,\n",
    "            ratio=5,  # Ratio of joint plot size to marginal plot size\n",
    "            space=0.1,  # Space between joint and marginal plots\n",
    "        )\n",
    "        g.plot_joint(sns.scatterplot, palette=palette, s=40, alpha=0.5, legend=False)\n",
    "        g.plot_marginals(\n",
    "            sns.histplot,\n",
    "            palette=palette,\n",
    "            bins=50,\n",
    "            multiple=\"stack\",\n",
    "            alpha=0.7,\n",
    "            legend=False,\n",
    "        )\n",
    "        g.set_axis_labels(r\"$\\nu^\\epsilon$\", r\"$\\nu^r$\", fontsize=20)\n",
    "        plt.suptitle(\n",
    "            r\"$\\nu^r$ vs $\\nu^\\epsilon$ by Feature Category\", fontsize=16, y=1.02\n",
    "        )\n",
    "        ordered_cats_in_plot = [\n",
    "            cat for cat in categories_to_display if cat in df_plot[\"Category\"].unique()\n",
    "        ]\n",
    "        handles = [\n",
    "            plt.Rectangle((0, 0), 1, 1, color=palette[cat])\n",
    "            for cat in ordered_cats_in_plot\n",
    "            if cat in palette\n",
    "        ]\n",
    "        labels = ordered_cats_in_plot\n",
    "        if handles:\n",
    "            g.fig.legend(\n",
    "                handles,\n",
    "                labels,\n",
    "                title=\"Feature Category\",\n",
    "                loc=\"upper right\",\n",
    "                bbox_to_anchor=(0.99, 0.97),\n",
    "            )\n",
    "\n",
    "        # Style adjustments\n",
    "        g.ax_joint.grid(True, linestyle=\"-\", alpha=0.3, color=\"gray\")\n",
    "        g.ax_marg_x.grid(True, linestyle=\"-\", alpha=0.3, color=\"gray\")\n",
    "        g.ax_marg_y.grid(True, linestyle=\"-\", alpha=0.3, color=\"gray\")\n",
    "        g.ax_joint.tick_params(axis=\"both\", which=\"major\", labelsize=14)\n",
    "\n",
    "        # g.ax_joint.set_xlim(0, 1.0)\n",
    "        # g.ax_joint.set_ylim(0, 1.0)\n",
    "\n",
    "        # Save plot\n",
    "        if self.save_dir:\n",
    "            filename = \"nu_plot_jointgrid.png\"\n",
    "            os.makedirs(self.save_dir, exist_ok=True)\n",
    "            save_path = os.path.join(self.save_dir, filename)\n",
    "            try:\n",
    "                # Use bbox_inches='tight' to prevent labels from being cut off\n",
    "                g.fig.savefig(save_path, bbox_inches=\"tight\", dpi=150)\n",
    "                print(f\"Nu plot saved to {save_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving nu plot: {e}\")\n",
    "\n",
    "        return g.fig\n",
    "\n",
    "\n",
    "save_dir = \"/homes/55/bwilop/wsg/experiments/tictactoe/crosscoder/checkpoints/experiment_2_strong_model_finetuned_model_2025-05-24-15-40\"\n",
    "val_dataloader = DataLoader(\n",
    "    tictactoe_val_data.games_data,\n",
    "    batch_size=1000,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n",
    "crosscoder_metrics = CrosscoderMetrics(save_dir, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (uv_venv2)",
   "language": "python",
   "name": "uv_venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
