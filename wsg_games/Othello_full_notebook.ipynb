{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtG8emy5-59z"
      },
      "source": [
        "# --- Preliminary ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZn51HV6BxlF"
      },
      "source": [
        "### Google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Xpul_ZBLjgq9"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade ipython ipykernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYcvcIePBo6N",
        "outputId": "1d5834f1-d587-4595-9c0d-29c753e17365"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbenwilop\u001b[0m (\u001b[33mbenwilop-rwth-aachen-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "\n",
        "!pip install python-dotenv --quiet\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import getpass\n",
        "import dotenv\n",
        "import wandb\n",
        "import os\n",
        "dotenv.load_dotenv(os.path.join('/content/drive/MyDrive/Colab Notebooks', 'wandbkey.env.txt'))\n",
        "api_key = os.getenv('WANDB_API_KEY')\n",
        "wandb.login(key=api_key)\n",
        "\n",
        "project_folder = '/content/drive/MyDrive/Colab Notebooks/WSG_masterthesis'\n",
        "data_folder = os.path.join(project_folder, 'data')\n",
        "experiment_folder = os.path.join(project_folder, 'experiments/PAPER_SWEEP_2')\n",
        "\n",
        "import torch as t\n",
        "DEVICE = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JSymT_uB2UA"
      },
      "source": [
        "### Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLcvzUNSlyTc",
        "outputId": "1e5593bd-3db4-4392-cd68-2202615d0f47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (5.9.5)\n",
            "Requirement already satisfied: pgn in /usr/local/lib/python3.12/dist-packages (0.1.0)\n",
            "Requirement already satisfied: transformer_lens in /usr/local/lib/python3.12/dist-packages (2.16.1)\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (1.10.1)\n",
            "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.14.1)\n",
            "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: datasets>=2.7.1 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.0.0)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.8.1)\n",
            "Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.3.2)\n",
            "Requirement already satisfied: numpy<2,>=1.26 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (2.2.2)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (13.9.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.2.1)\n",
            "Requirement already satisfied: torch>=2.6 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.67.1)\n",
            "Requirement already satisfied: transformers>=4.51 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.56.1)\n",
            "Requirement already satisfied: transformers-stream-generator<0.0.6,>=0.0.5 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.0.5)\n",
            "Requirement already satisfied: typeguard<5.0,>=4.2 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.15.0)\n",
            "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.21.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer_lens) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer_lens) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.35.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2025.3.0)\n",
            "Requirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from jaxtyping>=0.2.11->transformer_lens) (0.1.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->transformer_lens) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->transformer_lens) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->transformer_lens) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.6.0->transformer_lens) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.6.0->transformer_lens) (2.19.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.51->transformer_lens) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.51->transformer_lens) (0.22.0)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (4.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (2.11.9)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (2.38.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (3.12.15)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.23.0->transformer_lens) (1.1.10)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->transformer_lens) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.6->transformer_lens) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.6->transformer_lens) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (1.20.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.2)\n",
            "Requirement already satisfied: eindex-callum in /usr/local/lib/python3.12/dist-packages (0.1.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from eindex-callum) (2.8.0+cu126)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from eindex-callum) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->eindex-callum) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->eindex-callum) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->eindex-callum) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->eindex-callum) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->eindex-callum) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->eindex-callum) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->eindex-callum) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->eindex-callum) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->eindex-callum) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->eindex-callum) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->eindex-callum) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->eindex-callum) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->eindex-callum) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->eindex-callum) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->eindex-callum) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->eindex-callum) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->eindex-callum) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->eindex-callum) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->eindex-callum) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->eindex-callum) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->eindex-callum) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->eindex-callum) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->eindex-callum) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->eindex-callum) (3.0.2)\n",
            "Requirement already satisfied: pgn in /usr/local/lib/python3.12/dist-packages (0.1.0)\n",
            "Requirement already satisfied: jaxtyping in /usr/local/lib/python3.12/dist-packages (0.3.2)\n",
            "Requirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from jaxtyping) (0.1.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install psutil\n",
        "!pip install pgn\n",
        "!pip install transformer_lens\n",
        "!pip install eindex-callum\n",
        "!pip install pgn\n",
        "!pip install jaxtyping\n",
        "# !pip install eindex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZkJ22nhomChk"
      },
      "outputs": [],
      "source": [
        "# !pip install eindex\n",
        "# ! pip install eindex-callum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4h1qIe-bzic"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WQe8iS3qCVOj"
      },
      "outputs": [],
      "source": [
        "# standard library\n",
        "import gc\n",
        "import pickle\n",
        "from dataclasses import dataclass\n",
        "from math import floor, ceil\n",
        "import math\n",
        "from typing import Optional, List\n",
        "from copy import copy, deepcopy\n",
        "from datetime import datetime\n",
        "from enum import Enum\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "import multiprocessing\n",
        "import itertools\n",
        "import random\n",
        "import logging\n",
        "import sys\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from enum import Enum\n",
        "\n",
        "# miscellaneous\n",
        "import psutil\n",
        "import pgn\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "from scipy import stats\n",
        "\n",
        "# tensors\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch as t\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset, Subset\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from jaxtyping import Bool, Float, Int\n",
        "import einops\n",
        "from eindex import eindex\n",
        "\n",
        "# transformer_lens\n",
        "from transformer_lens import ActivationCache, HookedTransformer, HookedTransformerConfig\n",
        "from transformer_lens.hook_points import HookPoint\n",
        "from transformer_lens.utils import download_file_from_hf, get_act_name, to_numpy\n",
        "from transformer_lens.pretrained.weight_conversions.mingpt import convert_mingpt_weights\n",
        "from transformer_lens.utilities.devices import move_to_and_update_config\n",
        "\n",
        "# visuals\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUHIGkumd3iy"
      },
      "source": [
        "# --- /data ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlBlZGS11HFE"
      },
      "source": [
        "### Data description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXfCTXb6xQ_s"
      },
      "source": [
        "- The vocabulary size is 61, because we allow any of the 8x8 - 4 = 60 unoccupied squares to be played, plus the pass move. The vocab is ordered `pass, A0, A1, ..., H7`. Note that we'll be filtering out games where a `pass` move was played, so we don't need to worry about this.\n",
        "- We'll refer to squares in 3 different ways:\n",
        "    1. **label** - this is the string representation, i.e. `\"pass\"`, `\"A0\"`, `\"A1\"`, ..., `\"H7\"`.\n",
        "    2. **token id**, or **id** - this is the token ID in the model's vocab, i.e. `1` for A0, ..., `60` for H7. We skip `0` which is the token id for `pass`, and we skip the 4 middle squares since they're always occupied and so there are no moves in or predictions for these squares.\n",
        "    3. **square index**, or **square** - this is the zero-indexed value of the square in the size-64 board, i.e. `0` for A0, `1` for A1, ..., `63` for H7.\n",
        "- Black plays first in Othello, and so (in games with no passes) White plays last. Since we don't predict the very first move, this means the model's predictions are for (white 1, black 2, white 2, ..., white 29, black 30, white 30).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJBpHN2keuz_"
      },
      "source": [
        "### othello_board_state.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Y-_2C-Zce2zQ"
      },
      "outputs": [],
      "source": [
        "rows = list(\"abcdefgh\")\n",
        "columns = [str(_) for _ in range(1, 9)]\n",
        "eights = [[-1, 0], [-1, 1], [0, 1], [1, 1], [1, 0], [1, -1], [0, -1], [-1, -1]]\n",
        "fours = [[-1, 0], [1, 0], [0, 1], [0, -1]]\n",
        "\n",
        "def permit(s):\n",
        "    \"\"\"Chess notation to board index: a3 -> 2, A3 -> 2, b3 -> 10\"\"\"\n",
        "    s = s.lower()\n",
        "    if len(s) != 2:\n",
        "        return -1\n",
        "    if s[0] not in rows or s[1] not in columns:\n",
        "        return -1\n",
        "    return rows.index(s[0]) * 8 + columns.index(s[1])\n",
        "\n",
        "def permit_reverse(integer):\n",
        "    \"\"\"Board index to chess notation: 2 -> a3, 10 -> b3\"\"\"\n",
        "    r, c = integer // 8, integer % 8\n",
        "    return \"\".join([rows[r], columns[c]])\n",
        "\n",
        "class OthelloBoardState():\n",
        "    # 1 is black (X), -1 is white (O)\n",
        "    def __init__(self, board_size = 8):\n",
        "        self.board_size = board_size * board_size\n",
        "        board = np.zeros((8, 8))\n",
        "        board[3, 4] = 1\n",
        "        board[3, 3] = -1\n",
        "        board[4, 3] = 1\n",
        "        board[4, 4] = -1\n",
        "        self.initial_state = board\n",
        "        self.state = self.initial_state\n",
        "        self.age = np.zeros((8, 8))\n",
        "        self.next_hand_color = 1\n",
        "        self.history = []\n",
        "\n",
        "    def get_occupied(self):\n",
        "        \"\"\"List, 1 if occupied, 0 if not.\"\"\"\n",
        "        board = self.state\n",
        "        tbr = board.flatten() != 0\n",
        "        return tbr.tolist()\n",
        "\n",
        "    def get_state(self):\n",
        "        \"\"\"List, white 0, blank 1, black 2.\"\"\"\n",
        "        board = self.state + 1  # white 0, blank 1, black 2\n",
        "        tbr = board.flatten()\n",
        "        return tbr.tolist()\n",
        "\n",
        "    def get_age(self):\n",
        "        \"\"\"List, round index where stone was placed, 0 else.\"\"\"\n",
        "        return self.age.flatten().tolist()\n",
        "\n",
        "    def get_next_hand_color(self):\n",
        "        \"\"\"1 if black, -1 if white.\"\"\"\n",
        "        return (self.next_hand_color + 1) // 2\n",
        "\n",
        "    def update(self, moves: list[int], prt=False):\n",
        "        \"\"\"Applies moves and updates state (move is a square index 0, ..., 63)\"\"\"\n",
        "        if prt:\n",
        "            self.__print__()\n",
        "        for _, move in enumerate(moves):\n",
        "            self.umpire(move)\n",
        "            if prt:\n",
        "                self.__print__()\n",
        "\n",
        "    def umpire(self, move):\n",
        "        \"\"\"Applies one move and updates state (move is a square index 0, ..., 63)\"\"\"\n",
        "        r, c = move // 8, move % 8\n",
        "        assert self.state[r, c] == 0, f\"{r}-{c} is already occupied!\"\n",
        "        occupied = np.sum(self.state != 0)\n",
        "        color = self.next_hand_color\n",
        "        tbf = []\n",
        "        for direction in eights:\n",
        "            buffer = []\n",
        "            cur_r, cur_c = r, c\n",
        "            while 1:\n",
        "                cur_r, cur_c = cur_r + direction[0], cur_c + direction[1]\n",
        "                if cur_r < 0  or cur_r > 7 or cur_c < 0 or cur_c > 7:\n",
        "                    break\n",
        "                if self.state[cur_r, cur_c] == 0:\n",
        "                    break\n",
        "                elif self.state[cur_r, cur_c] == color:\n",
        "                    tbf.extend(buffer)\n",
        "                    break\n",
        "                else:\n",
        "                    buffer.append([cur_r, cur_c])\n",
        "\n",
        "        if len(tbf) == 0:  # means one hand is forfeited\n",
        "            # print(f\"One {color} move forfeited\")\n",
        "            color *= -1\n",
        "            self.next_hand_color *= -1\n",
        "            for direction in eights:\n",
        "                buffer = []\n",
        "                cur_r, cur_c = r, c\n",
        "                while 1:\n",
        "                    cur_r, cur_c = cur_r + direction[0], cur_c + direction[1]\n",
        "                    if cur_r < 0  or cur_r > 7 or cur_c < 0 or cur_c > 7:\n",
        "                        break\n",
        "                    if self.state[cur_r, cur_c] == 0:\n",
        "                        break\n",
        "                    elif self.state[cur_r, cur_c] == color:\n",
        "                        tbf.extend(buffer)\n",
        "                        break\n",
        "                    else:\n",
        "                        buffer.append([cur_r, cur_c])\n",
        "\n",
        "        if len(tbf) == 0:\n",
        "            valids = self.get_valid_moves()\n",
        "            if len(valids) == 0:\n",
        "                assert 0, \"Both color cannot put piece, game should have ended!\"\n",
        "            else:\n",
        "                assert 0, \"Illegal move!\"\n",
        "\n",
        "        self.age += 1\n",
        "        for ff in tbf:\n",
        "            self.state[ff[0], ff[1]] *= -1\n",
        "            self.age[ff[0], ff[1]] = 0\n",
        "        self.state[r, c] = color\n",
        "        self.age[r, c] = 0\n",
        "        self.next_hand_color *= -1\n",
        "        self.history.append(move)\n",
        "\n",
        "    def __print__(self, ):\n",
        "        \"\"\"Prints the board and move history-\"\"\"\n",
        "        print(\"-\"*20)\n",
        "        print([permit_reverse(_) for _ in self.history])\n",
        "        a = \"abcdefgh\"\n",
        "        for k, row in enumerate(self.state.tolist()):\n",
        "            tbp = []\n",
        "            for ele in row:\n",
        "                if ele == -1:\n",
        "                    tbp.append(\"O\")\n",
        "                elif ele == 0:\n",
        "                    tbp.append(\" \")\n",
        "                else:\n",
        "                    tbp.append(\"X\")\n",
        "            # tbp.append(\"\\n\")\n",
        "            print(\" \".join([a[k]] + tbp))\n",
        "        tbp = [str(k) for k in range(1, 9)]\n",
        "        print(\" \".join([\" \"] + tbp))\n",
        "        print(\"-\"*20)\n",
        "\n",
        "    def tentative_move(self, move):\n",
        "        \"\"\"\n",
        "        tentatively put a piece, do nothing to state\n",
        "        - returns 0 if this is not a move at all: occupied or both player have to forfeit\n",
        "        - return 1 if regular move\n",
        "        - return 2 if forfeit happens but the opponent can drop piece at this place\n",
        "         \"\"\"\n",
        "        r, c = move // 8, move % 8\n",
        "        if not self.state[r, c] == 0:\n",
        "            return 0\n",
        "        occupied = np.sum(self.state != 0)\n",
        "        color = self.next_hand_color\n",
        "        tbf = []\n",
        "        for direction in eights:\n",
        "            buffer = []\n",
        "            cur_r, cur_c = r, c\n",
        "            while 1:\n",
        "                cur_r, cur_c = cur_r + direction[0], cur_c + direction[1]\n",
        "                if cur_r < 0  or cur_r > 7 or cur_c < 0 or cur_c > 7:\n",
        "                    break\n",
        "                if self.state[cur_r, cur_c] == 0:\n",
        "                    break\n",
        "                elif self.state[cur_r, cur_c] == color:\n",
        "                    tbf.extend(buffer)\n",
        "                    break\n",
        "                else:\n",
        "                    buffer.append([cur_r, cur_c])\n",
        "        if len(tbf) != 0:\n",
        "            return 1\n",
        "        else:  # means one hand is forfeited\n",
        "            # print(f\"One {color} move forfeited\")\n",
        "            color *= -1\n",
        "            # self.next_hand_color *= -1\n",
        "            for direction in eights:\n",
        "                buffer = []\n",
        "                cur_r, cur_c = r, c\n",
        "                while 1:\n",
        "                    cur_r, cur_c = cur_r + direction[0], cur_c + direction[1]\n",
        "                    if cur_r < 0  or cur_r > 7 or cur_c < 0 or cur_c > 7:\n",
        "                        break\n",
        "                    if self.state[cur_r, cur_c] == 0:\n",
        "                        break\n",
        "                    elif self.state[cur_r, cur_c] == color:\n",
        "                        tbf.extend(buffer)\n",
        "                        break\n",
        "                    else:\n",
        "                        buffer.append([cur_r, cur_c])\n",
        "            if len(tbf) == 0:\n",
        "                return 0\n",
        "            else:\n",
        "                return 2\n",
        "\n",
        "    def get_valid_moves(self):\n",
        "        \"\"\"\n",
        "        Returns a list of square indices (0-63) where at least one player\n",
        "        could place a stone, i.e. either the player whos turn it is or if he\n",
        "        forfeits the other player.\n",
        "        \"\"\"\n",
        "        regular_moves = []\n",
        "        forfeit_moves = []\n",
        "        for move in range(64):\n",
        "            x = self.tentative_move(move)\n",
        "            if x == 1:\n",
        "                regular_moves.append(move)\n",
        "            elif x == 2:\n",
        "                forfeit_moves.append(move)\n",
        "            else:\n",
        "                pass\n",
        "        if len(regular_moves):\n",
        "            return regular_moves\n",
        "        elif len(forfeit_moves):\n",
        "            return forfeit_moves\n",
        "        else:\n",
        "            return []\n",
        "\n",
        "    def get_gt(self, moves, func, prt=False):\n",
        "        \"\"\"\n",
        "        Applies moves (list of square indices 0-63).\n",
        "        Returns list of result of func after each move.\n",
        "        Example:\n",
        "        - game already had 13 moves\n",
        "        - game.get_gt([45,6,18], 'get_age')\n",
        "        - returns: [13, 14, 15]\n",
        "        \"\"\"\n",
        "        # takes a new move or new moves and update state\n",
        "        container = []\n",
        "        if prt:\n",
        "            self.__print__()\n",
        "        for _, move in enumerate(moves):\n",
        "            self.umpire(move)\n",
        "            container.append(getattr(self, func)())\n",
        "            # to predict first y, we need already know the first x\n",
        "            if prt:\n",
        "                self.__print__()\n",
        "        return container\n",
        "\n",
        "    # NEXT_TO_OPPONENT rule\n",
        "    def get_moves_next_to_opponent(self):\n",
        "        \"\"\"\n",
        "        Returns a list of square indices (0-63) that are:\n",
        "        1. Empty\n",
        "        2. Adjacent to at least one opponent piece\n",
        "        Handles passing like standard Othello.\n",
        "        \"\"\"\n",
        "        def _get_moves_for_color(color):\n",
        "            opponent_color = -color\n",
        "            moves = []\n",
        "            for move in range(64):\n",
        "                r, c = move // 8, move % 8\n",
        "                if self.state[r, c] != 0:\n",
        "                    continue\n",
        "                for direction in fours:\n",
        "                    adj_r, adj_c = r + direction[0], c + direction[1]\n",
        "                    if 0 <= adj_r < 8 and 0 <= adj_c < 8:\n",
        "                        if self.state[adj_r, adj_c] == opponent_color:\n",
        "                            moves.append(move)\n",
        "                            break\n",
        "            return moves\n",
        "\n",
        "        # Try current player first\n",
        "        moves = _get_moves_for_color(self.next_hand_color)\n",
        "        if moves:\n",
        "            return moves\n",
        "\n",
        "        # If no moves, try opponent (forfeit)\n",
        "        moves = _get_moves_for_color(-self.next_hand_color)\n",
        "        return moves\n",
        "\n",
        "    def place_next_to_opponent(self, move):\n",
        "        \"\"\"Place a stone and flipps adjacent opponent pieces, handles forfeiting like umpire().\"\"\"\n",
        "        r, c = move // 8, move % 8\n",
        "        assert self.state[r, c] == 0, f\"{r}-{c} is already occupied!\"\n",
        "\n",
        "        color = self.next_hand_color\n",
        "        opponent_color = -color\n",
        "\n",
        "        # Check if move is adjacent to opponent\n",
        "        is_valid = False\n",
        "        for direction in fours:\n",
        "            adj_r, adj_c = r + direction[0], c + direction[1]\n",
        "            if 0 <= adj_r < 8 and 0 <= adj_c < 8:\n",
        "                if self.state[adj_r, adj_c] == opponent_color:\n",
        "                    is_valid = True\n",
        "                    break\n",
        "\n",
        "        if not is_valid:  # forfeit - try opposite color\n",
        "            color *= -1\n",
        "            self.next_hand_color *= -1  # Switch whose turn it is\n",
        "            opponent_color = -color\n",
        "            for direction in fours:\n",
        "                adj_r, adj_c = r + direction[0], c + direction[1]\n",
        "                if 0 <= adj_r < 8 and 0 <= adj_c < 8:\n",
        "                    if self.state[adj_r, adj_c] == opponent_color:\n",
        "                        is_valid = True\n",
        "                        break\n",
        "\n",
        "        if not is_valid:\n",
        "            # Check if any moves possible at all\n",
        "            moves = self.get_moves_next_to_opponent()\n",
        "            if len(moves) == 0:\n",
        "                assert 0, \"Both colors cannot put piece, game should have ended!\"\n",
        "            else:\n",
        "                assert 0, \"Illegal move!\"\n",
        "\n",
        "        self.age += 1\n",
        "        self.state[r, c] = color\n",
        "        self.age[r, c] = 0\n",
        "\n",
        "        # Now flip all adjacent opponent stones\n",
        "        for direction in fours:\n",
        "            adj_r, adj_c = r + direction[0], c + direction[1]\n",
        "            if 0 <= adj_r < 8 and 0 <= adj_c < 8:\n",
        "                if self.state[adj_r, adj_c] == opponent_color:\n",
        "                    self.state[adj_r, adj_c] = color   # flip to our color\n",
        "                    self.age[adj_r, adj_c] = 0         # reset age\n",
        "\n",
        "        self.next_hand_color *= -1  # Switch turns after placing\n",
        "        self.history.append(move)\n",
        "\n",
        "    def place_no_flipping(self, move):\n",
        "        \"\"\"Place a stone and does not flip any opponent pieces\"\"\"\n",
        "        r, c = move // 8, move % 8\n",
        "        assert self.state[r, c] == 0, f\"{r}-{c} is already occupied!\"\n",
        "        self.age += 1\n",
        "        self.state[r, c] = self.next_hand_color\n",
        "        self.age[r, c] = 0\n",
        "        self.next_hand_color *= -1\n",
        "        self.history.append(move)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF367sxmh2xt"
      },
      "source": [
        "### map_between_othello_representations.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0YWlbxn_h2QM"
      },
      "outputs": [],
      "source": [
        "MIDDLE_SQUARES = [27, 28, 35, 36]\n",
        "ALL_SQUARES = [i for i in range(64) if i not in MIDDLE_SQUARES]\n",
        "\n",
        "# Vocab (i.e. token IDs)\n",
        "VOCAB = list(range(61))  # pass + 60 open fields\n",
        "\n",
        "# Lists to map between token IDs (1 to 60, not including pass token) and square indices (0 to 63 inclusive)\n",
        "ID_TO_SQUARE = {0: -100, **{id: square for id, square in enumerate(ALL_SQUARES, start=1)}}\n",
        "SQUARE_TO_ID = {square: id for id, square in ID_TO_SQUARE.items()}\n",
        "\n",
        "\n",
        "alpha = \"ABCDEFGH\"\n",
        "\n",
        "\n",
        "def to_board_label(i):\n",
        "    return f\"{alpha[i//8]}{i%8}\"\n",
        "\n",
        "\n",
        "board_labels = list(map(to_board_label, ALL_SQUARES))\n",
        "\n",
        "\n",
        "def str_to_id(s):\n",
        "    return SQUARE_TO_ID[s] - 1\n",
        "\n",
        "\n",
        "def to_id(x):\n",
        "    \"\"\"\n",
        "    Maps from either square (0, 63) or board label (A0, H7) to id (1, 60).\n",
        "    \"\"\"\n",
        "    if isinstance(x, t.Tensor) and x.numel() == 1:\n",
        "        return to_id(x.item())\n",
        "    elif isinstance(x, list) or isinstance(x, t.Tensor) or isinstance(x, np.ndarray):\n",
        "        return [to_id(i) for i in x]\n",
        "    elif isinstance(x, int):\n",
        "        return SQUARE_TO_ID[x]\n",
        "    elif isinstance(x, str):\n",
        "        x = x.upper()\n",
        "        return to_id(to_square(x))\n",
        "\n",
        "\n",
        "def to_square(x):\n",
        "    \"\"\"\n",
        "    Maps from either id (1, 60) or board label (A0, H7) to square (0, 63).\n",
        "    \"\"\"\n",
        "    if isinstance(x, t.Tensor) and x.numel() == 1:\n",
        "        return to_square(x.item())\n",
        "    elif isinstance(x, list) or isinstance(x, t.Tensor) or isinstance(x, np.ndarray):\n",
        "        return [to_square(i) for i in x]\n",
        "    elif isinstance(x, int):\n",
        "        return ID_TO_SQUARE[x]\n",
        "    elif isinstance(x, str):\n",
        "        x = x.upper()\n",
        "        return 8 * alpha.index(x[0]) + int(x[1])\n",
        "\n",
        "\n",
        "def to_label(x, from_square=True):\n",
        "    \"\"\"\n",
        "    Maps from either id (1, 60) or square (0, 63) to board label (A0, H7).\n",
        "    \"\"\"\n",
        "    if isinstance(x, t.Tensor) and x.numel() == 1:\n",
        "        return to_label(x.item(), from_square=from_square)\n",
        "    elif isinstance(x, list) or isinstance(x, t.Tensor) or isinstance(x, np.ndarray):\n",
        "        return [to_label(i, from_square=from_square) for i in x]\n",
        "    elif isinstance(x, int):\n",
        "        if from_square:\n",
        "            return to_board_label(to_square(x))\n",
        "        else:\n",
        "            return to_board_label(x)\n",
        "    elif isinstance(x, str):\n",
        "        return x\n",
        "\n",
        "\n",
        "def square_to_label(x):\n",
        "    return to_label(x, from_square=False)\n",
        "\n",
        "\n",
        "def id_to_label(x):\n",
        "    return to_label(x, from_square=True)\n",
        "\n",
        "\n",
        "def id_to_square(x):\n",
        "    return to_square(x)\n",
        "\n",
        "\n",
        "def label_to_square(x):\n",
        "    return to_square(x)\n",
        "\n",
        "\n",
        "def square_to_id(x):\n",
        "    return to_id(x)\n",
        "\n",
        "\n",
        "def label_to_id(x):\n",
        "    return to_id(x)\n",
        "\n",
        "\n",
        "def moves_to_state(moves):\n",
        "    # moves is a list of square entries (ints from 0 to 63)\n",
        "    state = np.zeros((8, 8), dtype=bool)\n",
        "    for move in moves:\n",
        "        state[move // 8, move % 8] = 1.0\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvRRT56NfBbB"
      },
      "source": [
        "### othello.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TqKJv2o-kudQ"
      },
      "outputs": [],
      "source": [
        "class OthelloRule(Enum):\n",
        "    STANDARD = \"standard\"\n",
        "    BIAS_CLOCK = \"bias_clock\"\n",
        "    UNTRAINED = \"untrained\"\n",
        "    CHESS = \"chess\"\n",
        "    TINY_STORIES = \"tiny_stories\"\n",
        "    NEXT_TO_OPPONENT = \"next_to_opponent\"\n",
        "    CONSTANT_PARAMETERS = \"constant_parameters\"\n",
        "    NO_FLIPPING = \"no_flipping\"\n",
        "\n",
        "\n",
        "class Goal(Enum):\n",
        "    WEAK_GOAL = \"weak\"\n",
        "    STRONG_GOAL = \"strong\"\n",
        "\n",
        "\n",
        "def play_random_opening(ab: OthelloBoardState) -> list[int]:\n",
        "    # Select random Othello openening\n",
        "    all_openings = [\n",
        "            ('C3', 'C4'), ('D2', 'C4'), ('E5', 'F3'),\n",
        "            ('F4', 'D5'), ('F4', 'F3'), ('F4', 'F5'),\n",
        "            ('C3', 'E2'), ('E5', 'D5'), ('D2', 'C2'),\n",
        "            ('E5', 'F5'), ('C3', 'C2'), ('D2', 'E2')\n",
        "        ]\n",
        "    opening = random.choice(all_openings)\n",
        "    first_move = label_to_square(opening[0])\n",
        "    second_move = label_to_square(opening[1])\n",
        "\n",
        "    # Apply opening\n",
        "    tbr = []\n",
        "    tbr.append(first_move)\n",
        "    ab.place_next_to_opponent(first_move)\n",
        "    tbr.append(second_move)\n",
        "    ab.place_next_to_opponent(second_move)\n",
        "    return tbr\n",
        "\n",
        "\n",
        "def get_ood_game(othello_rule: OthelloRule):\n",
        "    \"\"\"\n",
        "    Randomly samples a game and returns a list of moves (square indices 0-63)\n",
        "    If a player has to forfeit, automatically switch to the other player.\n",
        "    \"\"\"\n",
        "    tbr = []\n",
        "    ab = OthelloBoardState()\n",
        "    move_count = 0\n",
        "    possible_next_steps = True\n",
        "\n",
        "    if othello_rule in [OthelloRule.NEXT_TO_OPPONENT, OthelloRule.NO_FLIPPING]:\n",
        "        tbr = play_random_opening(ab)\n",
        "        move_count = 2\n",
        "\n",
        "    while possible_next_steps:\n",
        "        # Get possible moves based on rule\n",
        "        if othello_rule in [OthelloRule.NEXT_TO_OPPONENT, OthelloRule.NO_FLIPPING]:\n",
        "            possible_next_steps = ab.get_moves_next_to_opponent()\n",
        "        else:\n",
        "            possible_next_steps = ab.get_valid_moves()\n",
        "\n",
        "        # End game if no moves available\n",
        "        if not possible_next_steps:\n",
        "            break\n",
        "\n",
        "        if othello_rule == OthelloRule.STANDARD:\n",
        "            next_step = random.choice(possible_next_steps)\n",
        "        elif othello_rule == OthelloRule.BIAS_CLOCK:\n",
        "            if move_count >= 2 and random.random() < 0.8:\n",
        "                # Rotate clockwise: top-left -> top-right -> bottom-right -> bottom-left\n",
        "                corner = move_count % 4\n",
        "                if corner == 0:  # top-left\n",
        "                    next_step = min(possible_next_steps, key=lambda x: (x // 8 + x % 8, x // 8))\n",
        "                elif corner == 1:  # top-right\n",
        "                    next_step = min(possible_next_steps, key=lambda x: (x // 8 + (7 - x % 8), x // 8))\n",
        "                elif corner == 2:  # bottom-right\n",
        "                    next_step = min(possible_next_steps, key=lambda x: ((7 - x // 8) + (7 - x % 8), 7 - x // 8))\n",
        "                else:  # bottom-left\n",
        "                    next_step = min(possible_next_steps, key=lambda x: ((7 - x // 8) + x % 8, 7 - x // 8))\n",
        "            else:\n",
        "                next_step = random.choice(possible_next_steps)\n",
        "        elif othello_rule == OthelloRule.NEXT_TO_OPPONENT:\n",
        "            next_step = random.choice(possible_next_steps)\n",
        "        elif othello_rule == OthelloRule.NO_FLIPPING:\n",
        "            if random.random() < 0.7:\n",
        "                next_step = random.choice(possible_next_steps)\n",
        "            else:  # 30% Chance for empty move on field\n",
        "                is_occupied = ab.get_occupied()\n",
        "                empty_moves = [i for i in range(64) if is_occupied[i] == 0]\n",
        "                next_step = random.choice(empty_moves)\n",
        "        else:\n",
        "            raise Exception(f\"Unknown othello rule: {othello_rule}\")\n",
        "\n",
        "        tbr.append(next_step)\n",
        "\n",
        "        # Apply move based on rule\n",
        "        if othello_rule == OthelloRule.NEXT_TO_OPPONENT:\n",
        "            ab.place_next_to_opponent(next_step)\n",
        "        elif othello_rule == OthelloRule.NO_FLIPPING:\n",
        "            ab.place_no_flipping(next_step)\n",
        "        else:\n",
        "            ab.update([next_step])\n",
        "\n",
        "        move_count += 1\n",
        "    return tbr\n",
        "\n",
        "\n",
        "def _generate_game_wrapper(args):\n",
        "    _, othello_rule = args\n",
        "    return get_ood_game(othello_rule)\n",
        "\n",
        "\n",
        "class Othello:\n",
        "    def __init__(self, data_folder, othello_rule: OthelloRule):\n",
        "        self.data_folder = data_folder\n",
        "        self.othello_rule = othello_rule\n",
        "        self.synthetic_othello_path = os.path.join(data_folder, self.othello_rule.value, \"othello_synthetic\")\n",
        "        self.train_val_test_path = os.path.join(data_folder, self.othello_rule.value, \"train_weakfinetune_val_test\")\n",
        "        self.train = []\n",
        "        self.weak_finetune = []\n",
        "        self.val = []\n",
        "        self.test = []\n",
        "        self.othello_rule_to_test: dict[OthelloRule, list] = {}\n",
        "\n",
        "    def generate_games(self, n_games):\n",
        "        \"\"\"\n",
        "        Generate n_games synthetic games and save as pickle files (1e5 games per file)\n",
        "        n_games must be a multiple of 1e5\n",
        "        \"\"\"\n",
        "        assert n_games % 100000 == 0, \"n_games must be multiple of 1e5\"\n",
        "\n",
        "        os.makedirs(self.synthetic_othello_path, exist_ok=True)\n",
        "\n",
        "        # Check existing files\n",
        "        existing_files = [f for f in os.listdir(self.synthetic_othello_path) if f.endswith('.pickle')]\n",
        "        existing_count = len(existing_files)\n",
        "        needed_files = n_games // 100000\n",
        "\n",
        "        print(f\"Found {existing_count} files, need {needed_files} total\")\n",
        "\n",
        "        if existing_count >= needed_files:\n",
        "            print(\"Already have enough files\")\n",
        "            return\n",
        "\n",
        "        # Generate missing files\n",
        "        for i in range(existing_count, needed_files):\n",
        "            print(f\"Generating file {i+1}/{needed_files}\")\n",
        "\n",
        "            # Generate 1e5 games\n",
        "            num_proc = multiprocessing.cpu_count()\n",
        "            p = multiprocessing.Pool(num_proc)\n",
        "            games = []\n",
        "\n",
        "            for game in tqdm(p.imap(_generate_game_wrapper, [(i, self.othello_rule) for i in range(100000)]), total=100000):\n",
        "                if game not in games:\n",
        "                    games.append(game)\n",
        "            p.close()\n",
        "\n",
        "            # Save with timestamp\n",
        "            t_start = time.strftime(\"_%Y%m%d_%H%M%S\")\n",
        "            filename = f'gen10e5{t_start}.pickle'\n",
        "            filepath = os.path.join(self.synthetic_othello_path, filename)\n",
        "\n",
        "            with open(filepath, 'wb') as handle:\n",
        "                pickle.dump(games, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "        print(f\"Generation complete: {needed_files} files\")\n",
        "\n",
        "    def generate_train_weakfinetune_val_test_split(self):\n",
        "        \"\"\"Generate train/weak_finetune/val/test split based on opening combinations\"\"\"\n",
        "        games = self.load_games()\n",
        "        print(f\"Loaded {len(games)} games\")\n",
        "\n",
        "        # Random split: 6 train, 3 weak_finetune, 1 val, 2 test\n",
        "        # Hardcoded such that for all rules, we always have the same split\n",
        "        # and no leakage. In TicTacToe, we can split all datasets at once,\n",
        "        # but in Othello we can always only load at most one in memory.\n",
        "        train_openings = [\n",
        "            ('C3', 'C4'), ('D2', 'C4'), ('E5', 'F3'),\n",
        "            ('F4', 'D5'), ('F4', 'F3'), ('F4', 'F5')\n",
        "        ]\n",
        "        weak_finetune_openings = [\n",
        "            ('C3', 'E2'), ('E5', 'D5'), ('D2', 'C2')\n",
        "        ]\n",
        "        val_openings = [\n",
        "            ('E5', 'F5')\n",
        "        ]\n",
        "        test_openings = [\n",
        "            ('C3', 'C2'), ('D2', 'E2')\n",
        "        ]\n",
        "\n",
        "        # Create index lookup for openings\n",
        "        opening_to_indices = {}\n",
        "        opening_combinations = set(train_openings) | set(weak_finetune_openings) | set(val_openings) | set(test_openings)\n",
        "        for combo in opening_combinations:\n",
        "            notation = (square_to_label(combo[0]), square_to_label(combo[1]))\n",
        "            opening_to_indices[notation] = combo\n",
        "\n",
        "        # Split games based on openings\n",
        "        train_games = []\n",
        "        weak_finetune_games = []\n",
        "        val_games = []\n",
        "        test_games = []\n",
        "        square_opening_to_list = {}\n",
        "        opening_to_n_games = {}\n",
        "        for opening_list, opening_games in zip([train_openings, weak_finetune_openings, val_openings, test_openings], [train_games, weak_finetune_games, val_games, test_games]):\n",
        "            for opening in opening_list:\n",
        "                square_tuple = (label_to_square(opening[0]), label_to_square(opening[1]))\n",
        "                square_opening_to_list[square_tuple] = opening_games\n",
        "                opening_to_n_games[square_tuple] = 0\n",
        "\n",
        "        n_early_finished_game = 0\n",
        "        for game in tqdm(games, \"Games\"):\n",
        "            if len(game) != 60:\n",
        "                n_early_finished_game += 1\n",
        "                continue\n",
        "            square_opening_to_list[(game[0], game[1])].append(game)\n",
        "            opening_to_n_games[(game[0], game[1])] += 1\n",
        "\n",
        "\n",
        "        print(\"\")\n",
        "        print(f\"Early finished game perc: {n_early_finished_game / len(games) * 100}%\")\n",
        "        print(f\"Split: {len(train_games)} train, {len(weak_finetune_games)} weak_finetune, {len(val_games)} val, {len(test_games)} test\")\n",
        "\n",
        "        for opening in opening_combinations:\n",
        "            n_games = opening_to_n_games[(label_to_square(opening[0]), label_to_square(opening[1]))]\n",
        "            print(f\"{opening}: {n_games}\")\n",
        "\n",
        "        # Save splits\n",
        "        os.makedirs(self.train_val_test_path, exist_ok=True)\n",
        "        with open(os.path.join(self.train_val_test_path, 'train.pickle'), 'wb') as f:\n",
        "            pickle.dump(train_games, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "        with open(os.path.join(self.train_val_test_path, 'weak_finetune.pickle'), 'wb') as f:\n",
        "            pickle.dump(weak_finetune_games, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "        with open(os.path.join(self.train_val_test_path, 'val.pickle'), 'wb') as f:\n",
        "            pickle.dump(val_games, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "        with open(os.path.join(self.train_val_test_path, 'test.pickle'), 'wb') as f:\n",
        "            pickle.dump(test_games, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "        print(\"Saved train/weak_finetune/val/test splits\")\n",
        "\n",
        "    def load_train_weakfinetune_val_test(self):\n",
        "        \"\"\"Load train/weak_finetune/val/test splits into attributes\"\"\"\n",
        "        train_path = os.path.join(self.train_val_test_path, 'train.pickle')\n",
        "        weak_finetune_path = os.path.join(self.train_val_test_path, 'weak_finetune.pickle')\n",
        "        val_path = os.path.join(self.train_val_test_path, 'val.pickle')\n",
        "        test_path = os.path.join(self.train_val_test_path, 'test.pickle')\n",
        "\n",
        "        with open(train_path, 'rb') as f:\n",
        "            self.train = pickle.load(f)\n",
        "            print(\"Loaded train games\")\n",
        "\n",
        "        with open(weak_finetune_path, 'rb') as f:\n",
        "            self.weak_finetune = pickle.load(f)\n",
        "            print(\"Loaded weak_finetune games\")\n",
        "\n",
        "        with open(val_path, 'rb') as f:\n",
        "            self.val = pickle.load(f)\n",
        "            print(\"Loaded val games\")\n",
        "\n",
        "        with open(test_path, 'rb') as f:\n",
        "            self.test = pickle.load(f)\n",
        "            self.othello_rule_to_test[self.othello_rule] = self.test\n",
        "\n",
        "        print(f\"Loaded: {len(self.train)} train, {len(self.weak_finetune)} weak_finetune, {len(self.val)} val, {len(self.test)} test games\")\n",
        "\n",
        "        for othello_rule in get_othello_rules_with_data():\n",
        "            if not othello_rule in self.othello_rule_to_test:\n",
        "              folder_path = os.path.join(self.data_folder, othello_rule.value, \"train_weakfinetune_val_test\")\n",
        "              test_path = os.path.join(folder_path, 'test.pickle')\n",
        "              with open(test_path, 'rb') as f:\n",
        "                self.othello_rule_to_test[othello_rule] = pickle.load(f)\n",
        "              print(f\"Loaded: {len(self.othello_rule_to_test[othello_rule])} test games for {othello_rule.value}\")\n",
        "\n",
        "    def load_test(self):\n",
        "        for othello_rule in get_othello_rules_with_data():\n",
        "            folder_path = os.path.join(self.data_folder, othello_rule.value, \"train_weakfinetune_val_test\")\n",
        "            test_path = os.path.join(folder_path, 'test.pickle')\n",
        "            with open(test_path, 'rb') as f:\n",
        "              self.othello_rule_to_test[othello_rule] = pickle.load(f)\n",
        "            print(f\"Loaded: {len(self.othello_rule_to_test[othello_rule])} test games for {othello_rule.value}\")\n",
        "\n",
        "    def load_games(self, n_games: int | None = None) -> list[list[int]]:\n",
        "        \"\"\"\n",
        "        Load n_games (all if n_games is None) from pickle files.\n",
        "        Removes duplicates.\n",
        "        \"\"\"\n",
        "        files = [f for f in os.listdir(self.synthetic_othello_path) if f.endswith('.pickle')]\n",
        "        if n_games:\n",
        "          needed_files = min(len(files), (n_games // 100000) + 1)\n",
        "        else:\n",
        "          needed_files = len(files)\n",
        "\n",
        "        loaded_games = []\n",
        "        loaded = 0\n",
        "        for i, f in enumerate(tqdm(files[:needed_files], desc=\"Loading files\")):\n",
        "            if n_games and loaded >= n_games:\n",
        "                break\n",
        "\n",
        "            with open(os.path.join(self.synthetic_othello_path, f), 'rb') as handle:\n",
        "                b = pickle.load(handle)\n",
        "                if n_games:\n",
        "                  remaining = n_games - loaded\n",
        "                  loaded_games.extend(b[:remaining])\n",
        "                  loaded += len(b[:remaining])\n",
        "                else:\n",
        "                  loaded_games.extend(b)\n",
        "                  loaded += len(b)\n",
        "\n",
        "        print(\"Deduplicating...\")\n",
        "        original_count = len(loaded_games)\n",
        "        loaded_games.sort()\n",
        "        loaded_games = [k for k, _ in itertools.groupby(loaded_games)]\n",
        "        random.shuffle(loaded_games)\n",
        "        duplicates_removed = original_count - len(loaded_games)\n",
        "        print(f\"Removed {duplicates_removed} duplicates ({len(loaded_games)} unique games)\")\n",
        "        self.sequences = loaded_games\n",
        "        return loaded_games"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IHl9qnHNp2f2"
      },
      "outputs": [],
      "source": [
        "# othello = Othello(data_folder, OthelloRule.NO_FLIPPING)\n",
        "# # othello.generate_games(100000000)\n",
        "# othello.generate_train_weakfinetune_val_test_split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zN0M8_Y_RIMA"
      },
      "outputs": [],
      "source": [
        "# # othello = Othello(data_folder, OthelloRule.NO_FLIPPING)\n",
        "# games = othello.load_games(100000)\n",
        "# dataset = CharDataset(games)\n",
        "# board_seqs_square, board_seqs_id, n_skipped = get_board_seqs_square_and_id(dataset, 10000)\n",
        "# print(f\"Skipped {n_skipped} games\")\n",
        "# plot_average_move_index(board_seqs_square)\n",
        "\n",
        "# example_game = games[3]\n",
        "# print(example_game)\n",
        "# plot_game_moves(example_game, list(range(40, 60)), othello_rule=OthelloRule.NO_FLIPPING)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRMwW4x7ieNT"
      },
      "source": [
        "### plot_othello.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ug-KwKifieZd"
      },
      "outputs": [],
      "source": [
        "def reorder_list_in_plotly_way(L: list, col_wrap: int):\n",
        "    \"\"\"\n",
        "    Helper function, because Plotly orders figures in an annoying way when there's column wrap.\n",
        "    \"\"\"\n",
        "    L_new = []\n",
        "    while len(L) > 0:\n",
        "        L_new.extend(L[-col_wrap:])\n",
        "        L = L[:-col_wrap]\n",
        "    return L_new\n",
        "\n",
        "\n",
        "# def plot_board_values(\n",
        "#     state: t.Tensor,\n",
        "#     board_titles: list[str] | None = None,\n",
        "#     boards_per_row: int | None = None,\n",
        "#     text: list[str] | list[list[str]] | None = None,\n",
        "#     filename: str | None = None,\n",
        "#     show: bool = True,\n",
        "#     **kwargs,\n",
        "# ):\n",
        "#     \"\"\"\n",
        "#     Takes input of shape (*N, 8, 8) and plots it as a board or series of boards. Colors are inferred by tensor values.\n",
        "\n",
        "#     Args:\n",
        "#         state:          If it's 2D then we plot a single board. If it's 3D then we plot multiple 8x8 boards.\n",
        "#         board_titles:   If supplied, we label the boards with these titles. Only valid if state is 3D.\n",
        "#         boards_per_row: If supplied, we show this many boards per row. Only valid if state is 3D.\n",
        "#         text:           Should be the same shape as state tensor. Used to add text annotations.\n",
        "#         kwargs:         Get passed into `px.imshow` (e.g. width and height)\n",
        "#     \"\"\"\n",
        "#     state = to_numpy(state)\n",
        "\n",
        "#     # Handle multiple boards\n",
        "#     if state.ndim == 3:\n",
        "#         boards_per_row = boards_per_row or state.shape[0]\n",
        "#         kwargs |= dict(facet_col=0, facet_col_wrap=boards_per_row)\n",
        "\n",
        "#     # Handle color coding, depending on the data type\n",
        "#     if state.dtype in [np.int64, np.int32]:\n",
        "#         kwargs |= dict(color_continuous_scale=\"Greys\")\n",
        "#     elif state.max().item() > 0:\n",
        "#         kwargs |= dict(color_continuous_scale=\"RdBu\", color_continuous_midpoint=0.0)\n",
        "#     else:\n",
        "#         kwargs |= dict(color_continuous_scale=\"Blues\")\n",
        "\n",
        "#     # Create the figure\n",
        "#     fig = px.imshow(to_numpy(state), y=list(\"ABCDEFGH\"), x=[str(i) for i in range(8)], aspect=\"equal\", **kwargs)\n",
        "\n",
        "#     # Optionally add board titles\n",
        "#     if board_titles is not None:\n",
        "#         board_titles = reorder_list_in_plotly_way(board_titles, boards_per_row)\n",
        "#         for i, title in enumerate(board_titles):\n",
        "#             fig.layout.annotations[i][\"text\"] = title\n",
        "\n",
        "#     # Optionally add text\n",
        "#     if text is not None:\n",
        "#         text: np.ndarray = np.array(text)\n",
        "#         try:\n",
        "#             text = np.broadcast_to(text, state.shape if state.ndim == 3 else (1, *state.shape))\n",
        "#         except ValueError:\n",
        "#             raise ValueError(f\"Shape mismatch: {text.shape=} should be broadcastable to {state.shape=}\")\n",
        "#         for i, _text in enumerate(text.tolist()):\n",
        "#             fig.data[i].update(text=_text, texttemplate=\"%{text}\", textfont={\"size\": 12})\n",
        "\n",
        "#     # If monochrome mode (just game state) then we hide the color scale\n",
        "#     if state.dtype in [np.int64, np.int32]:\n",
        "#         fig.update_coloraxes(showscale=False)\n",
        "#     if show:\n",
        "#         fig.show()\n",
        "#     if filename is not None:\n",
        "#         fig.write_html(filename)\n",
        "\n",
        "#     return fig\n",
        "\n",
        "import plotly.graph_objects as go # Make sure to import go\n",
        "import numpy as np # Make sure to import numpy\n",
        "\n",
        "def plot_board_values(\n",
        "    state: t.Tensor,\n",
        "    board_titles: list[str] | None = None,\n",
        "    boards_per_row: int | None = None,\n",
        "    text: list[str] | list[list[str]] | None = None,\n",
        "    legal_moves_mask: t.Tensor | None = None, # NEW: Added argument for the red overlay\n",
        "    filename: str | None = None,\n",
        "    show: bool = True,\n",
        "    **kwargs,\n",
        "):\n",
        "    \"\"\"\n",
        "    Takes input of shape (*N, 8, 8) and plots it as a board or series of boards. Colors are inferred by tensor values.\n",
        "\n",
        "    Args:\n",
        "        state:           If it's 2D then we plot a single board. If it's 3D then we plot multiple 8x8 boards.\n",
        "        board_titles:    If supplied, we label the boards with these titles. Only valid if state is 3D.\n",
        "        boards_per_row:  If supplied, we show this many boards per row. Only valid if state is 3D.\n",
        "        text:            Should be the same shape as state tensor. Used to add text annotations.\n",
        "        legal_moves_mask: A boolean tensor of the same shape as state, True for legal moves to be colored red.\n",
        "        kwargs:          Get passed into `px.imshow` (e.g. width and height)\n",
        "    \"\"\"\n",
        "    state = to_numpy(state)\n",
        "\n",
        "    # Handle multiple boards\n",
        "    if state.ndim == 3:\n",
        "        boards_per_row = boards_per_row or state.shape[0]\n",
        "        kwargs |= dict(facet_col=0, facet_col_wrap=boards_per_row)\n",
        "\n",
        "    # Handle color coding, depending on the data type\n",
        "    if state.dtype in [np.int64, np.int32]:\n",
        "        # CHANGED: Use 'Greys' to get black (-1), grey (0), and white (1) pieces.\n",
        "        kwargs |= dict(color_continuous_scale=\"Greys\", zmin=-1, zmax=1)\n",
        "    elif state.max().item() > 0:\n",
        "        kwargs |= dict(color_continuous_scale=\"RdBu\", color_continuous_midpoint=0.0)\n",
        "    else:\n",
        "        kwargs |= dict(color_continuous_scale=\"Blues\")\n",
        "\n",
        "    # Create the base figure with the pieces and board\n",
        "    fig = px.imshow(to_numpy(state), y=list(\"ABCDEFGH\"), x=[str(i) for i in range(8)], aspect=\"equal\", **kwargs)\n",
        "\n",
        "    # --- START OF NEW SECTION ---\n",
        "    # NEW: Overlay a second heatmap for legal moves if a mask is provided\n",
        "    if legal_moves_mask is not None:\n",
        "        legal_moves_mask = to_numpy(legal_moves_mask)\n",
        "        if legal_moves_mask.ndim == 2:\n",
        "            legal_moves_mask = np.expand_dims(legal_moves_mask, axis=0)\n",
        "\n",
        "        num_boards = state.shape[0] if state.ndim == 3 else 1\n",
        "\n",
        "        for i in range(num_boards):\n",
        "            # Create a matrix with 1s for legal moves and NaN (transparent) otherwise\n",
        "            z_overlay = np.where(legal_moves_mask[i], 1, np.nan)\n",
        "\n",
        "            # Add the heatmap as a new layer on the correct subplot\n",
        "            fig.add_trace(\n",
        "                go.Heatmap(\n",
        "                    z=z_overlay,\n",
        "                    colorscale=[[0, \"#ff0c0c\"], [1, \"#ff0c0c\"]], # Solid red color\n",
        "                    # colorscale=[[0, \"rgba(255, 12, 12, 0.5)\"], [1, \"rgba(255, 12, 12, 0.5)\"]], # Lighter, semi-transparent red\n",
        "                    showscale=False\n",
        "                ),\n",
        "                row=(i // boards_per_row) + 1 if state.ndim == 3 else None,\n",
        "                col=(i % boards_per_row) + 1 if state.ndim == 3 else None,\n",
        "            )\n",
        "    # --- END OF NEW SECTION ---\n",
        "\n",
        "    # Optionally add board titles\n",
        "    if board_titles is not None:\n",
        "        board_titles = reorder_list_in_plotly_way(board_titles, boards_per_row)\n",
        "        for i, title in enumerate(board_titles):\n",
        "            fig.layout.annotations[i][\"text\"] = title\n",
        "\n",
        "    # Optionally add text\n",
        "    if text is not None:\n",
        "        text: np.ndarray = np.array(text)\n",
        "        try:\n",
        "            text = np.broadcast_to(text, state.shape if state.ndim == 3 else (1, *state.shape))\n",
        "        except ValueError:\n",
        "            raise ValueError(f\"Shape mismatch: {text.shape=} should be broadcastable to {state.shape=}\")\n",
        "        for i, _text in enumerate(text.tolist()):\n",
        "            fig.data[0].update(text=_text, texttemplate=\"%{text}\", textfont={\"size\": 12})\n",
        "\n",
        "    # If monochrome mode (just game state) then we hide the color scale\n",
        "    if state.dtype in [np.int64, np.int32]:\n",
        "        fig.update_coloraxes(showscale=False)\n",
        "    if show:\n",
        "        fig.show()\n",
        "    if filename is not None:\n",
        "        fig.write_html(filename)\n",
        "\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8unPxOD58V0W"
      },
      "source": [
        "### char_dataset.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wrnYmJpP8ZFt"
      },
      "outputs": [],
      "source": [
        "class CharDataset(Dataset):  # WSG: Removed the ood_perc\n",
        "    def __init__(self, data):  # Data is from Othello class\n",
        "        assert max([len(data[i]) for i in range(min(len(data), 10000))]) == 60  # WSG\n",
        "\n",
        "        self.stoi = SQUARE_TO_ID  # Maps squares to IDs 1-60, -100 to 0\n",
        "        self.itos = ID_TO_SQUARE\n",
        "        self.vocab_size = 61\n",
        "        self.max_len = 60\n",
        "        self.block_size = 59\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the number of games.\"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Returns the x, y pair generated by the idx-th game.\"\"\"\n",
        "        # grab a chunk of (block_size + 1) characters from the data\n",
        "        chunk = self.data[idx]\n",
        "        if len(chunk) != self.max_len:\n",
        "            chunk += [-100, ] * (self.max_len - len(chunk))  # -100 can be ignored in CE\n",
        "        # encode every character to an integer\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "\n",
        "        x = t.tensor(dix[:-1], dtype=t.long)\n",
        "        y = t.tensor(dix[1:], dtype=t.long)\n",
        "        y[:2] = 0  # Ignore the first two moves since we train/test split over these\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7ExCwGRXzJh"
      },
      "source": [
        "### process_data.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MXj_WBqiOXPI"
      },
      "outputs": [],
      "source": [
        "def get_board_states_and_legal_moves(\n",
        "    games_square: Int[Tensor, \"n_games n_moves\"],\n",
        "    othello_rule: OthelloRule,\n",
        ") -> tuple[\n",
        "    Int[Tensor, \"n_games n_moves rows cols\"],\n",
        "    Int[Tensor, \"n_games n_moves rows cols\"],\n",
        "    list,\n",
        "]:\n",
        "    \"\"\"\n",
        "    Returns the following:\n",
        "        states:                 (n_games, n_moves, 8, 8): tensor of board states after each move\n",
        "        legal_moves:            (n_games, n_moves, 8, 8): tensor of 1s for legal moves, 0s for illegal moves\n",
        "        legal_moves_annotation: (n_games, n_moves, 8, 8): list containing strings of \"o\" for legal moves (for plotting)\n",
        "    \"\"\"\n",
        "    # Create tensors to store the board state & legal moves\n",
        "    n_games, n_moves = games_square.shape\n",
        "    states = t.zeros((n_games, 60, 8, 8), dtype=t.int32)\n",
        "    legal_moves = t.zeros((n_games, 60, 8, 8), dtype=t.int32)\n",
        "\n",
        "    # Loop over each game, populating state & legal moves tensors after each move\n",
        "    for n in range(n_games):\n",
        "        board = OthelloBoardState()\n",
        "        for i in range(n_moves):#\n",
        "            if othello_rule in [OthelloRule.STANDARD, OthelloRule.BIAS_CLOCK,\n",
        "                                OthelloRule.CHESS, OthelloRule.UNTRAINED,\n",
        "                                OthelloRule.TINY_STORIES, OthelloRule.CONSTANT_PARAMETERS]:\n",
        "                board.umpire(games_square[n, i].item())\n",
        "                states[n, i] = t.from_numpy(board.state)\n",
        "                legal_moves[n, i].flatten()[board.get_valid_moves()] = 1\n",
        "            elif othello_rule == OthelloRule.NEXT_TO_OPPONENT:\n",
        "                board.place_next_to_opponent(games_square[n, i].item())\n",
        "                states[n, i] = t.from_numpy(board.state)\n",
        "                legal_moves[n, i].flatten()[board.get_moves_next_to_opponent()] = 1\n",
        "            else:\n",
        "                raise Exception(f\"Unknown othello rule: {othello_rule}\")\n",
        "\n",
        "    # Convert legal moves to annotation\n",
        "    legal_moves_annotation = np.where(to_numpy(legal_moves), \"o\", \"\").tolist()\n",
        "\n",
        "    return states, legal_moves, legal_moves_annotation\n",
        "\n",
        "\n",
        "def get_board_seqs_square_and_id(dataset, n_games: int):\n",
        "    \"\"\"dataset has to contain square indices\"\"\"\n",
        "    games_data = [dataset.data[i] for i in range(min(int(n_games * 1.2), len(dataset.data)))]\n",
        "    board_seqs_square = []\n",
        "    board_seqs_id = []\n",
        "    n_skipped = 0\n",
        "    for game in games_data:\n",
        "        if len(game) != 60 or -100 in game:  # TODO Filtering out -100 here might be sketchy\n",
        "            n_skipped += 1\n",
        "            continue\n",
        "        board_seqs_square.append(game)\n",
        "        game_ids = [square_to_id(sq) for sq in game]\n",
        "        board_seqs_id.append(game_ids)\n",
        "\n",
        "    board_seqs_square = t.tensor(board_seqs_square[:n_games], dtype=t.long)\n",
        "    board_seqs_id = t.tensor(board_seqs_id[:n_games], dtype=t.long)\n",
        "    return board_seqs_square, board_seqs_id, n_skipped"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSmryN0PM8H_"
      },
      "source": [
        "### analyze_data.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4VxswhhmM8Qf"
      },
      "outputs": [],
      "source": [
        "def plot_average_move_index(board_seqs_square):\n",
        "    \"\"\"Plot average move index when each position is played\"\"\"\n",
        "    n_games, n_moves = board_seqs_square.shape\n",
        "    move_indices = t.arange(n_moves).unsqueeze(0).expand(n_games, -1)\n",
        "\n",
        "    avg_indices = t.zeros(64)\n",
        "    for square in range(64):\n",
        "        mask = (board_seqs_square == square)\n",
        "        if mask.any():\n",
        "            avg_indices[square] = move_indices[mask].float().mean()\n",
        "\n",
        "    avg_board = avg_indices.reshape(8, 8)\n",
        "\n",
        "    plot_board_values(\n",
        "        avg_board,\n",
        "        title=\"Average Move Index by Position\",\n",
        "        text=[[f\"{idx:.1f}\" for idx in row] for row in avg_board.tolist()],\n",
        "    )\n",
        "    return avg_board\n",
        "\n",
        "\n",
        "# def plot_game_moves(game: list[int], move_indices: list[int | str], othello_rule: OthelloRule = OthelloRule.STANDARD) -> None:\n",
        "#     \"\"\"\n",
        "#     Input:\n",
        "#     - game: Square Indices (0-63)\n",
        "#     - move_indices: move index (first move is move 0)\n",
        "#     \"\"\"\n",
        "#     game = to_id(game)\n",
        "#     n_moves = len(game)\n",
        "#     board_states = t.zeros((n_moves, 8, 8), dtype=t.int32)\n",
        "#     legal_moves = t.zeros((n_moves, 8, 8), dtype=t.int32)\n",
        "\n",
        "#     board = OthelloBoardState()\n",
        "#     for i, token_id in enumerate(game):\n",
        "#         if othello_rule in [OthelloRule.STANDARD, OthelloRule.BIAS_CLOCK]:\n",
        "#             board.umpire(id_to_square(token_id))\n",
        "#             board_states[i] = t.from_numpy(board.state)  # 8x8 numpy array of 0 (blank), -1 (black), 1 (white)\n",
        "#             legal_moves[i].flatten()[board.get_valid_moves()] = 1\n",
        "#         elif othello_rule == OthelloRule.NEXT_TO_OPPONENT:\n",
        "#             board.place_next_to_opponent(id_to_square(token_id))\n",
        "#             board_states[i] = t.from_numpy(board.state)\n",
        "#             legal_moves[i].flatten()[board.get_moves_next_to_opponent()] = 1\n",
        "#         elif othello_rule == OthelloRule.NO_FLIPPING:\n",
        "#             board.place_no_flipping(id_to_square(token_id))\n",
        "#             board_states[i] = t.from_numpy(board.state)\n",
        "#             is_occupied = board.get_occupied()\n",
        "#             empty_moves = [i for i in range(64) if is_occupied[i] == 0]\n",
        "#             legal_moves[i].flatten()[empty_moves] = 1  # Basically it can never be\n",
        "#         else:\n",
        "#             raise Exception(f\"Unknown othello rule: {othello_rule}\")\n",
        "\n",
        "#     selected_board_states = board_states[move_indices]\n",
        "#     selected_legal_moves = legal_moves[move_indices]\n",
        "#     legal_moves_annotation = np.where(to_numpy(selected_legal_moves), \"o\", \"\").tolist()\n",
        "#     # plot_board_values(\n",
        "#     #     selected_board_states,\n",
        "#     #     title=\"Board states\",\n",
        "#     #     width=1000,\n",
        "#     #     boards_per_row=5,\n",
        "#     #     board_titles=[f\"State after move {i}\" for i in move_indices],\n",
        "#     #     text=legal_moves_annotation,\n",
        "#     # )\n",
        "#     display_states = selected_board_states.clone().float() # Use float for custom color scale\n",
        "#     display_states[selected_legal_moves == 1] = 2\n",
        "\n",
        "#     zmin, zmax = -1, 2\n",
        "#     custom_colorscale = [\n",
        "#         [0.0, \"blue\"],                             # Color for value -1 (Black)\n",
        "#         [(0 - zmin) / (zmax - zmin), \"lightgrey\"], # Color for value 0 (Empty)\n",
        "#         [(1 - zmin) / (zmax - zmin), \"red\"],       # Color for value 1 (White)\n",
        "#         [1.0, \"#ff0c0c\"],                          # Color for value 2 (Legal Move)\n",
        "#     ]\n",
        "\n",
        "#     plot_board_values(\n",
        "#         display_states,\n",
        "#         title=\"Board states\",\n",
        "#         width=1000,\n",
        "#         boards_per_row=5,\n",
        "#         board_titles=[f\"State after move {i}\" for i in move_indices],\n",
        "#         color_continuous_scale=custom_colorscale,\n",
        "#         zmin=zmin,\n",
        "#         zmax=zmax, # Pass the new kwargs to override default color logic\n",
        "#     )\n",
        "\n",
        "def plot_game_moves(game: list[int], move_indices: list[int | str], othello_rule: OthelloRule = OthelloRule.STANDARD) -> None:\n",
        "    \"\"\"\n",
        "    Input:\n",
        "    - game: Square Indices (0-63)\n",
        "    - move_indices: move index (first move is move 0)\n",
        "    \"\"\"\n",
        "    game = to_id(game)\n",
        "    n_moves = len(game)\n",
        "    board_states = t.zeros((n_moves, 8, 8), dtype=t.int32)\n",
        "    legal_moves = t.zeros((n_moves, 8, 8), dtype=t.int32)\n",
        "\n",
        "    board = OthelloBoardState()\n",
        "    for i, token_id in enumerate(game):\n",
        "        if othello_rule in [OthelloRule.STANDARD, OthelloRule.BIAS_CLOCK]:\n",
        "            board.umpire(id_to_square(token_id))\n",
        "            board_states[i] = t.from_numpy(board.state)  # 8x8 numpy array of 0 (blank), -1 (black), 1 (white)\n",
        "            legal_moves[i].flatten()[board.get_valid_moves()] = 1\n",
        "        elif othello_rule == OthelloRule.NEXT_TO_OPPONENT:\n",
        "            board.place_next_to_opponent(id_to_square(token_id))\n",
        "            board_states[i] = t.from_numpy(board.state)\n",
        "            legal_moves[i].flatten()[board.get_moves_next_to_opponent()] = 1\n",
        "        elif othello_rule == OthelloRule.NO_FLIPPING:\n",
        "            board.place_no_flipping(id_to_square(token_id))\n",
        "            board_states[i] = t.from_numpy(board.state)\n",
        "            is_occupied = board.get_occupied()\n",
        "            empty_moves = [i for i in range(64) if is_occupied[i] == 0]\n",
        "            legal_moves[i].flatten()[empty_moves] = 1\n",
        "        else:\n",
        "            raise Exception(f\"Unknown othello rule: {othello_rule}\")\n",
        "\n",
        "    selected_board_states = board_states[move_indices]\n",
        "    selected_legal_moves = legal_moves[move_indices]\n",
        "\n",
        "    # --- CHANGED SECTION ---\n",
        "    # Call the plotting function, passing the board states and legal moves mask separately\n",
        "    plot_board_values(\n",
        "        selected_board_states, # Use the original board state with values -1, 0, 1\n",
        "        title=\"Board states\",\n",
        "        width=1000,\n",
        "        boards_per_row=5,\n",
        "        board_titles=[f\"State after move {i}\" for i in move_indices],\n",
        "        legal_moves_mask=selected_legal_moves.bool(), # Pass the legal moves for the red overlay\n",
        "    )\n",
        "    # --- END OF CHANGED SECTION ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyqteaDW9ssL"
      },
      "source": [
        "### computy_entropy.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IKNAcKTl4KUF"
      },
      "outputs": [],
      "source": [
        "def calculate_move_probabilities(legal_moves, othello_rule, move_count):\n",
        "    \"\"\"Calculate the probability distribution over legal moves for a given rule\"\"\"\n",
        "    if not legal_moves:\n",
        "        return {}\n",
        "\n",
        "    if othello_rule in [OthelloRule.STANDARD, OthelloRule.NEXT_TO_OPPONENT]:\n",
        "        # Uniform distribution\n",
        "        prob = 1.0 / len(legal_moves)\n",
        "        return {move: prob for move in legal_moves}\n",
        "\n",
        "    elif othello_rule == OthelloRule.BIAS_CLOCK:\n",
        "        probs = {}\n",
        "\n",
        "        if move_count < 2:\n",
        "            # First two moves are random\n",
        "            prob = 1.0 / len(legal_moves)\n",
        "            return {move: prob for move in legal_moves}\n",
        "\n",
        "        # 80% chance of biased move, 20% chance of random\n",
        "        bias_prob = 0.8\n",
        "        random_prob = 0.2\n",
        "\n",
        "        # Calculate corner preference based on move count\n",
        "        corner = move_count % 4\n",
        "\n",
        "        # Find the preferred move for this corner\n",
        "        if corner == 0:  # top-left\n",
        "            preferred_move = min(legal_moves, key=lambda x: (x // 8 + x % 8, x // 8))\n",
        "        elif corner == 1:  # top-right\n",
        "            preferred_move = min(legal_moves, key=lambda x: (x // 8 + (7 - x % 8), x // 8))\n",
        "        elif corner == 2:  # bottom-right\n",
        "            preferred_move = min(legal_moves, key=lambda x: ((7 - x // 8) + (7 - x % 8), 7 - x // 8))\n",
        "        else:  # bottom-left\n",
        "            preferred_move = min(legal_moves, key=lambda x: ((7 - x // 8) + x % 8, 7 - x // 8))\n",
        "\n",
        "        # Calculate probabilities\n",
        "        uniform_prob_per_move = random_prob / len(legal_moves)\n",
        "\n",
        "        for move in legal_moves:\n",
        "            if move == preferred_move:\n",
        "                probs[move] = bias_prob + uniform_prob_per_move\n",
        "            else:\n",
        "                probs[move] = uniform_prob_per_move\n",
        "\n",
        "        return probs\n",
        "\n",
        "    else:\n",
        "        raise Exception(f\"Unknown othello rule: {othello_rule}\")\n",
        "\n",
        "def calculate_entropy_from_probabilities(move_probs):\n",
        "    \"\"\"Calculate entropy from a probability distribution\"\"\"\n",
        "    entropy = 0\n",
        "    for prob in move_probs.values():\n",
        "        if prob > 0:\n",
        "            entropy -= prob * np.log(prob)\n",
        "    return entropy\n",
        "\n",
        "def calculate_legal_move_entropy(dataset, n_games, othello_rule):\n",
        "    \"\"\"Calculate entropy based on the actual move distribution for the given rule\"\"\"\n",
        "    total_entropy = 0\n",
        "    total_positions = 0\n",
        "\n",
        "    for i in tqdm(range(min(n_games, len(dataset))), desc=f\"Calculating entropy for {othello_rule.value}\"):\n",
        "        x, y = dataset[i]\n",
        "        board = OthelloBoardState()\n",
        "        move_count = 0\n",
        "\n",
        "        for pos in range(len(x)):\n",
        "            if pos < len(y) and y[pos].item() != 0:  # Valid prediction position\n",
        "                if othello_rule in [OthelloRule.STANDARD, OthelloRule.BIAS_CLOCK]:\n",
        "                    legal_moves = board.get_valid_moves()\n",
        "                elif othello_rule == OthelloRule.NEXT_TO_OPPONENT:\n",
        "                    legal_moves = board.get_moves_next_to_opponent()\n",
        "                else:\n",
        "                    raise Exception(f\"Unknown othello rule: {othello_rule}\")\n",
        "                if len(legal_moves) > 1:  # Only count when there's choice\n",
        "                    # Calculate probabilities based on the rule\n",
        "                    move_probs = calculate_move_probabilities(legal_moves, othello_rule, move_count)\n",
        "\n",
        "                    # Calculate entropy from the probability distribution\n",
        "                    entropy = calculate_entropy_from_probabilities(move_probs)\n",
        "                    total_entropy += entropy\n",
        "                    total_positions += 1\n",
        "\n",
        "            # Apply actual move to advance board\n",
        "            if pos < len(x) and x[pos].item() != 0:\n",
        "                actual_square = id_to_square(x[pos].item())\n",
        "                if othello_rule in [OthelloRule.STANDARD, OthelloRule.BIAS_CLOCK]:\n",
        "                    board.umpire(actual_square)\n",
        "                elif othello_rule == OthelloRule.NEXT_TO_OPPONENT:\n",
        "                    board.place_next_to_opponent(actual_square)\n",
        "                else:\n",
        "                    raise Exception(f\"Unknown othello rule: {othello_rule}\")\n",
        "\n",
        "                move_count += 1\n",
        "\n",
        "    return total_entropy / total_positions if total_positions > 0 else 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHdQp6Pl8UZ2"
      },
      "source": [
        "# --- mingpt/ ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuqz9jya8wGF"
      },
      "source": [
        "### model.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "WjfVtvMg8wQz"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "GPT model:\n",
        "- the initial stem consists of a combination of token encoding and a positional encoding\n",
        "- the meat of it is a uniform sequence of Transformer blocks\n",
        "    - each Transformer is a sequential combination of a 1-hidden-layer MLP block and a self-attention block\n",
        "    - all blocks feed into a central residual pathway similar to resnets\n",
        "- the final decoder is a linear projection into a vanilla Softmax classifier\n",
        "\"\"\"\n",
        "\n",
        "class GPTConfig:\n",
        "    \"\"\" base GPT config, params common to all GPT versions \"\"\"\n",
        "    embd_pdrop = 0.1\n",
        "    resid_pdrop = 0.1\n",
        "    attn_pdrop = 0.1\n",
        "\n",
        "    def __init__(self, vocab_size, block_size, **kwargs):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        for k,v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
        "    It is possible to use t.nn.MultiheadAttention here but I am including an\n",
        "    explicit implementation here to show that there is nothing too scary here.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads\n",
        "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # regularization\n",
        "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
        "        self.resid_drop = nn.Dropout(config.resid_pdrop)\n",
        "        # output projection\n",
        "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "        self.register_buffer(\"mask\", t.tril(t.ones(config.block_size, config.block_size))\n",
        "                                     .view(1, 1, config.block_size, config.block_size))\n",
        "        self.n_head = config.n_head\n",
        "\n",
        "    def forward(self, x, layer_past=None, only_last=-1):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
        "        if only_last != -1:\n",
        "            att[:, :, -only_last:, :-only_last] = float('-inf')\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = self.attn_drop(att) @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_drop(self.proj(y))\n",
        "        return y, att\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" an unassuming Transformer block \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            nn.Dropout(config.resid_pdrop),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_att=False, only_last=-1):\n",
        "        updt, att = self.attn(self.ln1(x), only_last=only_last)\n",
        "        x = x + updt\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        if return_att:\n",
        "            return x, att\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        # input embedding stem\n",
        "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.pos_emb = nn.Parameter(t.zeros(1, config.block_size, config.n_embd))\n",
        "        self.drop = nn.Dropout(config.embd_pdrop)\n",
        "        # transformer\n",
        "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
        "        self.n_layer = config.n_layer\n",
        "        # decoder head\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
        "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        self.block_size = config.block_size\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def get_block_size(self):\n",
        "        return self.block_size\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def configure_optimizers(self, train_config):\n",
        "        \"\"\"\n",
        "        This long function is unfortunately doing something very simple and is being very defensive:\n",
        "        We are separating out all parameters of the model into two buckets: those that will experience\n",
        "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
        "        We are then returning the PyTorch optimizer object.\n",
        "        \"\"\"\n",
        "\n",
        "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
        "        decay = set()\n",
        "        no_decay = set()\n",
        "        whitelist_weight_modules = (t.nn.Linear, )\n",
        "        blacklist_weight_modules = (t.nn.LayerNorm, t.nn.Embedding)\n",
        "        for mn, m in self.named_modules():\n",
        "            for pn, p in m.named_parameters():\n",
        "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
        "\n",
        "                if pn.endswith('bias'):\n",
        "                    # all biases will not be decayed\n",
        "                    no_decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
        "                    # weights of whitelist modules will be weight decayed\n",
        "                    decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
        "                    # weights of blacklist modules will NOT be weight decayed\n",
        "                    no_decay.add(fpn)\n",
        "\n",
        "        # special case the position embedding parameter in the root GPT module as not decayed\n",
        "        no_decay.add('pos_emb')\n",
        "\n",
        "        # validate that we considered every parameter\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        inter_params = decay & no_decay\n",
        "        union_params = decay | no_decay\n",
        "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
        "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
        "                                                    % (str(param_dict.keys() - union_params), )\n",
        "\n",
        "        # create the pytorch optimizer object\n",
        "        optim_groups = [\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
        "        ]\n",
        "        optimizer = t.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
        "        return optimizer\n",
        "\n",
        "    def forward(self, idx, targets=None, y_soft_weak_supervision=None):\n",
        "        b, t = idx.size()  # both of shape [B, T]\n",
        "        assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
        "\n",
        "        # forward the GPT model\n",
        "        token_embeddings = self.tok_emb(idx) # each index maps to a (learnable) vector\n",
        "        position_embeddings = self.pos_emb[:, :t, :] # each position maps to a (learnable) vector\n",
        "        x = self.drop(token_embeddings + position_embeddings)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)  # [B, T, f]\n",
        "        logits = self.head(x)  # [B, T, # Words]\n",
        "        # if we are given some desired targets also calculate the loss\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            if y_soft_weak_supervision is not None:\n",
        "                mask = (targets != 0).float()  # [B, T]\n",
        "\n",
        "                # Reshape\n",
        "                logits_flat = logits.view(-1, logits.size(-1))  # [B*T, vocab_size]\n",
        "                soft_targets_flat = y_soft_weak_supervision.view(-1, y_soft_weak_supervision.size(-1))  # [B*T, vocab_size]\n",
        "                mask_flat = mask.view(-1)  # [B*T]\n",
        "\n",
        "                # Softmax with soft labels\n",
        "                loss_per_token = F.cross_entropy(logits_flat, soft_targets_flat, reduction='none')  # [B*T]\n",
        "\n",
        "                # Apply mask to ignore pass tokens\n",
        "                masked_loss = loss_per_token * mask_flat\n",
        "                if mask_flat.sum() > 0:\n",
        "                    loss = masked_loss.sum() / mask_flat.sum()\n",
        "                else:\n",
        "                    loss = 0\n",
        "            else:\n",
        "                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=0)  # -100 in the string space is mapped to 0 in the index space\n",
        "        return logits, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQl3qrr8SV5Z"
      },
      "source": [
        "### create_models.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4iOlCuVL3tmc"
      },
      "outputs": [],
      "source": [
        "class ModelSize(Enum):\n",
        "    NANO = \"nano\"\n",
        "    MICRO = \"micro\"\n",
        "    MINI = \"mini\"\n",
        "    SMALL = \"small\"\n",
        "    MEDIUM = \"medium\"\n",
        "    LARGE = \"large\"\n",
        "    HUGE = \"huge\"\n",
        "\n",
        "def get_weak_strong_pairs():\n",
        "   ordered_sizes = [\n",
        "       ModelSize.NANO,\n",
        "       ModelSize.MICRO,\n",
        "       ModelSize.MINI,\n",
        "       ModelSize.SMALL,\n",
        "       ModelSize.MEDIUM,\n",
        "       ModelSize.LARGE,\n",
        "       ModelSize.HUGE\n",
        "   ]\n",
        "   all_sizes = set(ModelSize)\n",
        "   assert set(ordered_sizes) == all_sizes, f\"Missing sizes: {all_sizes - set(ordered_sizes)}\"\n",
        "\n",
        "   # Generate (weak, strong) pairs where weak < strong\n",
        "   pairs = []\n",
        "   for i, weak in enumerate(ordered_sizes):\n",
        "       for strong in ordered_sizes[i+1:]:\n",
        "           pairs.append((weak, strong))\n",
        "\n",
        "   return pairs\n",
        "\n",
        "def get_model_sizes():\n",
        "    model_sizes = {}\n",
        "    model_sizes[ModelSize.NANO] = {\n",
        "        \"n_layer\": 1,\n",
        "        \"n_head\": 1,\n",
        "        \"n_embd\": 7,\n",
        "    }\n",
        "    model_sizes[ModelSize.MICRO] = {\n",
        "        \"n_layer\": 1,\n",
        "        \"n_head\": 2,\n",
        "        \"n_embd\": 20,\n",
        "    }\n",
        "    model_sizes[ModelSize.MINI] = {\n",
        "        \"n_layer\": 2,\n",
        "        \"n_head\": 2,\n",
        "        \"n_embd\": 38,\n",
        "    }\n",
        "    model_sizes[ModelSize.SMALL] = {\n",
        "        \"n_layer\": 3,\n",
        "        \"n_head\": 3,\n",
        "        \"n_embd\": 72,\n",
        "    }\n",
        "    model_sizes[ModelSize.MEDIUM] = {\n",
        "        \"n_layer\": 4,\n",
        "        \"n_head\": 5,\n",
        "        \"n_embd\": 140,\n",
        "    }\n",
        "    model_sizes[ModelSize.LARGE] = {\n",
        "        \"n_layer\": 6,\n",
        "        \"n_head\": 6,\n",
        "        \"n_embd\": 264,\n",
        "    }\n",
        "    model_sizes[ModelSize.HUGE] = {\n",
        "        \"n_layer\": 8,\n",
        "        \"n_head\": 8,\n",
        "        \"n_embd\": 512,\n",
        "    }\n",
        "    return model_sizes\n",
        "\n",
        "\n",
        "def get_model_config(model_size: ModelSize, train_dataset):\n",
        "    size_params = get_model_sizes()[model_size]\n",
        "    return GPTConfig(train_dataset.vocab_size, train_dataset.block_size, n_layer=size_params[\"n_layer\"], n_head=size_params[\"n_head\"], n_embd=size_params[\"n_embd\"])\n",
        "\n",
        "\n",
        "def format_integer_scientific(n: float) -> str:\n",
        "    s = f\"{n:.1e}\"\n",
        "    return s.replace(\"e+\", \" * 10^\")\n",
        "\n",
        "def count_parameters(model: nn.Module) -> int:\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "def print_model_ratios(train_dataset) -> None:\n",
        "    last = 0\n",
        "    ratios = []\n",
        "    for model_size in ModelSize:\n",
        "        cfg = get_model_config(model_size, train_dataset)\n",
        "        model = GPT(cfg)\n",
        "        n = count_parameters(model)\n",
        "        if last != 0:\n",
        "            ratios.append(n / last)\n",
        "        last = n\n",
        "        print(model_size, format_integer_scientific(n))\n",
        "        del model\n",
        "    print(\"Ratio of consecutive model-sizes: \", ratios)\n",
        "\n",
        "\n",
        "# print_model_ratios(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOj-51o88zVd"
      },
      "source": [
        "### utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "nF8ChLr38zdB"
      },
      "outputs": [],
      "source": [
        "def top_k_logits(logits, k):\n",
        "    \"\"\"Sets all logits that are not in the top k to -inf.\"\"\"\n",
        "    v, ix = t.topk(logits, k)\n",
        "    out = logits.clone()\n",
        "    out[out < v[:, [-1]]] = -float('Inf')\n",
        "    return out\n",
        "\n",
        "@t.no_grad()\n",
        "def sample(model, x, steps, temperature=1.0, sample=False, top_k=None):\n",
        "    \"\"\"\n",
        "    take a conditioning sequence of indices in x (of shape (b,t)) and predict the next token in\n",
        "    the sequence, feeding the predictions back into the model each time. Clearly the sampling\n",
        "    has quadratic complexity unlike an RNN that is only linear, and has a finite context window\n",
        "    of block_size, unlike an RNN that has an infinite context window.\n",
        "    \"\"\"\n",
        "    block_size = model.get_block_size()\n",
        "    model.eval()\n",
        "    for k in range(steps):\n",
        "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
        "        logits, _ = model(x_cond)\n",
        "        # pluck the logits at the final step and scale by temperature\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        # optionally crop probabilities to only the top k options\n",
        "        if top_k is not None:\n",
        "            logits = top_k_logits(logits, top_k)\n",
        "        # apply softmax to convert to probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # sample from the distribution or take the most likely\n",
        "        if sample:\n",
        "            ix = t.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            _, ix = t.topk(probs, k=1, dim=-1)\n",
        "        # append to the sequence and continue\n",
        "        x = t.cat((x, ix), dim=1)\n",
        "\n",
        "    return x\n",
        "\n",
        "def count_parameters(model: nn.Module) -> int:\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "def get_model_size(model):\n",
        "  total_params = sum(p.numel() for p in model.parameters())\n",
        "  print(f\"Total parameters: {total_params:,}\")\n",
        "  model_size_mb = total_params * 4 / (1024 * 1024)\n",
        "  print(f\"Model size: {model_size_mb:.1f} MB\")\n",
        "  if t.cuda.is_available():\n",
        "      print(f\"GPU memory: {t.cuda.memory_allocated() / 1024**2:.1f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjTXE9OjZIrm"
      },
      "source": [
        "### convert_gpt_and_hooked.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "XN3HTkKmAVWU"
      },
      "outputs": [],
      "source": [
        "def convert_gpt_to_hooked(gpt_model, gpt_config):\n",
        "    \"\"\"Convert custom GPT model to HookedTransformer using built-in converter\"\"\"\n",
        "    # Create HookedTransformer config\n",
        "    hooked_config = HookedTransformerConfig(\n",
        "        n_layers=gpt_config.n_layer,\n",
        "        d_model=gpt_config.n_embd,\n",
        "        d_head=gpt_config.n_embd // gpt_config.n_head,\n",
        "        n_heads=gpt_config.n_head,\n",
        "        d_mlp=4 * gpt_config.n_embd,\n",
        "        d_vocab=gpt_config.vocab_size,\n",
        "        n_ctx=gpt_config.block_size,\n",
        "        act_fn=\"gelu\",\n",
        "        normalization_type=\"LN\",\n",
        "        positional_embedding_type=\"standard\",\n",
        "    )\n",
        "\n",
        "    # Create HookedTransformer\n",
        "    hooked_model = HookedTransformer(hooked_config)\n",
        "\n",
        "    # Use built-in converter\n",
        "    hooked_state_dict = convert_mingpt_weights(gpt_model.state_dict(), hooked_config)\n",
        "    hooked_model.load_state_dict(hooked_state_dict, strict=False)\n",
        "\n",
        "    return hooked_model\n",
        "\n",
        "def verify_identical_outputs(model_1, model_2, test_input):\n",
        "    \"\"\"Verify both models produce identical outputs\"\"\"\n",
        "    model_1.eval()\n",
        "    model_2.eval()\n",
        "\n",
        "    with t.no_grad():\n",
        "        gpt_logits, _ = model_1(test_input)\n",
        "        hooked_logits = model_2(test_input)\n",
        "\n",
        "        mean_diff = (gpt_logits - hooked_logits).abs().mean().item()\n",
        "        if mean_diff > 1e-4:\n",
        "            print(f\"Mean difference between outputs: {mean_diff}\")\n",
        "\n",
        "def convert_gpt_to_hooked_and_verify(model, conf, val_dataset, device='cuda', n_samples=100):\n",
        "  model = model.to(device)\n",
        "  hooked_model = convert_gpt_to_hooked(model, conf)\n",
        "  hooked_model = hooked_model.to(device)\n",
        "  test_inputs = t.stack([val_dataset[i][0] for i in range(n_samples)]).to(device)\n",
        "  verify_identical_outputs(model, hooked_model, test_inputs)\n",
        "  return hooked_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7arj-ecp3KpV"
      },
      "outputs": [],
      "source": [
        "def convert_hooked_state_dict_to_gpt(hooked_model, gpt_config):\n",
        "    \"\"\"\n",
        "    Convert HookedTransformer state dict to GPT state dict.\n",
        "\n",
        "    This reverses the convert_mingpt_weights function from transformer_lens.\n",
        "    \"\"\"\n",
        "    hooked_state_dict = hooked_model.state_dict()\n",
        "    gpt_state_dict = {}\n",
        "\n",
        "    # Token embeddings\n",
        "    gpt_state_dict['tok_emb.weight'] = hooked_state_dict['embed.W_E']\n",
        "\n",
        "    # Position embeddings: [n_ctx, d_model] -> [1, n_ctx, d_model]\n",
        "    gpt_state_dict['pos_emb'] = hooked_state_dict['pos_embed.W_pos'].unsqueeze(0)\n",
        "\n",
        "    # Convert transformer blocks\n",
        "    for layer_idx in range(gpt_config.n_layer):\n",
        "        hooked_prefix = f'blocks.{layer_idx}'\n",
        "        gpt_prefix = f'blocks.{layer_idx}'\n",
        "\n",
        "        # Layer norms\n",
        "        if hooked_model.cfg.normalization_type == \"LNPre\":\n",
        "            # Set identity LayerNorm (w=1, b=0)\n",
        "            gpt_state_dict[f'{gpt_prefix}.ln1.weight'] = t.ones(gpt_config.n_embd)\n",
        "            gpt_state_dict[f'{gpt_prefix}.ln1.bias'] = t.zeros(gpt_config.n_embd)\n",
        "            gpt_state_dict[f'{gpt_prefix}.ln2.weight'] = t.ones(gpt_config.n_embd)\n",
        "            gpt_state_dict[f'{gpt_prefix}.ln2.bias'] = t.zeros(gpt_config.n_embd)\n",
        "        else:\n",
        "            # Use existing parameters\n",
        "            gpt_state_dict[f'{gpt_prefix}.ln1.weight'] = hooked_state_dict[f'{hooked_prefix}.ln1.w']\n",
        "            gpt_state_dict[f'{gpt_prefix}.ln1.bias'] = hooked_state_dict[f'{hooked_prefix}.ln1.b']\n",
        "            gpt_state_dict[f'{gpt_prefix}.ln2.weight'] = hooked_state_dict[f'{hooked_prefix}.ln2.w']\n",
        "            gpt_state_dict[f'{gpt_prefix}.ln2.bias'] = hooked_state_dict[f'{hooked_prefix}.ln2.b']\n",
        "\n",
        "        # Attention weights\n",
        "        # The key insight: convert_mingpt_weights splits the GPT combined QKV into separate Q,K,V\n",
        "        # So we need to do the reverse: combine separate Q,K,V back into the GPT format\n",
        "\n",
        "        n_heads = gpt_config.n_head\n",
        "        d_model = gpt_config.n_embd\n",
        "        d_head = d_model // n_heads\n",
        "\n",
        "        # Get the separate Q, K, V weights: each is [n_heads, d_model, d_head]\n",
        "        W_Q = hooked_state_dict[f'{hooked_prefix}.attn.W_Q']  # [n_heads, d_model, d_head]\n",
        "        W_K = hooked_state_dict[f'{hooked_prefix}.attn.W_K']  # [n_heads, d_model, d_head]\n",
        "        W_V = hooked_state_dict[f'{hooked_prefix}.attn.W_V']  # [n_heads, d_model, d_head]\n",
        "\n",
        "        # The way convert_mingpt_weights does it (in reverse):\n",
        "        # It reshapes [d_model, d_model] to [n_heads, d_model, d_head] for each of Q,K,V\n",
        "        # So we need to reshape back: [n_heads, d_model, d_head] -> [d_model, d_model]\n",
        "\n",
        "        # Reshape each: [n_heads, d_model, d_head] -> [d_model, n_heads * d_head] = [d_model, d_model]\n",
        "        W_Q_reshaped = W_Q.transpose(0, 1).reshape(d_model, d_model)  # [d_model, d_model]\n",
        "        W_K_reshaped = W_K.transpose(0, 1).reshape(d_model, d_model)  # [d_model, d_model]\n",
        "        W_V_reshaped = W_V.transpose(0, 1).reshape(d_model, d_model)  # [d_model, d_model]\n",
        "\n",
        "        # Transpose for Linear layer weight format [out_features, in_features]\n",
        "        gpt_state_dict[f'{gpt_prefix}.attn.query.weight'] = W_Q_reshaped.T\n",
        "        gpt_state_dict[f'{gpt_prefix}.attn.key.weight'] = W_K_reshaped.T\n",
        "        gpt_state_dict[f'{gpt_prefix}.attn.value.weight'] = W_V_reshaped.T\n",
        "\n",
        "        # Attention biases\n",
        "        # Get separate biases: each is [n_heads, d_head]\n",
        "        b_Q = hooked_state_dict[f'{hooked_prefix}.attn.b_Q']  # [n_heads, d_head]\n",
        "        b_K = hooked_state_dict[f'{hooked_prefix}.attn.b_K']  # [n_heads, d_head]\n",
        "        b_V = hooked_state_dict[f'{hooked_prefix}.attn.b_V']  # [n_heads, d_head]\n",
        "\n",
        "        # Reshape: [n_heads, d_head] -> [n_heads * d_head] = [d_model]\n",
        "        gpt_state_dict[f'{gpt_prefix}.attn.query.bias'] = b_Q.reshape(-1)\n",
        "        gpt_state_dict[f'{gpt_prefix}.attn.key.bias'] = b_K.reshape(-1)\n",
        "        gpt_state_dict[f'{gpt_prefix}.attn.value.bias'] = b_V.reshape(-1)\n",
        "\n",
        "        # Attention output projection W_O\n",
        "        W_O = hooked_state_dict[f'{hooked_prefix}.attn.W_O']  # [n_heads, d_head, d_model]\n",
        "\n",
        "        # Reshape: [n_heads, d_head, d_model] -> [n_heads * d_head, d_model] = [d_model, d_model]\n",
        "        W_O_reshaped = W_O.reshape(n_heads * d_head, d_model)  # [d_model, d_model]\n",
        "\n",
        "        gpt_state_dict[f'{gpt_prefix}.attn.proj.weight'] = W_O_reshaped.T\n",
        "\n",
        "        # Attention output bias\n",
        "        gpt_state_dict[f'{gpt_prefix}.attn.proj.bias'] = hooked_state_dict[f'{hooked_prefix}.attn.b_O']\n",
        "\n",
        "        # MLP weights\n",
        "        gpt_state_dict[f'{gpt_prefix}.mlp.0.weight'] = hooked_state_dict[f'{hooked_prefix}.mlp.W_in'].T\n",
        "        gpt_state_dict[f'{gpt_prefix}.mlp.2.weight'] = hooked_state_dict[f'{hooked_prefix}.mlp.W_out'].T\n",
        "\n",
        "        # MLP biases\n",
        "        gpt_state_dict[f'{gpt_prefix}.mlp.0.bias'] = hooked_state_dict[f'{hooked_prefix}.mlp.b_in']\n",
        "        gpt_state_dict[f'{gpt_prefix}.mlp.2.bias'] = hooked_state_dict[f'{hooked_prefix}.mlp.b_out']\n",
        "\n",
        "    # Final layer norm\n",
        "    # Final layer norm\n",
        "    if hooked_model.cfg.normalization_type == \"LNPre\":\n",
        "        gpt_state_dict['ln_f.weight'] = t.ones(gpt_config.n_embd)\n",
        "        gpt_state_dict['ln_f.bias'] = t.zeros(gpt_config.n_embd)\n",
        "    else:\n",
        "        gpt_state_dict['ln_f.weight'] = hooked_state_dict['ln_final.w']\n",
        "        gpt_state_dict['ln_f.bias'] = hooked_state_dict['ln_final.b']\n",
        "\n",
        "    # Output head (unembedding)\n",
        "    gpt_state_dict['head.weight'] = hooked_state_dict['unembed.W_U'].T\n",
        "\n",
        "    return gpt_state_dict\n",
        "\n",
        "\n",
        "def convert_hooked_to_gpt(hooked_model, device):\n",
        "    \"\"\"\n",
        "    Convert HookedTransformer model back to custom GPT model format.\n",
        "    Returns: (gpt_model, gpt_config)\n",
        "    \"\"\"\n",
        "    hooked_config = hooked_model.cfg\n",
        "    gpt_config = GPTConfig(\n",
        "        vocab_size=hooked_config.d_vocab,\n",
        "        block_size=hooked_config.n_ctx,\n",
        "        n_layer=hooked_config.n_layers,\n",
        "        n_head=hooked_config.n_heads,\n",
        "        n_embd=hooked_config.d_model,\n",
        "    )\n",
        "    gpt_model = GPT(gpt_config)\n",
        "    gpt_model.to(device)\n",
        "    gpt_state_dict = convert_hooked_state_dict_to_gpt(hooked_model, gpt_config)\n",
        "\n",
        "    # Load state dict with strict=False to ignore missing mask buffers and unexpected bias\n",
        "    missing_keys, unexpected_keys = gpt_model.load_state_dict(gpt_state_dict, strict=False)\n",
        "    # assert len(missing_keys) == 0, f\"Missing keys: {missing_keys}\"\n",
        "    # assert len(unexpected_keys) == 0, f\"Unexpected keys: {unexpected_keys}\"\n",
        "\n",
        "    return gpt_model, gpt_config\n",
        "\n",
        "\n",
        "def convert_hooked_to_gpt_and_verify(hooked_model, val_dataset, device, n_samples=100):\n",
        "    gpt_model, gpt_config = convert_hooked_to_gpt(hooked_model, device)\n",
        "    gpt_model = gpt_model.to(device)\n",
        "    test_inputs = t.stack([val_dataset[i][0] for i in range(n_samples)]).to(device)\n",
        "    verify_identical_outputs(gpt_model, hooked_model, test_inputs)\n",
        "    return gpt_model, gpt_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "rE6KxMK4Lgkz"
      },
      "outputs": [],
      "source": [
        "# https://github.com/adamkarvonen/chess_llm_interpretability/blob/main/utils/nanogpt_to_transformer_lens.ipynb\n",
        "\n",
        "def convert_nanogpt_weights(\n",
        "    old_state_dict, cfg: HookedTransformerConfig, bias: bool = False, device='cpu'\n",
        "):\n",
        "    \"\"\"For https://github.com/karpathy/nanoGPT\n",
        "    There are two complications with converting nanogpt models:\n",
        "    The first is that some state dicts have an unwanted prefix on keys that needs to be removed.\n",
        "    The second is that the models can be saved with or without bias. By default, there\n",
        "    is no bias. This function can handle both cases.\"\"\"\n",
        "    # Nanogpt models saved after torch.compile() have this unwanted prefix\n",
        "    # This is a simple way to remove it\n",
        "    unwanted_prefix = \"_orig_mod.\"\n",
        "    for k, v in list(old_state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            old_state_dict[k[len(unwanted_prefix) :]] = old_state_dict.pop(k)\n",
        "\n",
        "    new_state_dict = {}\n",
        "    new_state_dict[\"pos_embed.W_pos\"] = old_state_dict[\"transformer.wpe.weight\"]\n",
        "    new_state_dict[\"embed.W_E\"] = old_state_dict[\"transformer.wte.weight\"]\n",
        "\n",
        "    new_state_dict[\"ln_final.w\"] = old_state_dict[\"transformer.ln_f.weight\"]\n",
        "    new_state_dict[\"ln_final.b\"] = t.zeros_like(\n",
        "        old_state_dict[\"transformer.ln_f.weight\"]\n",
        "    )\n",
        "    new_state_dict[\"unembed.W_U\"] = old_state_dict[\"lm_head.weight\"].T\n",
        "\n",
        "    if bias:\n",
        "        new_state_dict[\"ln_final.b\"] = old_state_dict[\"transformer.ln_f.bias\"]\n",
        "\n",
        "    for layer in range(cfg.n_layers):\n",
        "        layer_key = f\"transformer.h.{layer}\"\n",
        "\n",
        "        new_state_dict[f\"blocks.{layer}.ln1.w\"] = old_state_dict[\n",
        "            f\"{layer_key}.ln_1.weight\"\n",
        "        ]\n",
        "        # A bias of zeros is required for folding layer norm\n",
        "        new_state_dict[f\"blocks.{layer}.ln1.b\"] = t.zeros_like(\n",
        "            old_state_dict[f\"{layer_key}.ln_1.weight\"]\n",
        "        )\n",
        "        new_state_dict[f\"blocks.{layer}.ln2.w\"] = old_state_dict[\n",
        "            f\"{layer_key}.ln_2.weight\"\n",
        "        ]\n",
        "        new_state_dict[f\"blocks.{layer}.ln2.b\"] = t.zeros_like(\n",
        "            old_state_dict[f\"{layer_key}.ln_2.weight\"]\n",
        "        )\n",
        "\n",
        "        W = old_state_dict[f\"{layer_key}.attn.c_attn.weight\"]\n",
        "        W_Q, W_K, W_V = t.tensor_split(W, 3, dim=0)\n",
        "        W_Q = einops.rearrange(W_Q, \"(i h) m->i m h\", i=cfg.n_heads)\n",
        "        W_K = einops.rearrange(W_K, \"(i h) m->i m h\", i=cfg.n_heads)\n",
        "        W_V = einops.rearrange(W_V, \"(i h) m->i m h\", i=cfg.n_heads)\n",
        "        new_state_dict[f\"blocks.{layer}.attn.W_Q\"] = W_Q\n",
        "        new_state_dict[f\"blocks.{layer}.attn.W_K\"] = W_K\n",
        "        new_state_dict[f\"blocks.{layer}.attn.W_V\"] = W_V\n",
        "\n",
        "        W_O = old_state_dict[f\"{layer_key}.attn.c_proj.weight\"]\n",
        "        W_O = einops.rearrange(W_O, \"m (i h)->i h m\", i=cfg.n_heads)\n",
        "        new_state_dict[f\"blocks.{layer}.attn.W_O\"] = W_O\n",
        "\n",
        "        new_state_dict[f\"blocks.{layer}.mlp.W_in\"] = old_state_dict[\n",
        "            f\"{layer_key}.mlp.c_fc.weight\"\n",
        "        ].T\n",
        "        new_state_dict[f\"blocks.{layer}.mlp.W_out\"] = old_state_dict[\n",
        "            f\"{layer_key}.mlp.c_proj.weight\"\n",
        "        ].T\n",
        "\n",
        "        if bias:\n",
        "            new_state_dict[f\"blocks.{layer}.ln1.b\"] = old_state_dict[\n",
        "                f\"{layer_key}.ln_1.bias\"\n",
        "            ]\n",
        "            new_state_dict[f\"blocks.{layer}.ln2.b\"] = old_state_dict[\n",
        "                f\"{layer_key}.ln_2.bias\"\n",
        "            ]\n",
        "            new_state_dict[f\"blocks.{layer}.mlp.b_in\"] = old_state_dict[\n",
        "                f\"{layer_key}.mlp.c_fc.bias\"\n",
        "            ]\n",
        "            new_state_dict[f\"blocks.{layer}.mlp.b_out\"] = old_state_dict[\n",
        "                f\"{layer_key}.mlp.c_proj.bias\"\n",
        "            ]\n",
        "\n",
        "            B = old_state_dict[f\"{layer_key}.attn.c_attn.bias\"]\n",
        "            B_Q, B_K, B_V = t.tensor_split(B, 3, dim=0)\n",
        "            B_Q = einops.rearrange(B_Q, \"(i h)->i h\", i=cfg.n_heads)\n",
        "            B_K = einops.rearrange(B_K, \"(i h)->i h\", i=cfg.n_heads)\n",
        "            B_V = einops.rearrange(B_V, \"(i h)->i h\", i=cfg.n_heads)\n",
        "            new_state_dict[f\"blocks.{layer}.attn.b_Q\"] = B_Q\n",
        "            new_state_dict[f\"blocks.{layer}.attn.b_K\"] = B_K\n",
        "            new_state_dict[f\"blocks.{layer}.attn.b_V\"] = B_V\n",
        "            new_state_dict[f\"blocks.{layer}.attn.b_O\"] = old_state_dict[\n",
        "                f\"{layer_key}.attn.c_proj.bias\"\n",
        "            ]\n",
        "\n",
        "    return new_state_dict\n",
        "\n",
        "\n",
        "def convert_nanogpt_to_hooked_and_verify(nanogpt_model_dict, device):\n",
        "    # for name, param in nanogpt_model_dict.items():\n",
        "    #     if name.startswith(\"transformer.h.0\") or not name.startswith(\"transformer.h\"):\n",
        "    #         print(name, param.shape)\n",
        "\n",
        "    cfg = HookedTransformerConfig(\n",
        "        n_layers = 8,\n",
        "        d_model = 512,\n",
        "        d_head = 64,\n",
        "        n_heads = 8,\n",
        "        d_mlp = 2048,\n",
        "        d_vocab = 32,\n",
        "        n_ctx = 1023,\n",
        "        act_fn=\"gelu\",\n",
        "        normalization_type=\"LNPre\"\n",
        "    )\n",
        "    model = HookedTransformer(cfg)\n",
        "    model.load_and_process_state_dict(convert_nanogpt_weights(nanogpt_model_dict, cfg, bias=True, device=device))\n",
        "    sample_input = t.tensor([[6, 4, 27, 9, 0, 27, 11, 0, 7, 4, 19, 28, 8, 0, 26, 10, 0, 8, 4, 19, 25, 8, 0, 26, 9, 0, 9, 4, 19, 27, 7, 0, 25, 10, 0, 10, 4, 25, 8, 0, 26, 8, 0, 11, 4, 19, 28, 9, 0, 25, 9, 0, 12, 4, 21, 23, 9, 2, 0, 17, 26, 12, 0, 13, 4, 21, 31, 25, 9, 0, 19, 28, 11, 0, 14, 4, 27, 10, 0, 19, 29, 9, 0, 6, 5, 4, 30, 8, 0, 19, 31, 28, 7, 0, 6, 6, 4, 18, 31, 28, 7, 0, 21, 24, 11, 2, 0, 6, 7, 4, 18, 27, 6, 0, 17, 24, 10, 0, 6, 8, 4, 21, 25, 13, 2, 0, 18, 27, 12, 0, 6, 9, 4, 17, 31, 26, 8, 0, 17, 26, 12, 0, 6, 10, 4, 21, 25, 9, 0, 19, 25, 11, 0, 6, 11, 4, 17, 27, 9, 0, 22, 25, 13, 0, 6, 12, 4, 21, 24, 8, 0, 21, 25, 12, 0, 6, 13, 4, 26, 9, 0, 22, 24, 13, 0, 6, 14, 4, 17, 27, 8, 0, 19, 23, 10, 0, 7, 5, 4, 21, 26, 6, 0, 29, 11, 0, 7, 6, 4, 17, 26, 8, 0, 17, 29, 12, 0, 7, 7, 4, 22, 28, 6, 0, 19, 25, 11, 0, 7, 8, 4, 18, 28, 7, 0, 22, 30, 27, 13, 0, 7, 9, 4, 18, 29, 6, 0, 30, 11, 0, 7, 10, 4, 22, 25, 6, 0, 29, 10, 0, 7, 11, 4, 19, 30, 10, 0, 17, 30, 13, 0, 7, 12, 4, 19, 26, 7, 0, 21, 24, 11, 0, 7, 13, 4, 19, 28, 11, 0, 22, 27, 26, 13, 0, 7, 14, 4, 19, 31, 26, 12, 0, 22, 31, 26, 12, 0, 8, 5, 4, 21, 28, 8, 0, 21, 31, 24, 7, 0, 8, 6, 4, 21, 31, 28, 12, 2, 0, 18, 26, 13, 0, 8, 7, 4, 21, 28, 13, 2]])\n",
        "    model(sample_input)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vS-ZcVE_clMs"
      },
      "outputs": [],
      "source": [
        "# For models with \"LN\", 1e-5 error, if \"LNPre\", then 0.06 (othello_gpt) -> small error\n",
        "# But if we only use them for finetuning, it should be fine.\n",
        "\n",
        "# othello_gpt = load_othello_gpt()\n",
        "# loaded_hooked_model = load_tiny_stories_instruct_8m(DEVICE)\n",
        "# gpt_model, gpt_config = convert_hooked_to_gpt_and_verify(loaded_hooked_model, val_dataset, DEVICE)\n",
        "# hooked_model = convert_gpt_to_hooked_and_verify(gpt_model, gpt_config, val_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agBm7yQHYXAp"
      },
      "source": [
        "### load_pretrained_models.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9aCqWiFnYXQL"
      },
      "outputs": [],
      "source": [
        "def load_othello_gpt(device):\n",
        "  cfg = HookedTransformerConfig(\n",
        "      n_layers=8,\n",
        "      d_model=512,\n",
        "      d_head=64,\n",
        "      n_heads=8,\n",
        "      d_mlp=2048,\n",
        "      d_vocab=61,\n",
        "      n_ctx=59,\n",
        "      act_fn=\"gelu\",\n",
        "      normalization_type=\"LNPre\",\n",
        "      device=device,\n",
        "  )\n",
        "  model = HookedTransformer(cfg)\n",
        "\n",
        "  state_dict_synthetic = download_file_from_hf(\"NeelNanda/Othello-GPT-Transformer-Lens\", \"synthetic_model.pth\")\n",
        "  # state_dict_championship = download_file_from_hf(\"NeelNanda/Othello-GPT-Transformer-Lens\", \"championship_model.pth\")\n",
        "  model.load_state_dict(state_dict_synthetic)\n",
        "  return model\n",
        "\n",
        "\n",
        "def transform_hooked_transformer_to_othello_vocab(hooked_model):\n",
        "    # Create new model with Othello configuration\n",
        "    othello_cfg = HookedTransformerConfig(\n",
        "        n_layers=hooked_model.cfg.n_layers,\n",
        "        d_model=hooked_model.cfg.d_model,\n",
        "        d_head=hooked_model.cfg.d_head,\n",
        "        n_heads=hooked_model.cfg.n_heads,\n",
        "        d_mlp=hooked_model.cfg.d_mlp,\n",
        "        d_vocab=61,\n",
        "        n_ctx=59,\n",
        "        act_fn=hooked_model.cfg.act_fn,\n",
        "        normalization_type=hooked_model.cfg.normalization_type,\n",
        "        device=hooked_model.cfg.device,\n",
        "    )\n",
        "\n",
        "    othello_model = HookedTransformer(othello_cfg)\n",
        "\n",
        "    # Copy all weights except embeddings\n",
        "    with t.no_grad():\n",
        "        state_dict = hooked_model.state_dict()\n",
        "        new_state_dict = othello_model.state_dict()\n",
        "\n",
        "        for name, param in state_dict.items():\n",
        "            if not any(embed_name in name for embed_name in ['embed', 'mask',]):  # unembed and pos_embed might work without   #'pos_embed', 'unembed',\n",
        "                new_state_dict[name].copy_(param)\n",
        "            else:\n",
        "                print(f\"Skipping {name}: not in new state dict\")\n",
        "            # if name not in new_state_dict:\n",
        "            #     print(f\"Skipping {name}: not in new state dict\")\n",
        "            # elif not param.shape == new_state_dict[name].shape:\n",
        "            #     print(f\"Skipping {name}: shape mismatch\")\n",
        "            # else:\n",
        "            #     new_state_dict[name].copy_(param)\n",
        "    return othello_model\n",
        "\n",
        "\n",
        "def load_tiny_stories_instruct_8m(device, force_othello_vocab: bool = True) -> HookedTransformer:\n",
        "    hooked_model = HookedTransformer.from_pretrained(\"roneneldan/TinyStories-Instruct-8M\", device=DEVICE)\n",
        "    if force_othello_vocab:\n",
        "        return transform_hooked_transformer_to_othello_vocab(hooked_model)\n",
        "    else:\n",
        "        return hooked_model\n",
        "\n",
        "\n",
        "def load_hooked_chess_gpt(device, force_othello_vocab: bool = True) -> HookedTransformer:\n",
        "    t.set_grad_enabled(False)\n",
        "    # model_url = 'https://huggingface.co/adamkarvonen/nanogpt_transformer_lens_examples/resolve/main/ckpt_8layers_no_bias_no_opt.pt'\n",
        "    model_url = 'https://huggingface.co/adamkarvonen/nanogpt_transformer_lens_examples/resolve/main/ckpt_8layers_bias_no_opt.pt'\n",
        "    checkpoint = t.hub.load_state_dict_from_url(model_url, map_location=device)\n",
        "    nanogpt_model_dict = checkpoint[\"model\"]\n",
        "    hooked_chess_gpt = convert_nanogpt_to_hooked_and_verify(nanogpt_model_dict, device=device)\n",
        "    if force_othello_vocab:\n",
        "        return transform_hooked_transformer_to_othello_vocab(hooked_chess_gpt)\n",
        "    else:\n",
        "        return hooked_chess_gpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qhbM1hZN-cko"
      },
      "outputs": [],
      "source": [
        "# chess_gpt = load_chess_gpt(DEVICE, force_othello_vocab=False)\n",
        "# chess_gpt.cfg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThMvq0yOYnXU"
      },
      "source": [
        "### test_hooked_transformer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4jIIkUcUYnhS"
      },
      "outputs": [],
      "source": [
        "def test_hooked_transformer(model, othello_rule, test_dataset, batch_size, n_games, device):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "        dict: {\n",
        "            'ce_loss': float,\n",
        "            'illegal_move_percentage': float,\n",
        "            'total_predictions': int\n",
        "        }\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    test_subset = Subset(test_dataset, range(n_games))\n",
        "    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
        "    total_loss = 0\n",
        "    total_predictions = 0\n",
        "    illegal_moves = 0\n",
        "    with t.no_grad():\n",
        "        for x, y in tqdm(test_loader, desc=\"Validating\"):\n",
        "            x = x.to(device)  # [B, T]\n",
        "            y = y.to(device)  # [B, T]\n",
        "            B = x.size(0)\n",
        "            T = x.size(1)\n",
        "\n",
        "            # Sum of CE loss\n",
        "            # logits = model(x)\n",
        "            logits = model(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=0)\n",
        "            total_loss += loss.item() * B\n",
        "\n",
        "            # Check illegal moves\n",
        "            for b in range(B):\n",
        "                board = OthelloBoardState()\n",
        "                for t_step in range(T):\n",
        "                    if t_step > 0:  # Skip first move since we don't predict it\n",
        "                        pred_token_id = logits[b, t_step-1].argmax().item()\n",
        "                        if pred_token_id != 0:  # Skip padding tokens\n",
        "                            pred_square = id_to_square(pred_token_id)\n",
        "                            if othello_rule in [OthelloRule.STANDARD, OthelloRule.BIAS_CLOCK, OthelloRule.CHESS, OthelloRule.UNTRAINED, OthelloRule.CONSTANT_PARAMETERS]:\n",
        "                                valid_moves = board.get_valid_moves()\n",
        "                            elif othello_rule == OthelloRule.NEXT_TO_OPPONENT:\n",
        "                                valid_moves = board.get_moves_next_to_opponent()\n",
        "                            elif othello_rule == OthelloRule.NO_FLIPPING:\n",
        "                                is_occupied = board.get_occupied()\n",
        "                                valid_moves = [i for i in range(64) if is_occupied[i] == 0]\n",
        "                            else:\n",
        "                                raise ValueError(f\"Invalid OthelloRule: {othello_rule}\")\n",
        "                            if pred_square not in valid_moves:\n",
        "                                illegal_moves += 1\n",
        "\n",
        "                            total_predictions += 1\n",
        "\n",
        "                    # Apply actual move to advance board state\n",
        "                    if t_step < T - 1:  # Don't predict beyond sequence\n",
        "                        actual_token_id = x[b, t_step].item()\n",
        "                        if actual_token_id != 0:  # Skip padding\n",
        "                            actual_square = id_to_square(actual_token_id)\n",
        "                            if othello_rule in [OthelloRule.STANDARD, OthelloRule.BIAS_CLOCK, OthelloRule.CHESS, OthelloRule.UNTRAINED, OthelloRule.CONSTANT_PARAMETERS]:\n",
        "                                board.umpire(actual_square)\n",
        "                            elif othello_rule == OthelloRule.NEXT_TO_OPPONENT:\n",
        "                                board.place_next_to_opponent(actual_square)\n",
        "                            elif othello_rule == OthelloRule.NO_FLIPPING:\n",
        "                                board.place_no_flipping(actual_square)\n",
        "                            else:\n",
        "                                raise ValueError(f\"Invalid OthelloRule: {othello_rule}\")\n",
        "\n",
        "    # Calculate metrics\n",
        "    avg_ce_loss = total_loss / len(test_subset)\n",
        "    illegal_percentage = (illegal_moves / total_predictions * 100) if total_predictions > 0.0 else 0.0\n",
        "\n",
        "    return {\n",
        "        'ce_loss': avg_ce_loss,\n",
        "        'illegal_move_percentage': illegal_percentage,\n",
        "        'total_predictions': total_predictions,\n",
        "        'illegal_moves': illegal_moves\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tc8OKY5ZhHqn"
      },
      "source": [
        "### save_load_models.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "1Pt7Y0Q5hH1g"
      },
      "outputs": [],
      "source": [
        "def save_model(\n",
        "    model,\n",
        "    run_id: str,\n",
        "    project_name: str,\n",
        "    experiment_name: str,\n",
        "    experiment_folder: str,\n",
        "    index: int,\n",
        "    overwrite: bool,\n",
        "    final: bool,\n",
        "    linear_probe: bool = False,\n",
        "    fake_probe: str = \"\",\n",
        ") -> None:\n",
        "    project_dir = os.path.join(experiment_folder, project_name)\n",
        "    if project_dir not in sys.path:\n",
        "        sys.path.append(project_dir)\n",
        "    os.makedirs(project_dir, exist_ok=True)\n",
        "    file_name = f\"{experiment_name}_{run_id}.pkl\"\n",
        "    file_path = os.path.join(project_dir, file_name)\n",
        "    if not overwrite:\n",
        "      assert not os.path.exists(file_path)\n",
        "    t.save(model, file_path)\n",
        "    print(f\"Model saved to {file_path}\")\n",
        "\n",
        "\n",
        "def get_experiment_name_pretrained(\n",
        "    model_size: str,\n",
        "    othello_rule: OthelloRule,\n",
        "    index: int,\n",
        "    final: bool=False,\n",
        "    linear_probe: bool = False,\n",
        "    fake_probe: str = \"\",\n",
        ") -> str:\n",
        "    if fake_probe:\n",
        "        assert linear_probe, \"fake_probe=True requires linear_probe=True\"\n",
        "    prefix = \"\"\n",
        "    if final:\n",
        "        prefix = \"final_\"\n",
        "    if fake_probe:\n",
        "        prefix += fake_probe + \"_\"\n",
        "    if linear_probe:\n",
        "        prefix += \"linear_probe_\"\n",
        "    else:\n",
        "        prefix += \"experiment_\"\n",
        "\n",
        "    return f\"{prefix}{index}_{model_size}_{othello_rule.value}\"\n",
        "\n",
        "\n",
        "def load_model_pretrained_get_matching_files(\n",
        "    project_name: str,\n",
        "    model_size: str,\n",
        "    othello_rule: OthelloRule,\n",
        "    experiment_folder: str,\n",
        "    index: int,\n",
        "    final: bool,\n",
        "    linear_probe: bool = False,\n",
        "    fake_probe: str = \"\",\n",
        ") -> list[str]:\n",
        "    project_dir = os.path.join(experiment_folder, project_name)\n",
        "    experiment_name = get_experiment_name_pretrained(model_size, othello_rule, index, final, linear_probe, fake_probe)\n",
        "    pattern = os.path.join(project_dir, experiment_name + \"*.pkl\")\n",
        "    matching_files = glob.glob(pattern)\n",
        "    return matching_files\n",
        "\n",
        "\n",
        "def load_model_pretrained(\n",
        "    project_name: str,\n",
        "    model_size: str,\n",
        "    othello_rule: OthelloRule,\n",
        "    experiment_folder: str,\n",
        "    device: t.device,\n",
        "    index: int,\n",
        "    final: bool,\n",
        "    linear_probe: bool = False,\n",
        "    fake_probe: str = \"\",\n",
        ") -> t.nn.Module:\n",
        "    matching_files = load_model_pretrained_get_matching_files(\n",
        "        project_name, model_size, othello_rule, experiment_folder, index, final, linear_probe, fake_probe\n",
        "    )\n",
        "    if not matching_files:\n",
        "        print(\n",
        "            f\"No model files found for size {model_size}, othello_rule {othello_rule}, index {index}, final {final}, linear_probe {linear_probe}, fake_probe {fake_probe}\"\n",
        "        )\n",
        "        return None\n",
        "\n",
        "    # Pick the most recent file based on modification time.\n",
        "    latest_file = max(matching_files, key=os.path.getmtime)\n",
        "    print(f\"Loading model from {latest_file}\")\n",
        "    if linear_probe:\n",
        "        linear_probe_tensor = t.load(latest_file, weights_only=False, map_location=device)\n",
        "        return linear_probe_tensor\n",
        "    else:\n",
        "        with t.serialization.safe_globals({HookedTransformer}):\n",
        "            model = t.load(latest_file, weights_only=False, map_location=device)\n",
        "            model = move_to_and_update_config(model, device)\n",
        "        return model\n",
        "\n",
        "\n",
        "def get_experiment_name_finetuned(\n",
        "    weak_model_size: str,\n",
        "    weak_rule: OthelloRule,\n",
        "    strong_model_size: str,\n",
        "    strong_rule: OthelloRule,\n",
        "    index: int,\n",
        "    final: bool = False,\n",
        "    linear_probe: bool = False,\n",
        "    fake_probe: str = \"\",\n",
        ") -> str:\n",
        "    if fake_probe:\n",
        "        assert linear_probe, \"fake_probe=True requires linear_probe=True\"\n",
        "    prefix = \"\"\n",
        "    if final:\n",
        "        prefix = \"final_\"\n",
        "    if fake_probe:\n",
        "        prefix += fake_probe + \"_\"\n",
        "    if linear_probe:\n",
        "        prefix += \"linear_probe_\"\n",
        "    else:\n",
        "        prefix += \"experiment_\"\n",
        "    return f\"{prefix}{index}_{weak_model_size}_{strong_model_size}_{weak_rule}_to_{strong_rule}\"\n",
        "\n",
        "\n",
        "def load_finetuned_model_get_matching_files(\n",
        "    project_name: str,\n",
        "    weak_model_size: str,\n",
        "    weak_rule: OthelloRule,\n",
        "    strong_model_size: str,\n",
        "    strong_rule: OthelloRule,\n",
        "    experiment_folder: str,\n",
        "    index: int,\n",
        "    final: bool,\n",
        "    linear_probe: bool = False,\n",
        "    fake_probe: str = \"\",\n",
        ") -> list[str]:\n",
        "    project_dir = os.path.join(experiment_folder, project_name)\n",
        "    experiment_name = get_experiment_name_finetuned(weak_model_size, weak_rule, strong_model_size, strong_rule, index, final, linear_probe, fake_probe)\n",
        "    pattern = os.path.join(project_dir, experiment_name + \"*.pkl\")\n",
        "    print(\"pattern: \", pattern)\n",
        "    matching_files = glob.glob(pattern)\n",
        "    return matching_files\n",
        "\n",
        "\n",
        "def load_finetuned_model(\n",
        "    project_name: str,\n",
        "    weak_model_size: str,\n",
        "    weak_rule: OthelloRule,\n",
        "    strong_model_size: str,\n",
        "    strong_rule: OthelloRule,\n",
        "    experiment_folder: str,\n",
        "    device: t.device,\n",
        "    index,\n",
        "    final: bool,\n",
        "    linear_probe: bool = False,\n",
        "    fake_probe: str = \"\",\n",
        ") -> t.nn.Module:\n",
        "    matching_files = load_finetuned_model_get_matching_files(\n",
        "        project_name, weak_model_size, weak_rule, strong_model_size, strong_rule, experiment_folder, index, final, linear_probe, fake_probe\n",
        "    )\n",
        "    print(\"matching_files: \", matching_files)\n",
        "    if not matching_files:\n",
        "        print(\n",
        "            f\"No finetuned model files found for weak_model_size {weak_model_size}, weak_rule {weak_rule}, strong_model_size {strong_model_size}, strong_rule {strong_rule}, index {index}, final {final}, linear_probe {linear_probe}\"\n",
        "        )\n",
        "        return None\n",
        "\n",
        "    # Return newest model\n",
        "    latest_file = max(matching_files, key=os.path.getmtime)\n",
        "    if linear_probe:\n",
        "        linear_probe_tensor = t.load(latest_file, weights_only=False, map_location=device)\n",
        "        return linear_probe_tensor\n",
        "    else:\n",
        "        with t.serialization.safe_globals({HookedTransformer}):\n",
        "            finetuned_model = t.load(latest_file, weights_only=False, map_location=device)\n",
        "            finetuned_model = move_to_and_update_config(finetuned_model, device)\n",
        "        return finetuned_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7iUk7uY8wcE"
      },
      "source": [
        "### trainer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "l_Fg-yTK8wlL"
      },
      "outputs": [],
      "source": [
        "class TrainerConfig:\n",
        "    # optimization parameters\n",
        "    max_epochs: int\n",
        "    early_stopping_patience: int\n",
        "    validate_every_n_steps: int\n",
        "    batch_size: int\n",
        "    learning_rate: float\n",
        "    betas: tuple[float, float]\n",
        "    grad_norm_clip: float\n",
        "    weight_decay: float # only applied on matmul weights\n",
        "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
        "    lr_decay = False\n",
        "    warmup_tokens = None # 375e6 # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
        "    final_tokens = None # 260e9 # (at what point we reach 10% of original LR)\n",
        "    num_workers: int # for DataLoader\n",
        "    n_games_to_test_legal_moves: int\n",
        "    n_tokens_to_test_and_val: int\n",
        "\n",
        "    # checkpoint settings\n",
        "    experiment_name: str\n",
        "    experiment_folder: str\n",
        "    project_name: str\n",
        "    model_size: ModelSize\n",
        "    othello_rule: OthelloRule\n",
        "    index: int\n",
        "    final: bool\n",
        "\n",
        "    # Wandb\n",
        "    use_wandb: bool\n",
        "    wandb_project: str | None\n",
        "    wandb_name: str | None\n",
        "\n",
        "    # Saving\n",
        "    save_model: bool\n",
        "    save_every_validation: bool\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        for k,v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "\n",
        "class TrainerConfigFinetune(TrainerConfig):\n",
        "    weak_supervision_model_size: ModelSize\n",
        "    weak_supervision_rule: OthelloRule\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, val_dataset, othello_rule_to_test, config, model_cfg, device):\n",
        "        self.model = model\n",
        "        self.val_dataset = val_dataset\n",
        "        self.othello_rule_to_test = othello_rule_to_test\n",
        "        self.config = config\n",
        "        self.model_cfg = model_cfg\n",
        "        self.run_id=0\n",
        "\n",
        "        # take over whatever gpus are on the system\n",
        "        self.device = device\n",
        "        if self.device != 'cpu':\n",
        "            self.model = t.nn.DataParallel(self.model).to(self.device)\n",
        "\n",
        "    def save_checkpoint(self, index=None, final=None):\n",
        "        # DataParallel wrappers keep raw model object in .module attribute\n",
        "        gpt_model = self.model.module if hasattr(self.model, \"module\") else self.model\n",
        "        hooked_transformer_model = convert_gpt_to_hooked_and_verify(gpt_model, self.model_cfg, self.val_dataset)\n",
        "        if index is None:\n",
        "            index = self.config.index\n",
        "        experiment_name = self.config.experiment_name\n",
        "        if final is None:\n",
        "            final = self.config.final\n",
        "        if final:\n",
        "            experiment_name = \"final_\" + experiment_name\n",
        "        save_model(\n",
        "            hooked_transformer_model,\n",
        "            self.run_id,\n",
        "            self.config.project_name,\n",
        "            experiment_name,\n",
        "            self.config.experiment_folder,\n",
        "            index=index,\n",
        "            overwrite=True,\n",
        "            final=final,\n",
        "        )\n",
        "\n",
        "    def _get_mean_loss_on_dataset(self, dataset) -> float:\n",
        "        self.model.train(False)\n",
        "        indices = random.sample(range(len(dataset)), min(self.config.n_tokens_to_test_and_val, len(dataset)))\n",
        "        subset = Subset(dataset, indices)\n",
        "        data_loader = DataLoader(subset, shuffle=True, pin_memory=True,\n",
        "                                batch_size=self.config.batch_size,\n",
        "                                num_workers=self.config.num_workers)\n",
        "        loss_sum = 0\n",
        "        for it, (x, y) in enumerate(data_loader):\n",
        "            x = x.to(self.device)\n",
        "            y = y.to(self.device)\n",
        "            with t.set_grad_enabled(False):\n",
        "                logits, loss = self.model(x, y)\n",
        "                loss_sum += loss.item()\n",
        "        self.model.train(True)\n",
        "        return loss_sum / len(data_loader)\n",
        "\n",
        "    def _get_legal_move_accuracy(self, dataset, othello_rule) -> float:\n",
        "        self.model.train(False)\n",
        "        indices = random.sample(range(len(dataset)), min(self.config.n_games_to_test_legal_moves, len(dataset)))\n",
        "        subset = Subset(dataset, indices)\n",
        "        data_loader = DataLoader(subset, batch_size=self.config.batch_size, shuffle=False)\n",
        "\n",
        "        total_predictions = 0\n",
        "        legal_moves = 0\n",
        "\n",
        "        with t.no_grad():\n",
        "            for x, y in data_loader:\n",
        "                x = x.to(self.device)\n",
        "                y = y.to(self.device)\n",
        "                B, T = x.size()\n",
        "\n",
        "                logits = self.model(x)[0]  # Get logits only\n",
        "\n",
        "                for b in range(B):\n",
        "                    board = OthelloBoardState()\n",
        "                    for t_step in range(T):\n",
        "                        if t_step > 0:\n",
        "                            pred_token_id = logits[b, t_step-1].argmax().item()\n",
        "                            if pred_token_id != 0:\n",
        "                                pred_square = id_to_square(pred_token_id)\n",
        "                                valid_moves = board.get_valid_moves()\n",
        "                                if pred_square in valid_moves:\n",
        "                                    legal_moves += 1\n",
        "                                total_predictions += 1\n",
        "\n",
        "                        if t_step < T - 1:\n",
        "                            actual_token_id = x[b, t_step].item()\n",
        "                            if actual_token_id != 0:\n",
        "                                actual_square = id_to_square(actual_token_id)\n",
        "                                if othello_rule in [OthelloRule.STANDARD, OthelloRule.BIAS_CLOCK, OthelloRule.CHESS, OthelloRule.UNTRAINED, OthelloRule.TINY_STORIES, OthelloRule.CONSTANT_PARAMETERS]:\n",
        "                                    board.umpire(actual_square)\n",
        "                                elif othello_rule == OthelloRule.NEXT_TO_OPPONENT:\n",
        "                                    board.place_next_to_opponent(actual_square)\n",
        "                                elif othello_rule == OthelloRule.NO_FLIPPING:\n",
        "                                    board.place_no_flipping(actual_square)\n",
        "                                else:\n",
        "                                    raise ValueError(f\"Unknown OthelloRule: {othello_rule}\")\n",
        "\n",
        "        self.model.train(True)\n",
        "        return legal_moves / total_predictions if total_predictions > 0 else 0.0\n",
        "\n",
        "    def get_train_dataset(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def perform_extra_tests(self, step):\n",
        "        pass\n",
        "\n",
        "    def _test(self, step) -> str:\n",
        "        self.perform_extra_tests(step)\n",
        "        if self.config.use_wandb:\n",
        "            legal_move_chance = self._get_legal_move_accuracy(self.get_train_dataset(), self.config.othello_rule)\n",
        "            wandb.log({\"train/legal_move_chance\": legal_move_chance}, step = step)\n",
        "            # Loss and legal moves for each rule\n",
        "            for othello_rule in self.othello_rule_to_test:\n",
        "                dataset = self.othello_rule_to_test[othello_rule]\n",
        "                loss = self._get_mean_loss_on_dataset(dataset)\n",
        "                legal_move_chance = self._get_legal_move_accuracy(dataset, othello_rule)\n",
        "                wandb.log({f\"test/{othello_rule.value}_loss\": loss,\n",
        "                           f\"test/{othello_rule.value}_legal_move_chance\": legal_move_chance},\n",
        "                          step = step\n",
        "                          )\n",
        "\n",
        "    def _validate(self, step):\n",
        "        \"\"\"Returns if True iff. early-stop got triggered\"\"\"\n",
        "        if self.config.save_every_validation:\n",
        "            self.save_checkpoint(index=step)\n",
        "\n",
        "        loss = self._get_mean_loss_on_dataset(self.val_dataset)\n",
        "        if self.config.use_wandb:\n",
        "            wandb.log({\"val/loss\": loss, \"val/patience_counter\": self.patience_counter}, step = step)\n",
        "\n",
        "        if loss < self.best_loss:\n",
        "            self.patience_counter = 0\n",
        "            self.best_loss = loss\n",
        "            if self.config.save_model:\n",
        "                self.save_checkpoint()\n",
        "        else:\n",
        "            self.patience_counter += 1\n",
        "            if self.patience_counter >= self.config.early_stopping_patience:\n",
        "                print(f\"Early Stop Triggered at step {step}\")\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def get_train_loss(self, x, y):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def _run_epoch_train(self, optimizer, epoch: int):\n",
        "        \"\"\"Returns if True iff. early-stop got triggered\"\"\"\n",
        "        self.model.train(True)\n",
        "        data = self.get_train_dataset()\n",
        "        train_loader = DataLoader(data, shuffle=True, pin_memory=True,\n",
        "                            batch_size=self.config.batch_size,\n",
        "                            num_workers=self.config.num_workers)\n",
        "        train_loader_len = len(train_loader)\n",
        "\n",
        "        losses = []\n",
        "        pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "        for it, (x, y) in pbar:\n",
        "            step = epoch*train_loader_len + it\n",
        "            x = x.to(self.device)  # [B, T]\n",
        "            y = y.to(self.device)  # [B, T]\n",
        "            with t.set_grad_enabled(True):\n",
        "                loss = self.get_train_loss(x, y)\n",
        "                losses.append(loss)\n",
        "\n",
        "            # backprop\n",
        "            self.model.zero_grad()\n",
        "            loss.backward()\n",
        "            t.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.grad_norm_clip)\n",
        "            optimizer.step()\n",
        "\n",
        "            # decay learning rate\n",
        "            if self.config.lr_decay:\n",
        "                self.tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
        "                if self.tokens < self.config.warmup_tokens:\n",
        "                    # linear warmup\n",
        "                    lr_mult = float(self.tokens) / float(max(1, self.config.warmup_tokens))\n",
        "                else:\n",
        "                    # cosine learning rate decay\n",
        "                    progress = float(self.tokens - self.config.warmup_tokens) / float(max(1, self.config.final_tokens - self.config.warmup_tokens))\n",
        "                    lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "                lr = self.config.learning_rate * lr_mult\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] = lr\n",
        "            else:\n",
        "                lr = self.config.learning_rate\n",
        "\n",
        "            # report progress\n",
        "            pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. lr {lr:e}\")\n",
        "            if self.config.use_wandb:\n",
        "              wandb.log({\n",
        "                      \"train/loss\": loss.item(),\n",
        "                      \"train/lr\": lr,\n",
        "                  },\n",
        "                  step=step,\n",
        "              )\n",
        "            if step % self.config.validate_every_n_steps == 0:\n",
        "                self._test(step)\n",
        "                stop = self._validate(step)\n",
        "                if stop:\n",
        "                    return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def train(self):\n",
        "        raw_model = self.model.module if hasattr(self.model, \"module\") else self.model\n",
        "        optimizer = raw_model.configure_optimizers(self.config)\n",
        "        if self.config.use_wandb:\n",
        "            wandb.init(project=self.config.wandb_project, name=self.config.wandb_name, config=self.config)\n",
        "            self.run_id = wandb.run.id\n",
        "\n",
        "        self.best_loss = float('inf')\n",
        "        self.patience_counter = 0\n",
        "        self.tokens = 0 # counter used for learning rate decay. Also lets _run_epoch fail if train hasnt been called first (and hence variable doesn't exist).\n",
        "        for epoch in range(self.config.max_epochs):\n",
        "            stop = self._run_epoch_train(optimizer, epoch)\n",
        "            if stop:\n",
        "                break\n",
        "\n",
        "        if self.config.save_model:\n",
        "            self.save_checkpoint(final=True)\n",
        "\n",
        "\n",
        "class TrainerPretrain(Trainer):\n",
        "    def __init__(self, model, train_dataset, val_dataset, othello_rule_to_test, config, model_cfg, device):\n",
        "        super().__init__(model, val_dataset, othello_rule_to_test, config, model_cfg, device)\n",
        "        self.train_dataset = train_dataset\n",
        "\n",
        "    def get_train_dataset(self):\n",
        "        return self.train_dataset\n",
        "\n",
        "    def get_train_loss(self, x, y):\n",
        "        logits, loss = self.model(x, y)\n",
        "        return loss\n",
        "\n",
        "\n",
        "class TrainerFinetune(Trainer):\n",
        "    def __init__(self, model, weak_model, weak_finetune_dataset, val_dataset, othello_rule_to_test, config, model_cfg, device):\n",
        "        super().__init__(model, val_dataset, othello_rule_to_test, config, model_cfg, device)\n",
        "        self.weak_model = weak_model\n",
        "        self.weak_finetune_dataset = weak_finetune_dataset\n",
        "\n",
        "    def get_train_dataset(self):\n",
        "        return self.weak_finetune_dataset\n",
        "\n",
        "    def perform_extra_tests(self, step):\n",
        "        if self.config.use_wandb:\n",
        "            self.model.train(False)\n",
        "            indices = random.sample(range(len(self.val_dataset)), min(self.config.n_tokens_to_test_and_val, len(self.val_dataset)))\n",
        "            subset = Subset(self.val_dataset, indices)\n",
        "            data_loader = DataLoader(subset, shuffle=True, pin_memory=True,\n",
        "                                    batch_size=self.config.batch_size,\n",
        "                                    num_workers=self.config.num_workers)\n",
        "            loss_sum = 0\n",
        "            for it, (x, y) in enumerate(data_loader):\n",
        "                x = x.to(self.device)\n",
        "                y = y.to(self.device)\n",
        "                with t.set_grad_enabled(False):\n",
        "                    loss = self.get_train_loss(x, y)\n",
        "                    loss_sum += loss.item()\n",
        "            self.model.train(True)\n",
        "            wandb.log({\n",
        "                      \"val/loss_soft_weak_labels\": loss_sum / len(data_loader),\n",
        "                  },\n",
        "                  step=step,\n",
        "              )\n",
        "\n",
        "    def get_train_loss(self, x, y):\n",
        "        logits_weak_supervision, _ = self.weak_model(x)\n",
        "        y_soft_weak_supervision = F.softmax(logits_weak_supervision, dim=-1)\n",
        "        logits, loss = self.model(x, y, y_soft_weak_supervision)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFKAJF7S7aGF"
      },
      "source": [
        "### train_and_load_chess_gpt_embeddings.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ymivBhE97YHx"
      },
      "outputs": [],
      "source": [
        "class VocabProjectionGPT(nn.Module):\n",
        "    \"\"\"\n",
        "    Wrapper around a GPT model to adapt to new n_vocab.\n",
        "    Instead of embedding W, it uses W_new = W @ H, where H is initialized as a\n",
        "    random matrix and can be trained.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_gpt_model, target_vocab_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # Base model with freezed parameters\n",
        "        self.base_gpt = base_gpt_model\n",
        "        for param in self.base_gpt.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.source_vocab_size = self.base_gpt.tok_emb.num_embeddings\n",
        "        self.target_vocab_size = target_vocab_size\n",
        "        self.n_embd = self.base_gpt.tok_emb.embedding_dim\n",
        "\n",
        "        # Projection matrices\n",
        "        self.input_projection = nn.Linear(target_vocab_size, self.source_vocab_size, bias=False)\n",
        "        self.output_projection = nn.Linear(self.source_vocab_size, target_vocab_size, bias=False)\n",
        "        nn.init.normal_(self.input_projection.weight, mean=0.0, std=0.02)\n",
        "        nn.init.normal_(self.output_projection.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def get_block_size(self):\n",
        "        return self.base_gpt.get_block_size()\n",
        "\n",
        "    def configure_optimizers(self, train_config):\n",
        "        \"\"\"Only optimize the projection matrices\"\"\"\n",
        "        projection_params = list(self.input_projection.parameters()) + list(self.output_projection.parameters())\n",
        "        optim_groups = [\n",
        "            {\"params\": projection_params, \"weight_decay\": train_config.weight_decay}\n",
        "        ]\n",
        "        optimizer = t.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
        "        return optimizer\n",
        "\n",
        "    def forward(self, idx, targets=None, y_soft_weak_supervision=None):\n",
        "        \"\"\"\n",
        "        Forward pass with composed embedding matrices\n",
        "\n",
        "        Args:\n",
        "            idx: [B, T] - Othello token IDs\n",
        "            targets: [B, T] - Othello target token IDs (optional)\n",
        "            y_soft_weak_supervision: [B, T, target_vocab_size] - Soft labels (optional)\n",
        "        \"\"\"\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.base_gpt.block_size, \"Cannot forward, model block size is exhausted.\"\n",
        "\n",
        "        # Create embeddings\n",
        "        composed_embedding_weight = self.input_projection.weight.T @ self.base_gpt.tok_emb.weight\n",
        "        composed_head_weight = self.base_gpt.head.weight.T @ self.output_projection.weight.T\n",
        "\n",
        "        # Forward\n",
        "        token_embeddings = F.embedding(idx, composed_embedding_weight)\n",
        "        position_embeddings = self.base_gpt.pos_emb[:, :t, :]  # [B, T, n_embd]\n",
        "        x = self.base_gpt.drop(token_embeddings + position_embeddings)\n",
        "        x = self.base_gpt.blocks(x)\n",
        "        x = self.base_gpt.ln_f(x)\n",
        "        logits = F.linear(x, composed_head_weight.T)  # [B, T, target_vocab_size]\n",
        "\n",
        "        # Calculate loss if targets provided\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            if y_soft_weak_supervision is not None:\n",
        "                # Soft supervision case\n",
        "                mask = (targets != 0).float()  # [B, T]\n",
        "\n",
        "                logits_flat = logits.view(-1, logits.size(-1))  # [B*T, target_vocab_size]\n",
        "                soft_targets_flat = y_soft_weak_supervision.view(-1, y_soft_weak_supervision.size(-1))  # [B*T, target_vocab_size]\n",
        "                mask_flat = mask.view(-1)  # [B*T]\n",
        "\n",
        "                loss_per_token = F.cross_entropy(logits_flat, soft_targets_flat, reduction='none')  # [B*T]\n",
        "                masked_loss = loss_per_token * mask_flat\n",
        "\n",
        "                if mask_flat.sum() > 0:\n",
        "                    loss = masked_loss.sum() / mask_flat.sum()\n",
        "                else:\n",
        "                    loss = t.tensor(0.0, device=logits.device)\n",
        "            else:\n",
        "                # Standard cross-entropy loss\n",
        "                loss = F.cross_entropy(\n",
        "                    logits.view(-1, logits.size(-1)),\n",
        "                    targets.view(-1),\n",
        "                    ignore_index=0\n",
        "                )\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "class ProjectionTrainerConfig(TrainerConfig):\n",
        "    \"\"\"Config for training projection matrices only\"\"\"\n",
        "    target_vocab_size: int  # Othello vocabulary size (61)\n",
        "\n",
        "\n",
        "class ProjectionTrainer(TrainerPretrain):\n",
        "    \"\"\"Trainer of VocabProjectionGPT that only trains the projection matrices\"\"\"\n",
        "\n",
        "    def __init__(self, train_dataset, val_dataset, othello_rule_to_test, config, device):\n",
        "        chess_gpt = self.load_chess_gpt(val_dataset, device)\n",
        "        model = VocabProjectionGPT(\n",
        "            base_gpt_model=chess_gpt,\n",
        "            target_vocab_size=config.target_vocab_size\n",
        "        )\n",
        "        model_cfg = None\n",
        "        super().__init__(model, train_dataset, val_dataset, othello_rule_to_test, config, model_cfg, device)\n",
        "\n",
        "    def load_chess_gpt(self, val_dataset, device):\n",
        "        \"\"\"Load the pre-trained ChessGPT model\"\"\"\n",
        "        hooked_chess_gpt = load_hooked_chess_gpt(device, force_othello_vocab=False)\n",
        "        gpt_model, gpt_config = convert_hooked_to_gpt(hooked_chess_gpt, device)\n",
        "        return gpt_model\n",
        "\n",
        "    def save_checkpoint(self, final=False, index=None):\n",
        "        \"\"\"Save only the projection matrices\"\"\"\n",
        "        model = self.model.module if hasattr(self.model, \"module\") else self.model\n",
        "        projection_state = {\n",
        "            'input_projection': model.input_projection.state_dict(),\n",
        "            'output_projection': model.output_projection.state_dict(),\n",
        "            'source_vocab_size': model.source_vocab_size,\n",
        "            'target_vocab_size': model.target_vocab_size,\n",
        "        }\n",
        "        save_path = f\"{self.config.experiment_folder}/{self.config.experiment_name}/projection_matrices_{'final' if final else index}.pt\"\n",
        "        t.save(projection_state, save_path)\n",
        "        print(f\"Saved projection matrices to {save_path}\")\n",
        "\n",
        "\n",
        "\n",
        "def create_othello_gpt_from_chess(projection_matrices_path: str, device: str) -> HookedTransformer:\n",
        "    \"\"\"\n",
        "    Create a HookedTransformer OthelloGPT by loading ChessGPT and applying\n",
        "    learned projection matrices to create new embedding and unembedding layers.\n",
        "    \"\"\"\n",
        "    # Load model\n",
        "    hooked_chess_gpt = load_hooked_chess_gpt(device, force_othello_vocab=False)\n",
        "    chess_gpt, chess_config = convert_hooked_to_gpt(hooked_chess_gpt, device)\n",
        "\n",
        "    # Load embedding matrices\n",
        "    projection_data = t.load(projection_matrices_path, map_location=device)\n",
        "    input_proj_weight = projection_data['input_projection']['weight'].to(device)\n",
        "    output_proj_weight = projection_data['output_projection']['weight'].to(device)\n",
        "    target_vocab_size = projection_data['target_vocab_size']\n",
        "\n",
        "    # Create new embedding matrices and GPT model\n",
        "    with t.no_grad():\n",
        "        composed_embedding_weight = input_proj_weight.T @ chess_gpt.tok_emb.weight\n",
        "        composed_head_weight = chess_gpt.head.weight.T @ output_proj_weight.T\n",
        "\n",
        "    othello_config = GPTConfig(\n",
        "        vocab_size=target_vocab_size,\n",
        "        block_size=chess_gpt.block_size,\n",
        "        n_embd=chess_gpt.tok_emb.embedding_dim,\n",
        "        n_head=chess_gpt.blocks[0].attn.n_head,\n",
        "        n_layer=len(chess_gpt.blocks)\n",
        "    )\n",
        "    othello_gpt = GPT(othello_config).to(device)\n",
        "\n",
        "    # Insert embedding matrices\n",
        "    with t.no_grad():\n",
        "        othello_gpt.blocks.load_state_dict(chess_gpt.blocks.state_dict())\n",
        "        othello_gpt.ln_f.load_state_dict(chess_gpt.ln_f.state_dict())\n",
        "        othello_gpt.drop = chess_gpt.drop\n",
        "        othello_gpt.tok_emb.weight.data.copy_(composed_embedding_weight)\n",
        "        othello_gpt.head.weight.data.copy_(composed_head_weight.T)\n",
        "\n",
        "    # Return HookedTransformer\n",
        "    hooked_othello_gpt = convert_gpt_to_hooked(othello_gpt, othello_config)\n",
        "    return hooked_othello_gpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "5HrM31lME2Qf"
      },
      "outputs": [],
      "source": [
        "# final_othello_gpt = create_othello_gpt_from_chess(\n",
        "#     projection_matrices_path=f\"{experiment_folder}/copy_projection_matrices_None.pt\",\n",
        "#     device=DEVICE\n",
        "# )\n",
        "# test_hooked_transformer(final_othello_gpt, OthelloRule.STANDARD, othello_rule_to_test[OthelloRule.STANDARD], 64, 10, DEVICE)\n",
        "\n",
        "# experiment_name = f\"experiment_0_{ModelSize.HUGE}_{OthelloRule.CHESS.value}\"\n",
        "# save_model(final_othello_gpt, 0, \"pretrain_extra_models\", experiment_name, experiment_folder, index=0, final=False, overwrite=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "TfjsA4N_aFuq"
      },
      "outputs": [],
      "source": [
        "# tiny_stories = load_tiny_stories_instruct_8m(DEVICE, force_othello_vocab=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQE6p_vS0X9C"
      },
      "source": [
        "# --- MultiProbe ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oyzic3pGk6qh"
      },
      "source": [
        "### fake_board_state_transform.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "OYiz-Qdtk61H"
      },
      "outputs": [],
      "source": [
        "class FakeBoardStateTransform(nn.Module):\n",
        "    pass\n",
        "\n",
        "class FakeBoardStateTransformPlacedBefore(FakeBoardStateTransform):\n",
        "    \"\"\"\n",
        "    I.e. empty -> 0, theirs/mine -> 1. This means we track if this token\n",
        "    has occured before.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.name = 'placed_before'\n",
        "\n",
        "    def forward(self, board_state):\n",
        "        return (board_state != 0).long()\n",
        "\n",
        "class FakeBoardStateTransformBlackWhite(FakeBoardStateTransform):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.name = 'black_white'\n",
        "\n",
        "    def forward(self, board_state):\n",
        "        return board_state\n",
        "\n",
        "\n",
        "class FakeBoardStateTransformModulo(FakeBoardStateTransform):\n",
        "    \"\"\"\n",
        "    Transformation that transforms real board states into fake features.\n",
        "    Categorical [8, 8] -> one-hot [8, 8, 3] -> flatten [192]\n",
        "    -> random matrix [192] -> reshape [8, 8, 3] -> argmax -> Categorical [8, 8]\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.name = 'modulo'\n",
        "        self.linear = nn.Linear(64, 64, bias=False)\n",
        "        with t.no_grad():\n",
        "            # random {-1, 0, 1}\n",
        "            new_weights = t.randint(0, 3, self.linear.weight.shape) - 1\n",
        "            self.linear.weight.copy_(new_weights.float())\n",
        "\n",
        "    def forward(self, board_state):\n",
        "        # Categorical -> vector\n",
        "        batch, seq_len, rows, cols = board_state.shape\n",
        "        board_flat = einops.rearrange(\n",
        "            board_state.long().float(),\n",
        "            \"batch seq rows cols -> batch seq (rows cols)\"\n",
        "        )\n",
        "\n",
        "        # Vector ---fake---> vector\n",
        "        fake_features = self.linear(board_flat) % 3\n",
        "\n",
        "        # Vector -> categorical\n",
        "        fake_features_reshaped = einops.rearrange(\n",
        "            fake_features,\n",
        "            \"batch seq (rows cols) -> batch seq rows cols\",\n",
        "            rows=rows, cols=cols\n",
        "        )\n",
        "        fake_state = fake_features_reshaped.floor().long()\n",
        "\n",
        "        unique_values = t.unique(fake_state)\n",
        "        assert t.all(t.isin(unique_values, t.tensor([0, 1, 2], device=fake_state.device))), \\\n",
        "            f\"Output contains invalid classes: {unique_values}. Expected only [0, 1, 2]\"\n",
        "\n",
        "        return fake_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "OoPV4riSlp_q"
      },
      "outputs": [],
      "source": [
        "def analyze_transform(train_board_state_dataset, fake_board_state_transform,\n",
        "                     othello_rule, pos_start=5, pos_end=-5, n_samples=500):\n",
        "   # Sample games\n",
        "   indices = np.random.choice(len(train_board_state_dataset),\n",
        "                             min(n_samples, len(train_board_state_dataset)), replace=False)\n",
        "\n",
        "   # Get board states\n",
        "   games_square = train_board_state_dataset.board_seqs_square[indices]\n",
        "   state = get_board_states_and_legal_moves(games_square, othello_rule)[0]\n",
        "\n",
        "   if not fake_board_state_transform or fake_board_state_transform.name in ['placed_before', 'modulo']:\n",
        "       # Transform: {0: empty, 1: black, -1: white} to {0: empty, 1: theirs, 2: mine}\n",
        "       state[:, ::2][state[:, ::2] == -1] = 2\n",
        "       state[:, 1::2][state[:, 1::2] == 1] = 2\n",
        "       state[:, 1::2][state[:, 1::2] == -1] = 1\n",
        "\n",
        "   # Slice and transform\n",
        "   original_states = state[:, pos_start:pos_end]\n",
        "   if fake_board_state_transform:\n",
        "      with t.no_grad():\n",
        "          fake_states = fake_board_state_transform(original_states)\n",
        "   else:\n",
        "      fake_states = original_states\n",
        "\n",
        "   # Correlation\n",
        "   original_flat = original_states.flatten().cpu().numpy()\n",
        "   fake_flat = fake_states.flatten().cpu().numpy()\n",
        "   correlation = pearsonr(original_flat, fake_flat)[0]\n",
        "\n",
        "   # Field-wise accuracy (64 predictions)\n",
        "   accuracies_fake = []\n",
        "   accuracies_original = []\n",
        "   for r in range(8):\n",
        "       for c in range(8):\n",
        "           fake_field = fake_states[:, :, r, c].flatten().cpu().numpy()\n",
        "           orig_field = original_states[:, :, r, c].flatten().cpu().numpy()\n",
        "\n",
        "           # Most common fake value for this field\n",
        "           most_common_fake = np.bincount(fake_field).argmax()\n",
        "           most_common_original = np.bincount(orig_field).argmax()\n",
        "\n",
        "           # Accuracy if always predicting most_common\n",
        "           accuracy_fake = (most_common_fake == fake_field).mean()\n",
        "           accuracies_fake.append(accuracy_fake)\n",
        "           accuracy_original = (most_common_original == orig_field).mean()\n",
        "           accuracies_original.append(accuracy_original)\n",
        "\n",
        "   print(f\"Correlation: {correlation:.4f}\")\n",
        "   print(f\"Mean field accuracy - Fake: {np.mean(accuracies_fake):.4f}\")\n",
        "   print(f\"Mean field accuracy - Original: {np.mean(accuracies_original):.4f}\")\n",
        "\n",
        "   # Original state value distribution\n",
        "   print(\"Original State Values (%):\")\n",
        "   orig_vals, orig_counts = np.unique(original_flat, return_counts=True)\n",
        "   for value, count in zip(orig_vals, orig_counts):\n",
        "       percentage = (count / len(original_flat)) * 100\n",
        "       print(f\"  Value {int(value)}: {percentage:.2f}%\")\n",
        "\n",
        "   # Fake state value distribution\n",
        "   print(\"\\nFake State Values (%):\")\n",
        "   fake_vals, fake_counts = np.unique(fake_flat, return_counts=True)\n",
        "   for value, count in zip(fake_vals, fake_counts):\n",
        "       percentage = (count / len(fake_flat)) * 100\n",
        "       print(f\"  Value {int(value)}: {percentage:.2f}%\")\n",
        "\n",
        "\n",
        "# fake_board_state_transform = FakeBoardStateTransform()\n",
        "# analyze_transform(train_board_state_dataset, None,\n",
        "#                              OthelloRule.STANDARD, n_samples=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNdXLKRdbPXZ"
      },
      "source": [
        "### evaluate_probe_generalization.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "pxCtqNS1NYSw"
      },
      "outputs": [],
      "source": [
        "def evaluate_probe_generalization(\n",
        "    probe,\n",
        "    model,\n",
        "    board_seqs_square_test,\n",
        "    board_seqs_id_test,\n",
        "    layer,\n",
        "    device,\n",
        "    othello_rule: OthelloRule,\n",
        "    plot_results=True,\n",
        "    fake_board_state_transform: FakeBoardStateTransform | None = None\n",
        "):\n",
        "    # Focus games\n",
        "    focus_games_id = board_seqs_id_test  # shape [n_games, 60]\n",
        "    focus_games_square = board_seqs_square_test  # shape [n_games, 60]\n",
        "    focus_states, focus_legal_moves, focus_legal_moves_annotation = get_board_states_and_legal_moves(focus_games_square, othello_rule)\n",
        "\n",
        "    # Focus cache\n",
        "    focus_logits, focus_cache = model.run_with_cache(focus_games_id[:, :-1].to(device))\n",
        "\n",
        "    # Focus their vs. mine (0: empty, 1: theirs, 2: mine)\n",
        "    focus_states_theirs_vs_mine = focus_states * (-1 + 2 * (t.arange(focus_states.shape[1]) % 2))[None, :, None, None]\n",
        "    focus_states_theirs_vs_mine[focus_states_theirs_vs_mine == 1] = 2\n",
        "    focus_states_theirs_vs_mine[focus_states_theirs_vs_mine == -1] = 1\n",
        "\n",
        "    focus_states_theirs_vs_mine = focus_states_theirs_vs_mine.to(device)\n",
        "\n",
        "    if fake_board_state_transform:\n",
        "        focus_states_theirs_vs_mine = fake_board_state_transform(focus_states_theirs_vs_mine)\n",
        "\n",
        "    # Here, we test out each of our 3 probe modes (even / odd / both) on each of these 3 settings\n",
        "    # (even / odd / both). Hopefully we should see all 3 probes generalize!\n",
        "    probe_out = einops.einsum(\n",
        "        focus_cache[\"resid_post\", layer],\n",
        "        probe,\n",
        "        \"game move d_model, mode d_model row col options -> mode game move row col options\",\n",
        "    )\n",
        "    # probe_out_value = probe_out.argmax(dim=-1).cpu()  # mode game move row col\n",
        "    probe_out_value = probe_out.argmax(dim=-1)\n",
        "\n",
        "    # For each mode, get the accuracy on even / odd / both\n",
        "    is_correct = probe_out_value == focus_states_theirs_vs_mine[:, :-1]  # mode game move row col\n",
        "    accuracies_even = einops.reduce(is_correct[:, 6:-6:2].float(), \"mode game move row col -> mode row col\", \"mean\")\n",
        "    accuracies_odd = einops.reduce(is_correct[:, 5:-5:2].float(), \"mode game move row col -> mode row col\", \"mean\")\n",
        "    accuracies_all = einops.reduce(is_correct[:, 5:-5].float(), \"mode game move row col -> mode row col\", \"mean\")\n",
        "\n",
        "    # Print diagonal accuracies (probe mode matching data split)\n",
        "    mean_accuracy_even_on_even = accuracies_even[0].mean()  # mode 0 = even probe\n",
        "    mean_accuracy_odd_on_odd = accuracies_odd[1].mean()  # mode 1 = odd probe\n",
        "    mean_accuracy_both_on_all = accuracies_all[2].mean()  # mode 2 = both probe\n",
        "\n",
        "    # Get all 3x3 accuracies, stacked over first dim\n",
        "    accuracies_stacked = t.concat([accuracies_even, accuracies_odd, accuracies_all], dim=0)\n",
        "\n",
        "    # Plot results!\n",
        "    board_titles = [\n",
        "        f\"{probe_mode} probe on {data_mode} data\"\n",
        "        for data_mode in [\"even\", \"odd\", \"all\"]\n",
        "        for probe_mode in [\"even\", \"odd\", \"both\"]\n",
        "    ]\n",
        "\n",
        "    fig = plot_board_values(\n",
        "        1 - accuracies_stacked,\n",
        "        title=\"Average Error Rate of Linear Probe\",\n",
        "        board_titles=board_titles,\n",
        "        boards_per_row=3,\n",
        "        zmax=0.25,\n",
        "        zmin=-0.25,\n",
        "        height=1000,\n",
        "        width=900,\n",
        "        show=plot_results,\n",
        "    )\n",
        "    return fig, mean_accuracy_even_on_even, mean_accuracy_odd_on_odd, mean_accuracy_both_on_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7taVfY8YIIm"
      },
      "source": [
        "### linear_multi_probe_trainer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "L6VhGIIT0XLh"
      },
      "outputs": [],
      "source": [
        "class BoardStateDataset:\n",
        "    def __init__(self, char_dataset: CharDataset, n_games: int):\n",
        "        self.char_dataset = char_dataset\n",
        "        self.board_seqs_square, self.board_seqs_id, self.n_skipped = get_board_seqs_square_and_id(self.char_dataset, n_games)\n",
        "        print(\"Skipped {} games\".format(self.n_skipped))\n",
        "        self.n_games = len(self.board_seqs_square)\n",
        "\n",
        "    def __len__(self):\n",
        "        assert self.n_games == len(self.board_seqs_square)\n",
        "        return self.n_games\n",
        "\n",
        "@dataclass\n",
        "class MultiProbeTrainingArgs:\n",
        "    device: str\n",
        "\n",
        "    # Data to train on\n",
        "    othello_rule: OthelloRule = OthelloRule.STANDARD  # What to train on\n",
        "    modes: int = 3  # even, odd, both (i.e. the data we train on)\n",
        "    layer: int = 6\n",
        "    pos_start: int = 5\n",
        "    pos_end: int = -5  # i.e. we slice [pos_start: model.n_ctx + pos_end]\n",
        "\n",
        "    # Game state (options are blank/mine/theirs)\n",
        "    options: int = 3\n",
        "    rows: int = 8\n",
        "    cols: int = 8\n",
        "    fake_board_state_transform: FakeBoardStateTransform | None = None\n",
        "\n",
        "    # Training\n",
        "    epochs: int = 5\n",
        "    validate_every_n_steps: int = 100\n",
        "    early_stopping_patience: int = 10\n",
        "    n_tokens_to_test_and_val: int = 100\n",
        "    batch_size: int = 32  # Can't be too big, because of memory problems if we load the activations of a model\n",
        "    lr: float = 1e-4  # high for quick testing\n",
        "    betas: tuple[float, float] = (0.9, 0.99)\n",
        "    weight_decay: float = 0.01\n",
        "\n",
        "    # Wandb\n",
        "    experiment_folder: str = \"\"\n",
        "    experiment_name: str = \"\"\n",
        "\n",
        "    use_wandb: bool = True\n",
        "    wandb_project: str | None = None\n",
        "    wandb_name: str | None = None\n",
        "\n",
        "    def setup_linear_probe(self, model: HookedTransformer):\n",
        "        linear_probe = t.randn(\n",
        "            self.modes,\n",
        "            model.cfg.d_model,\n",
        "            self.rows,\n",
        "            self.cols,\n",
        "            self.options,\n",
        "            device=self.device,\n",
        "        ) / np.sqrt(model.cfg.d_model)\n",
        "        linear_probe.requires_grad = True\n",
        "        return linear_probe\n",
        "\n",
        "\n",
        "class LinearMultiProbeTrainer:\n",
        "    def __init__(self,\n",
        "                 model: HookedTransformer,\n",
        "                 args: MultiProbeTrainingArgs,\n",
        "                 train_board_state_dataset: BoardStateDataset | None,\n",
        "                 val_board_state_dataset: BoardStateDataset | None,\n",
        "                 othello_rule_to_board_state_test_dataset: dict[OthelloRule, list[BoardStateDataset]] | None,\n",
        "                 ):\n",
        "        self.model = model\n",
        "        self.args = args\n",
        "        self.linear_probe = args.setup_linear_probe(self.model)\n",
        "        self.train_board_state_dataset = train_board_state_dataset\n",
        "        self.val_board_state_dataset = val_board_state_dataset\n",
        "        self.othello_rule_to_board_state_test_dataset = othello_rule_to_board_state_test_dataset\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        \"\"\"Save linear probe and training state.\"\"\"\n",
        "        save_model(\n",
        "            model=self.linear_probe.detach().cpu(),\n",
        "            run_id=self.run_id,\n",
        "            project_name=self.args.wandb_project,\n",
        "            experiment_name=self.args.experiment_name,\n",
        "            experiment_folder=self.args.experiment_folder,\n",
        "            index=0,\n",
        "            overwrite=True,\n",
        "            final=False,\n",
        "            linear_probe=True\n",
        "        )\n",
        "\n",
        "    def _get_mean_loss(self, games_id, games_square, wandb_folder: str, othello_rule: OthelloRule):\n",
        "        pos_start = self.args.pos_start\n",
        "        # pos_end = self.args.pos_end + self.model.cfg.n_ctx\n",
        "        pos_end = self.args.pos_end + 61  # TODO CURRENTLY HARDCODED FOR OTHELLO\n",
        "\n",
        "        # Cache resid_post from all our games (ignoring the last one)\n",
        "        with t.inference_mode():\n",
        "            _, cache = self.model.run_with_cache(\n",
        "                games_id[:, :-1].to(self.args.device),\n",
        "                return_type=None,\n",
        "                names_filter=lambda name: name.endswith(\"resid_post\"),\n",
        "            )\n",
        "\n",
        "        # We're training on all modes, so we slice all resid values in our range.\n",
        "        resid_post = cache[\"resid_post\", self.args.layer][:, pos_start:pos_end]\n",
        "\n",
        "        # Get probe output, i.e. probe_logits[m, g, p, r, c] = the 3-vector of logit predictions from\n",
        "        # mode-m probe, for what color is in square [r, c] AFTER the p-th move is played in game g.\n",
        "        probe_logits = einops.einsum(\n",
        "            resid_post,\n",
        "            self.linear_probe,\n",
        "            \"game pos d_model, mode d_model rows cols options -> mode game pos rows cols options\",\n",
        "        )\n",
        "        probe_logprobs = probe_logits.log_softmax(-1)\n",
        "\n",
        "        # Get the actual game state. The original state has {0: empty, 1: black, -1: white} and\n",
        "        # we want our probe to be in the basis {0: empty, 1: theirs, 2: mine}. For even moves,\n",
        "        # mine = white, so we map -1 -> 2. For odd moves, mine = black, so we map {1, -1} -> {2, 1}.\n",
        "        state = get_board_states_and_legal_moves(games_square, othello_rule)[0]\n",
        "        if not fake_board_state_transform or fake_board_state_transform.name in ['placed_before', 'modulo']:\n",
        "            # Transform: {0: empty, 1: black, -1: white} to {0: empty, 1: theirs, 2: mine}\n",
        "            state[:, ::2][state[:, ::2] == -1] = 2\n",
        "            state[:, 1::2][state[:, 1::2] == 1] = 2\n",
        "            state[:, 1::2][state[:, 1::2] == -1] = 1\n",
        "        else:\n",
        "            # Transform: {0: empty, 1: black, -1: white} to {0: empty, 1: black, 2: white}\n",
        "            state[state == -1] = 2\n",
        "\n",
        "        if self.args.fake_board_state_transform:\n",
        "            state = self.args.fake_board_state_transform(state)\n",
        "        # pos_start -> pos_end\n",
        "        state = state[:, pos_start:pos_end]\n",
        "\n",
        "        # Index into the probe logprobs with the correct indices (note, each of our 3 probe modes\n",
        "        # gives us a different tensor of logprobs).\n",
        "        # In _get_mean_loss, before eindex:\n",
        "        correct_probe_logprobs = eindex(\n",
        "            probe_logprobs,\n",
        "            state,\n",
        "            \"mode game pos row col [game pos row col]\",  # -> shape [mode game pos row col]\n",
        "        )\n",
        "        # Get the logprobs we'll be using for our 3 different probes. Remember that for the even\n",
        "        # and odd probes we need to take only the even/odd moves respectively (and also that we've\n",
        "        # already sliced logprobs from pos_start: pos_end).\n",
        "        pos_start_even, pos_start_odd = (0, 1) if pos_start % 2 == 0 else (1, 0)\n",
        "        even_probe_logprobs = correct_probe_logprobs[0, pos_start_even::2]\n",
        "        odd_probe_logprobs = correct_probe_logprobs[1, pos_start_odd::2]\n",
        "        both_probe_logprobs = correct_probe_logprobs[2]\n",
        "        # Get our 3 different loss functions\n",
        "        loss_even = -einops.reduce(even_probe_logprobs, \"game pos row col -> row col\", \"mean\").sum()\n",
        "        loss_odd = -einops.reduce(odd_probe_logprobs, \"game pos row col -> row col\", \"mean\").sum()\n",
        "        loss_both = -einops.reduce(both_probe_logprobs, \"game pos row col -> row col\", \"mean\").sum()\n",
        "        # We backprop on the sum of all 3 losses\n",
        "        loss = loss_even + loss_odd + loss_both\n",
        "\n",
        "        if self.args.use_wandb:\n",
        "            wandb.log(\n",
        "                {\n",
        "                    wandb_folder + \"loss\": loss.item(),\n",
        "                    # wandb_folder + \"loss_even\": loss_even.item(),\n",
        "                    # wandb_folder + \"loss_odd\": loss_odd.item(),\n",
        "                    # wandb_folder + \"loss_both\": loss_both.item(),\n",
        "                },\n",
        "                step=self.step,\n",
        "            )\n",
        "        return loss\n",
        "\n",
        "    def shuffle_training_indices(self):\n",
        "        \"\"\"\n",
        "        Returns the tensors you'll use to index into the training data.\n",
        "        \"\"\"\n",
        "        num_games = len(self.train_board_state_dataset)\n",
        "        n_indices = num_games - (num_games % self.args.batch_size)\n",
        "        full_train_indices = t.randperm(num_games)[:n_indices]\n",
        "        full_train_indices = einops.rearrange(\n",
        "            full_train_indices, \"(batch_idx game_idx) -> batch_idx game_idx\", game_idx=self.args.batch_size\n",
        "        )\n",
        "        return full_train_indices\n",
        "\n",
        "    def validate(self) -> bool:\n",
        "        self.model.train(False)\n",
        "        indices = random.sample(range(len(self.val_board_state_dataset)), min(self.args.n_tokens_to_test_and_val, len(self.val_board_state_dataset)))\n",
        "        games_id = self.val_board_state_dataset.board_seqs_id[indices]\n",
        "        games_square = self.val_board_state_dataset.board_seqs_square[indices]\n",
        "        loss = self._get_mean_loss(games_id, games_square, wandb_folder=\"val/\", othello_rule=self.args.othello_rule)\n",
        "\n",
        "        if loss < self.best_loss:\n",
        "            self.patience_counter = 0\n",
        "            self.best_loss = loss\n",
        "            self.save_checkpoint()\n",
        "        else:\n",
        "            self.patience_counter += 1\n",
        "\n",
        "        if self.patience_counter >= self.args.early_stopping_patience:\n",
        "            print(f\"Early Stop Triggered at step {self.step}\")\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def test(self):\n",
        "        self.model.train(False)\n",
        "        for othello_rule, dataset in self.othello_rule_to_board_state_test_dataset.items():\n",
        "            indices = random.sample(range(len(dataset)), min(self.args.n_tokens_to_test_and_val, len(dataset)))\n",
        "            games_id = dataset.board_seqs_id[indices]\n",
        "            games_square = dataset.board_seqs_square[indices]\n",
        "            loss = self._get_mean_loss(games_id, games_square, wandb_folder=f\"test/{othello_rule.value}_\", othello_rule=othello_rule)\n",
        "            if self.args.use_wandb:\n",
        "                fig, mean_accuracy_even_on_even, mean_accuracy_odd_on_odd, mean_accuracy_both_on_all = evaluate_probe_generalization(self.linear_probe, self.model, games_square, games_id,\n",
        "                                                                                                                                     self.args.layer, DEVICE, othello_rule,\n",
        "                                                                                                                                     plot_results=False, fake_board_state_transform=self.args.fake_board_state_transform)\n",
        "                wandb.log(\n",
        "                    {\n",
        "                        \"test/mean_accuracy_even_on_even\": mean_accuracy_even_on_even,\n",
        "                        \"test/mean_accuracy_odd_on_odd\": mean_accuracy_odd_on_odd,\n",
        "                        \"test/mean_accuracy_both_on_all\": mean_accuracy_both_on_all,\n",
        "                        \"test/mean_accuracy\": wandb.Plotly(fig),\n",
        "                    },\n",
        "                    step=self.step,\n",
        "                )\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        assert self.train_board_state_dataset is not None\n",
        "        assert self.val_board_state_dataset is not None\n",
        "        assert self.othello_rule_to_board_state_test_dataset is not None\n",
        "        self.step = 0\n",
        "        self.run_id = 0\n",
        "        if self.args.use_wandb:\n",
        "            wandb.init(project=self.args.wandb_project, name=self.args.wandb_name, config=self.args)\n",
        "            self.run_id = wandb.run.id\n",
        "\n",
        "        optimizer = t.optim.AdamW(\n",
        "            [self.linear_probe], lr=self.args.lr, betas=self.args.betas, weight_decay=self.args.weight_decay\n",
        "        )\n",
        "\n",
        "        self.best_loss = float('inf')\n",
        "        self.patience_counter = 0\n",
        "        early_stopped = False\n",
        "        for epoch in range(self.args.epochs):\n",
        "            if early_stopped:\n",
        "                break\n",
        "            print(f\"Epoch {epoch + 1}/{self.args.epochs}\")\n",
        "            full_train_indices = self.shuffle_training_indices()\n",
        "            progress_bar = tqdm(full_train_indices)\n",
        "            for indices in progress_bar:\n",
        "                if early_stopped:\n",
        "                    break\n",
        "                games_id = self.train_board_state_dataset.board_seqs_id[indices]  # shape [batch n_moves=60]\n",
        "                games_square = self.train_board_state_dataset.board_seqs_square[indices]  # shape [batch n_moves=60]\n",
        "                loss = self._get_mean_loss(games_id, games_square, wandb_folder=\"train/\", othello_rule=self.args.othello_rule)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                progress_bar.set_description(f\"Loss = {loss:.4f}\")\n",
        "\n",
        "                if self.step % self.args.validate_every_n_steps == 0:\n",
        "                    early_stopped = self.validate()\n",
        "                    self.test()\n",
        "\n",
        "                self.step += 1\n",
        "\n",
        "        if self.args.use_wandb:\n",
        "            wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD0hMaHVN0ih"
      },
      "source": [
        "# --- Data ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "b-uoUhhdRF-M"
      },
      "outputs": [],
      "source": [
        "def get_othello_rules_with_data():\n",
        "    return [OthelloRule.STANDARD] # , OthelloRule.NEXT_TO_OPPONENT, OthelloRule.NO_FLIPPING]\n",
        "\n",
        "def get_othello_rule(goal: Goal) -> OthelloRule:\n",
        "    if goal == Goal.WEAK_GOAL:\n",
        "        return OthelloRule.STANDARD\n",
        "    elif goal == Goal.STRONG_GOAL:\n",
        "        # return OthelloRule.BIAS_CLOCK\n",
        "        # return OthelloRule.NEXT_TO_OPPONENT\n",
        "        # return OthelloRule.CHESS\n",
        "        # return OthelloRule.UNTRAINED\n",
        "        # return OthelloRule.CONSTANT_PARAMETERS\n",
        "        return OthelloRule.NO_FLIPPING\n",
        "    else:\n",
        "        raise Exception(f\"Unknown goal: {goal}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O32GEknzUgx0"
      },
      "source": [
        "Load all data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJuk12gDRWUd",
        "outputId": "38c273f5-98db-443f-e79a-3e1e460f0b09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded train games\n",
            "Loaded weak_finetune games\n",
            "Loaded val games\n",
            "Loaded: 26539984 train, 13271044 weak_finetune, 4423295 val, 8853073 test games\n",
            "Skipped 0 games\n",
            "Skipped 0 games\n",
            "Skipped 0 games\n"
          ]
        }
      ],
      "source": [
        "## Data for get_othello_rules_with_data ###\n",
        "# data sets\n",
        "othello = Othello(data_folder, OthelloRule.STANDARD)\n",
        "#### othello.generate_train_weakfinetune_val_test_split()\n",
        "othello.load_train_weakfinetune_val_test()\n",
        "train_dataset = CharDataset(othello.train)\n",
        "val_dataset = CharDataset(othello.val)\n",
        "test_dataset = CharDataset(othello.test)\n",
        "weak_finetune_dataset = CharDataset(othello.weak_finetune)\n",
        "othello_rule_to_test = {}\n",
        "\n",
        "# test data\n",
        "for rule in get_othello_rules_with_data():\n",
        "     othello_rule_to_test[rule] = CharDataset(othello.othello_rule_to_test[rule])\n",
        "\n",
        "# # data to train linear probe\n",
        "train_board_state_dataset = BoardStateDataset(train_dataset, 150000)\n",
        "val_board_state_dataset = BoardStateDataset(val_dataset, 100)\n",
        "othello_rule_to_board_state_test_dataset = {}\n",
        "for rule in get_othello_rules_with_data():\n",
        "   char_test_data = othello_rule_to_test[rule]\n",
        "   othello_rule_to_board_state_test_dataset[rule] = BoardStateDataset(char_test_data, 100)\n",
        "\n",
        "# ## Data for extra rules ###\n",
        "# for rule in OthelloRule:\n",
        "#     if rule not in othello_rule_to_test:\n",
        "#         othello_rule_to_test[rule] = othello_rule_to_test[OthelloRule.STANDARD]\n",
        "\n",
        "# for rule in OthelloRule:\n",
        "#     if rule not in othello_rule_to_board_state_test_dataset:\n",
        "#         othello_rule_to_board_state_test_dataset[rule] = othello_rule_to_board_state_test_dataset[OthelloRule.STANDARD]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlbVxGWLIfvb"
      },
      "source": [
        "### Test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35c0-p14UiYN"
      },
      "source": [
        "Load only test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "b32Gs_cC2Iii"
      },
      "outputs": [],
      "source": [
        "# othello = Othello(data_folder, OthelloRule.STANDARD)\n",
        "# othello.load_test()\n",
        "# othello_rule_to_test = {}\n",
        "# othello_rule_to_board_state_test_dataset = {}\n",
        "# for rule in get_othello_rules_with_data():\n",
        "#     othello_rule_to_test[rule] = CharDataset(othello.othello_rule_to_test[rule])\n",
        "#     othello_rule_to_board_state_test_dataset[rule] = BoardStateDataset(othello_rule_to_test[rule], 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "h7nUAceAo9df"
      },
      "outputs": [],
      "source": [
        "# othello = Othello(data_folder, OthelloRule.NEXT_TO_OPPONENT)\n",
        "# games = othello.load_games(1000)\n",
        "# dataset = CharDataset(games)\n",
        "# board_seqs_square, board_seqs_id, n_skipped = get_board_seqs_square_and_id(othello_rule_to_test[OthelloRule.NEXT_TO_OPPONENT], 10000)\n",
        "# print(f\"Skipped {n_skipped} games\")\n",
        "# plot_average_move_index(board_seqs_square)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "eYLb4I7FGO1y",
        "outputId": "47c96a5d-8e9d-4d9d-a29b-87b8c300fc89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[44, 45, 46, 52, 59, 47, 19, 20, 21, 26, 38, 12, 43, 60, 54, 34, 18, 14, 33, 17, 61, 63, 10, 30, 31, 25, 62, 9, 13, 32, 29, 58, 7, 3, 1, 6, 40, 15, 4, 22, 0, 42, 2, 48, 23, 41, 24, 5, 49, 11, 39, 56, 37, 16, 51, 53, 57, 8, 55]\n",
            "['F4', 'F5', 'F6', 'G4', 'H3', 'F7', 'C3', 'C4', 'C5', 'D2', 'E6', 'B4', 'F3', 'H4', 'G6', 'E2', 'C2', 'B6', 'E1', 'C1', 'H5', 'H7', 'B2', 'D6', 'D7', 'D1', 'H6', 'B1', 'B5', 'E0', 'D5', 'H2', 'A7', 'A3', 'A1', 'A6', 'F0', 'B7', 'A4', 'C6', 'A0', 'F2', 'A2', 'G0', 'C7', 'F1', 'D0', 'A5', 'G1', 'B3', 'E7', 'H0', 'E5', 'C0', 'G3', 'G5', 'H1', 'B0', 'G7']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"fd006bfc-c802-4f3d-b73a-979a863633d0\" class=\"plotly-graph-div\" style=\"height:525px; width:1000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"fd006bfc-c802-4f3d-b73a-979a863633d0\")) {                    Plotly.newPlot(                        \"fd006bfc-c802-4f3d-b73a-979a863633d0\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"x\":[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"],\"y\":[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"],\"z\":[[0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0],[0,0,0,-1,1,0,0,0],[0,0,0,1,-1,0,0,0],[0,0,0,0,1,-1,0,0],[0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"x: %{x}\\u003cbr\\u003ey: %{y}\\u003cbr\\u003ecolor: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"},{\"colorscale\":[[0,\"#ff0c0c\"],[1,\"#ff0c0c\"]],\"showscale\":false,\"z\":[[null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null],[null,null,null,1.0,null,null,null,null],[null,null,1.0,null,null,null,null,null],[null,null,null,null,null,1.0,null,null],[null,null,null,null,null,null,1.0,null],[null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.18400000000000002],\"scaleanchor\":\"y\",\"constrain\":\"domain\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\"},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.20400000000000001,0.388],\"matches\":\"x\"},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,1.0],\"matches\":\"y\",\"showticklabels\":false},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.40800000000000003,0.5920000000000001],\"matches\":\"x\"},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.0,1.0],\"matches\":\"y\",\"showticklabels\":false},\"xaxis4\":{\"anchor\":\"y4\",\"domain\":[0.6120000000000001,0.7960000000000002],\"matches\":\"x\"},\"yaxis4\":{\"anchor\":\"x4\",\"domain\":[0.0,1.0],\"matches\":\"y\",\"showticklabels\":false},\"xaxis5\":{\"anchor\":\"y5\",\"domain\":[0.8160000000000001,1.0],\"matches\":\"x\"},\"yaxis5\":{\"anchor\":\"x5\",\"domain\":[0.0,1.0],\"matches\":\"y\",\"showticklabels\":false},\"annotations\":[{\"font\":{},\"showarrow\":false,\"text\":\"State after move 1\",\"x\":0.09200000000000001,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(255,255,255)\"],[0.125,\"rgb(240,240,240)\"],[0.25,\"rgb(217,217,217)\"],[0.375,\"rgb(189,189,189)\"],[0.5,\"rgb(150,150,150)\"],[0.625,\"rgb(115,115,115)\"],[0.75,\"rgb(82,82,82)\"],[0.875,\"rgb(37,37,37)\"],[1.0,\"rgb(0,0,0)\"]],\"cmin\":-1,\"cmax\":1,\"showscale\":false},\"title\":{\"text\":\"Board states\"},\"width\":1000},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('fd006bfc-c802-4f3d-b73a-979a863633d0');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# example_game = othello.train[0]\n",
        "example_game = [to_square(i) for i in train_dataset[0][0].tolist()]\n",
        "# example_game = othello_rule_to_test[OthelloRule.STANDARD].data[0]\n",
        "# example_game = [to_square(i) for i in othello_rule_to_test[OthelloRule.STANDARD][0][0].tolist()]\n",
        "print(example_game)\n",
        "print([to_label(to_id(s)) for s in example_game])\n",
        "plot_game_moves(example_game, [1], othello_rule=OthelloRule.STANDARD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "M3gwrPE1vaUI"
      },
      "outputs": [],
      "source": [
        "# entropy = calculate_legal_move_entropy(othello_rule_to_test[OthelloRule.NEXT_TO_OPPONENT], 10000, OthelloRule.NEXT_TO_OPPONENT)\n",
        "# print(f\"Legal move uniform entropy: {entropy:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdfy6jmhFtgU",
        "outputId": "3b3b5d5d-7851-4ccd-f6d8-b7766efb661a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FakeBoardStateTransformModulo(\n",
            "  (linear): Linear(in_features=64, out_features=64, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "def load_fake_board_state_transform(experiment_folder: str, fake_board_state_name: str, device: str, overwrite=False):\n",
        "    path = os.path.join(experiment_folder, f'fake_board_state_transform_{fake_board_state_name}.pt')\n",
        "    if not overwrite and os.path.exists(path):\n",
        "        return t.load(path, map_location=device, weights_only=False)\n",
        "    else:\n",
        "        if fake_board_state_name == 'placed_before':\n",
        "            transform = FakeBoardStateTransformPlacedBefore()\n",
        "        elif fake_board_state_name == 'black_white':\n",
        "            transform = FakeBoardStateTransformBlackWhite()\n",
        "        elif fake_board_state_name == 'modulo':\n",
        "            transform = FakeBoardStateTransformModulo()\n",
        "        else:\n",
        "            raise Exception(f\"Unknown fake board state transform: {fake_board_state_name}\")\n",
        "        assert transform.name == fake_board_state_name\n",
        "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "        t.save(transform, path)\n",
        "        return transform\n",
        "\n",
        "# fake_board_state_name = 'placed_before'\n",
        "# fake_board_state_name = 'black_white'\n",
        "fake_board_state_name = 'modulo'\n",
        "fake_board_state_transform = load_fake_board_state_transform(experiment_folder, fake_board_state_name, DEVICE, overwrite=False)\n",
        "print(fake_board_state_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "oQp8yowuZfD6",
        "outputId": "cb5c73be-be1b-4eda-9a52-eb8320c995ba"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1456987571.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mException\u001b[0m: "
          ]
        }
      ],
      "source": [
        "raise Exception()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibUnvgVDT2sc"
      },
      "source": [
        "# --- Run Single ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6KeGvQd-xri"
      },
      "source": [
        "### Load OthelloGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4nHYW2XT1-T"
      },
      "outputs": [],
      "source": [
        "othello_gpt = load_othello_gpt(device=DEVICE)\n",
        "get_model_size(othello_gpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foPiTL1TT4dM"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "results = test_hooked_transformer(othello_gpt, othello_rule=OthelloRule.STANDARD, test_dataset=test_dataset, batch_size=512, n_games=10000, device=DEVICE)\n",
        "print(\"\")\n",
        "print(f\"Validation CE Loss: {results['ce_loss']:.6f}\")\n",
        "print(f\"Illegal Move %: {results['illegal_move_percentage']:.6f}%\")\n",
        "print(f\"Total Predictions: {results['total_predictions']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLI0jsAcxzDq"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "results = test_hooked_transformer(hooked_model, othello_rule=OthelloRule.STANDARD, test_dataset=test_dataset, batch_size=512, n_games=10000, device=DEVICE)\n",
        "print(\"\")\n",
        "print(f\"Validation CE Loss: {results['ce_loss']:.6f}\")\n",
        "print(f\"Illegal Move %: {results['illegal_move_percentage']:.6f}%\")\n",
        "print(f\"Total Predictions: {results['total_predictions']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6phKVorZgBJ"
      },
      "outputs": [],
      "source": [
        "raise Exception()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8PxZcUe0zJL"
      },
      "source": [
        "### Pretrain Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwJL2tBgo5-r"
      },
      "outputs": [],
      "source": [
        "def pretrain_model(model_size: ModelSize, othello_rule: OthelloRule,\n",
        "                train_dataset: CharDataset, val_dataset: CharDataset,\n",
        "                othello_rule_to_test: dict[OthelloRule, list[CharDataset]],\n",
        "                experiment_folder: str, project_name: str,\n",
        "                ):\n",
        "    assert othello_rule is not OthelloRule.UNTRAINED\n",
        "    model_cfg = get_model_config(model_size, train_dataset)\n",
        "    model = GPT(model_cfg)\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
        "    experiment_name = get_experiment_name_pretrained(model_size, othello_rule, index=0)\n",
        "    experiment_name += f\"_{timestamp}\"\n",
        "\n",
        "    max_epochs = 2\n",
        "    validate_n_times = 100\n",
        "    batch_size = 512\n",
        "    validate_every_n_steps = int(max_epochs * len(train_dataset) / (validate_n_times * batch_size))\n",
        "    print(\"validate_every_n_steps: \", validate_every_n_steps)\n",
        "    # initialize a trainer instance and kick off training\n",
        "    t_start = time.strftime(\"_%Y%m%d_%H%M%S\")\n",
        "    trainer_cfg = TrainerConfig(\n",
        "        max_epochs=max_epochs,\n",
        "        early_stopping_patience=float('inf'),\n",
        "        validate_every_n_steps=validate_every_n_steps,\n",
        "        weight_decay=0.1,\n",
        "        batch_size=batch_size,  #  We only have one GPU\n",
        "        learning_rate=5e-4,  # 5e-4\n",
        "        betas = (0.9, 0.95),\n",
        "        grad_norm_clip = 1.0,\n",
        "        lr_decay=True,\n",
        "        warmup_tokens=len(train_dataset)*train_dataset.block_size*max_epochs*0.05, # Warum up over first 5%\n",
        "        final_tokens=len(train_dataset)*train_dataset.block_size*max_epochs,\n",
        "        num_workers=0,  # For dataloader, was 1 before\n",
        "        n_tokens_to_test_and_val=10**4,\n",
        "        n_games_to_test_legal_moves=20,\n",
        "        # checkpoint settings\n",
        "        save_model=True,\n",
        "        save_every_validation=False,\n",
        "        experiment_folder=experiment_folder,\n",
        "        project_name=project_name,\n",
        "        experiment_name=experiment_name,\n",
        "        model_size=model_size,\n",
        "        othello_rule=othello_rule,\n",
        "        index=0,\n",
        "        final=False,\n",
        "        # wandb\n",
        "        wandb_project=project_name,\n",
        "        use_wandb=True,\n",
        "        wandb_name=experiment_name\n",
        "    )\n",
        "    trainer = TrainerPretrain(model, train_dataset, val_dataset, othello_rule_to_test, trainer_cfg, model_cfg, DEVICE)\n",
        "    device = trainer.device\n",
        "    print(t_start)\n",
        "    trainer.train()\n",
        "\n",
        "\n",
        "project_name = \"0_pretrain_sweep\"\n",
        "for model_size in [ModelSize.NANO, ModelSize.MINI, ModelSize.MICRO, ModelSize.SMALL, ModelSize.MEDIUM]:\n",
        "    pretrain_model(model_size, OthelloRule.STANDARD,\n",
        "                train_dataset, val_dataset, othello_rule_to_test,\n",
        "                experiment_folder, project_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EK7cI0By3_v"
      },
      "outputs": [],
      "source": [
        "def create_untrained_model(model_size, experiment_folder, project_name, val_dataset, constant_everything: bool, device):\n",
        "    model_cfg = get_model_config(model_size, val_dataset)\n",
        "    model = GPT(model_cfg)\n",
        "    model.to(device)\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
        "    if constant_everything:\n",
        "        othello_rule = OthelloRule.CONSTANT_PARAMETERS\n",
        "    else:\n",
        "        othello_rule = OthelloRule.UNTRAINED\n",
        "    experiment_name = get_experiment_name_pretrained(model_size, othello_rule, index=0, final=False)\n",
        "    experiment_name += f\"_{timestamp}\"\n",
        "    run_id = 0\n",
        "\n",
        "    hooked_transformer_model = convert_gpt_to_hooked_and_verify(model, model_cfg, val_dataset, device=device)\n",
        "    if constant_everything:\n",
        "        with t.no_grad():\n",
        "          for name, param in hooked_transformer_model.named_parameters():\n",
        "              if not 'embed' in name and not name in ['blocks.0.attn.W_Q', 'blocks.0.attn.b_Q', 'blocks.0.attn.W_K', 'blocks.0.attn.b_K']:\n",
        "                mean_val = param.mean().item()\n",
        "                param.fill_(mean_val)\n",
        "\n",
        "    save_model(\n",
        "        hooked_transformer_model,\n",
        "        run_id,\n",
        "        project_name,\n",
        "        experiment_name,\n",
        "        experiment_folder,\n",
        "        index=0,\n",
        "        overwrite=True,\n",
        "        final=True,\n",
        "    )\n",
        "    return hooked_transformer_model\n",
        "\n",
        "# project_name = \"0_pretrain_sweep\"  # \"pretrain_sweep_1\"\n",
        "# for model_size in ModelSize:\n",
        "#     create_untrained_model(model_size, experiment_folder, project_name, val_dataset, constant_everything=True, device=DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQ6obsmRZhmj"
      },
      "outputs": [],
      "source": [
        "raise Exception()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdOSlYFi6-HI"
      },
      "source": [
        "### train_chess_gpt_embedding_matrices.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iG9OeV6B6-jS"
      },
      "outputs": [],
      "source": [
        "def train_chess_gpt_embedding_matrices(model_size, othello_rule, experiment_folder, project_name, train_dataset, val_dataset, othello_rule_to_test, device):\n",
        "    \"\"\"Complete pipeline: train projections, then create final model\"\"\"\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
        "    experiment_name = get_experiment_name_pretrained(model_size, othello_rule, index=0)\n",
        "    experiment_name += f\"_{timestamp}\"\n",
        "\n",
        "    max_epochs = 2\n",
        "    validate_n_times = 100\n",
        "    batch_size = 512\n",
        "    validate_every_n_steps = 100 # int(max_epochs * len(train_dataset) / (validate_n_times * batch_size))\n",
        "    print(\"validate_every_n_steps: \", validate_every_n_steps)\n",
        "    # initialize a trainer instance and kick off training\n",
        "    t_start = time.strftime(\"_%Y%m%d_%H%M%S\")\n",
        "    trainer_cfg = ProjectionTrainerConfig(\n",
        "        max_epochs=max_epochs,\n",
        "        early_stopping_patience=float('inf'),\n",
        "        validate_every_n_steps=validate_every_n_steps,\n",
        "        weight_decay=0.1,\n",
        "        batch_size=batch_size,  #  We only have one GPU\n",
        "        learning_rate=5e-4,  # 5e-4\n",
        "        betas = (0.9, 0.95),\n",
        "        grad_norm_clip = 1.0,\n",
        "        lr_decay=True,\n",
        "        warmup_tokens=len(train_dataset)*train_dataset.block_size*max_epochs*0.05, # Warm up over first 5%\n",
        "        final_tokens=len(train_dataset)*train_dataset.block_size*max_epochs,\n",
        "        num_workers=0,  # For dataloader, was 1 before\n",
        "        n_tokens_to_test_and_val=10**4,\n",
        "        n_games_to_test_legal_moves=20,\n",
        "        # checkpoint settings\n",
        "        save_model=True,\n",
        "        save_every_validation=False,\n",
        "        experiment_folder=experiment_folder,\n",
        "        project_name=project_name,\n",
        "        experiment_name=experiment_name,\n",
        "        model_size=model_size,\n",
        "        othello_rule=othello_rule,\n",
        "        index=0,\n",
        "        final=False,\n",
        "        # wandb\n",
        "        wandb_project=project_name,\n",
        "        use_wandb=True,\n",
        "        wandb_name=experiment_name,\n",
        "        # Llava\n",
        "        target_vocab_size=61\n",
        "    )\n",
        "\n",
        "    trainer = ProjectionTrainer(train_dataset, val_dataset, othello_rule_to_test, trainer_cfg, device)\n",
        "    trainer.train()\n",
        "\n",
        "\n",
        "# project_name = \"playground_othello\"\n",
        "# train_chess_gpt_embedding_matrices(ModelSize.HUGE, OthelloRule.BIAS_CLOCK, experiment_folder, project_name, train_dataset, val_dataset, othello_rule_to_test, DEVICE)\n",
        "\n",
        "\n",
        "# final_othello_gpt = create_othello_gpt_from_chess(\n",
        "#     projection_matrices_path=f\"{experiment_folder}/copy_projection_matrices_None.pt\",\n",
        "#     device=DEVICE\n",
        "# )\n",
        "# test_hooked_transformer(final_othello_gpt, othello_rule_to_test[OthelloRule.BIAS_CLOCK], 64, 100, DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V820C1vGXVY1"
      },
      "source": [
        "### Finetune model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9UI_0DcUpHv"
      },
      "outputs": [],
      "source": [
        "weak_size = ModelSize.MINI\n",
        "strong_size = ModelSize.SMALL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc9xQ7tqBN3E"
      },
      "source": [
        "weak model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELqSFxljV_8t"
      },
      "outputs": [],
      "source": [
        "weak_model = load_model_pretrained(\n",
        "    project_name=\"0_pretrain_sweep\",\n",
        "    model_size=weak_size,\n",
        "    othello_rule=OthelloRule.STANDARD,\n",
        "    experiment_folder=experiment_folder,\n",
        "    device=DEVICE,\n",
        "    index=0,\n",
        "    final=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDYd8HC7BP7O"
      },
      "source": [
        "strong model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5oEB0jMVOCo"
      },
      "outputs": [],
      "source": [
        "strong_model = load_model_pretrained(\n",
        "    project_name=\"0_pretrain_sweep\",\n",
        "    model_size=strong_size,\n",
        "    othello_rule=OthelloRule.CONSTANT_PARAMETERS,\n",
        "    experiment_folder=experiment_folder,\n",
        "    device=DEVICE,\n",
        "    index=0,\n",
        "    final=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGB5gMY5UyXn"
      },
      "outputs": [],
      "source": [
        "# strong_model = load_model_pretrained(\n",
        "#     project_name=\"pretrain_sweep_1\",\n",
        "#     model_size=strong_size,\n",
        "#     othello_rule=OthelloRule.UNTRAINED,\n",
        "#     experiment_folder=experiment_folder,\n",
        "#     device=DEVICE,\n",
        "#     index=0,\n",
        "#     final=True,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B45O_uenu9La"
      },
      "outputs": [],
      "source": [
        "# strong_model = load_tiny_stories_instruct_8m(DEVICE)\n",
        "# strong_model = final_othello_gpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBdADstpV9f8"
      },
      "outputs": [],
      "source": [
        "def finetune(weak_model, weak_model_size: ModelSize, weak_othello_rule: OthelloRule,\n",
        "             strong_model, strong_model_size: ModelSize, strong_othello_rule: OthelloRule,\n",
        "                weak_finetune_dataset: CharDataset, val_dataset: CharDataset,\n",
        "                othello_rule_to_test: dict[OthelloRule, list[CharDataset]],\n",
        "                experiment_folder: str, project_name: str,\n",
        "                ):\n",
        "    weak_model, _ = convert_hooked_to_gpt_and_verify(weak_model, val_dataset, DEVICE)\n",
        "    strong_model, model_cfg = convert_hooked_to_gpt_and_verify(strong_model, val_dataset, DEVICE)\n",
        "\n",
        "    # model_cfg = get_model_config(strong_model_size, train_dataset)\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
        "    experiment_name = get_experiment_name_finetuned(weak_model_size, weak_othello_rule, strong_model_size, strong_othello_rule, index=0)\n",
        "    experiment_name += f\"_{timestamp}\"\n",
        "\n",
        "    max_epochs = 2\n",
        "    batch_size = 512\n",
        "    validate_every_n_steps = 100 # 10  or 100 for untrained/constant_parameters\n",
        "    print(\"validate_every_n_steps: \", validate_every_n_steps)\n",
        "    # initialize a trainer instance and kick off training\n",
        "    t_start = time.strftime(\"_%Y%m%d_%H%M%S\")\n",
        "    trainer_cfg = TrainerConfigFinetune(\n",
        "        max_epochs=max_epochs,\n",
        "        early_stopping_patience=100,\n",
        "        validate_every_n_steps=validate_every_n_steps,\n",
        "        weight_decay=0.1,\n",
        "        batch_size=batch_size,  #  We only have one GPU\n",
        "        learning_rate=1e-5,  # 5e-4\n",
        "        betas = (0.9, 0.95),\n",
        "        grad_norm_clip = 1.0,\n",
        "        lr_decay=False,\n",
        "        warmup_tokens=len(weak_finetune_dataset)*weak_finetune_dataset.block_size*max_epochs*0.05, # Warum up over first 5%\n",
        "        final_tokens=len(weak_finetune_dataset)*weak_finetune_dataset.block_size*max_epochs,\n",
        "        num_workers=0,  # For dataloader, was 1 before\n",
        "        n_tokens_to_test_and_val=10**4,\n",
        "        n_games_to_test_legal_moves=20,\n",
        "        # checkpoint settings\n",
        "        save_model=True,\n",
        "        save_every_validation=False,\n",
        "        experiment_folder=experiment_folder,\n",
        "        project_name=project_name,\n",
        "        experiment_name=experiment_name,\n",
        "        model_size=strong_model_size,\n",
        "        othello_rule=weak_othello_rule,  # We want to measure legal moves on the weak rule on which it gets trained\n",
        "        index=0,\n",
        "        final=False,\n",
        "        # wandb\n",
        "        wandb_project=project_name,\n",
        "        use_wandb=True,\n",
        "        wandb_name=experiment_name,\n",
        "        # finetuning\n",
        "        weak_supervision_model_size=weak_model_size,\n",
        "        weak_supervision_rule=weak_othello_rule,\n",
        "    )\n",
        "    trainer = TrainerFinetune(strong_model, weak_model, weak_finetune_dataset, val_dataset, othello_rule_to_test, trainer_cfg, model_cfg, DEVICE)\n",
        "    device = trainer.device\n",
        "    print(t_start)\n",
        "    trainer.train()\n",
        "\n",
        "\n",
        "project_name = \"finetune_sweep_constant_parameters\"\n",
        "finetune(weak_model, weak_size, OthelloRule.STANDARD,\n",
        "         strong_model, strong_size, OthelloRule.CONSTANT_PARAMETERS,\n",
        "            weak_finetune_dataset, val_dataset, othello_rule_to_test,\n",
        "            experiment_folder, project_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rs3nOk7_WCrd"
      },
      "outputs": [],
      "source": [
        "raise Exception()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy2e79Ay0ops"
      },
      "source": [
        "### Train MultiProbe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqdIKWvEXyCi"
      },
      "outputs": [],
      "source": [
        "# Pretrain\n",
        "othello_rule = OthelloRule.STANDARD\n",
        "model_size = ModelSize.HUGE\n",
        "hooked_model = load_model_pretrained(\n",
        "    project_name=\"pretrain_sweep_1\",\n",
        "    model_size=model_size,\n",
        "    othello_rule=othello_rule,\n",
        "    experiment_folder=experiment_folder,\n",
        "    device=DEVICE,\n",
        "    index=0,\n",
        "    final=False,\n",
        "    linear_probe=False,\n",
        ")\n",
        "\n",
        "# # Finetune\n",
        "# weak_model_size = ModelSize.NANO\n",
        "# weak_rule = OthelloRule.STANDARD\n",
        "# strong_model_size = ModelSize.HUGE\n",
        "# strong_rule = OthelloRule.BIAS_CLOCK\n",
        "# hooked_model = load_finetuned_model(\n",
        "#     project_name=\"finetune_sweep_2\",\n",
        "#     weak_model_size=weak_model_size,\n",
        "#     weak_rule=weak_rule,\n",
        "#     strong_model_size=strong_model_size,\n",
        "#     strong_rule=othello_rule,\n",
        "#     experiment_folder=experiment_folder,\n",
        "#     device=DEVICE,\n",
        "#     index=0,\n",
        "#     final=False,\n",
        "# )\n",
        "\n",
        "layer = int(hooked_model.cfg.n_layers / 4 * 3)\n",
        "print(layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R815OcbQGmlN"
      },
      "outputs": [],
      "source": [
        "fake_board_state_transform.linear.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--HNWhffP3dK"
      },
      "outputs": [],
      "source": [
        "def train_linear_probe(hooked_model: HookedTransformer,\n",
        "                       layer: int,\n",
        "                       project_name: str,\n",
        "                       experiment_name: str,\n",
        "                       experiment_folder: str,\n",
        "                       train_board_state_dataset: BoardStateDataset,\n",
        "                       val_board_state_dataset: BoardStateDataset,\n",
        "                       othello_rule_to_board_state_test_dataset: dict[OthelloRule, list[BoardStateDataset]],\n",
        "                       fake_board_state_transform: FakeBoardStateTransform | None = None,\n",
        "                       ):\n",
        "    t.set_grad_enabled(True)\n",
        "    args = MultiProbeTrainingArgs(\n",
        "        device=DEVICE,\n",
        "        layer=layer,\n",
        "        experiment_folder=experiment_folder,\n",
        "        experiment_name = experiment_name,\n",
        "        wandb_project=project_name,\n",
        "        use_wandb=True,\n",
        "        wandb_name=experiment_name,\n",
        "        lr=1e-4,\n",
        "        epochs=1,\n",
        "        validate_every_n_steps=100,\n",
        "        early_stopping_patience=2,\n",
        "        fake_board_state_transform=fake_board_state_transform,\n",
        "        )\n",
        "\n",
        "    # trainer = LinearMultiProbeTrainer(othello_gpt, args)\n",
        "    trainer = LinearMultiProbeTrainer(hooked_model,\n",
        "                                      args,\n",
        "                                      train_board_state_dataset,\n",
        "                                      val_board_state_dataset,\n",
        "                                      othello_rule_to_board_state_test_dataset\n",
        "                                      )\n",
        "    trainer.train()\n",
        "\n",
        "\n",
        "def train_linear_probe_pretrained(hooked_model: HookedTransformer,\n",
        "                       layer: int,\n",
        "                       project_name: str,\n",
        "                       othello_rule: OthelloRule,\n",
        "                       model_size: ModelSize,\n",
        "                       experiment_folder: str,\n",
        "                       train_board_state_dataset: BoardStateDataset,\n",
        "                       val_board_state_dataset: BoardStateDataset,\n",
        "                       othello_rule_to_board_state_test_dataset: dict[OthelloRule, list[BoardStateDataset]],\n",
        "                       final: bool,\n",
        "                       fake_board_state_transform: FakeBoardStateTransform | None = None,\n",
        "                       ):\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
        "    fake_board_state_transform_name = fake_board_state_transform.name if fake_board_state_transform is not None else None\n",
        "    experiment_name = get_experiment_name_pretrained(model_size, othello_rule, index=0, final=final, linear_probe=True, fake_probe=fake_board_state_transform_name)\n",
        "    experiment_name += f\"_{timestamp}\"\n",
        "    train_linear_probe(hooked_model, layer, project_name, experiment_name, experiment_folder,\n",
        "                       train_board_state_dataset, val_board_state_dataset, othello_rule_to_board_state_test_dataset, fake_board_state_transform)\n",
        "\n",
        "\n",
        "def train_linear_probe_finetuned(hooked_model: HookedTransformer,\n",
        "                       layer: int,\n",
        "                       project_name: str,\n",
        "                       weak_model_size: ModelSize,\n",
        "                       weak_rule: OthelloRule,\n",
        "                       strong_model_size: ModelSize,\n",
        "                       strong_rule: OthelloRule,\n",
        "                       experiment_folder: str,\n",
        "                       train_board_state_dataset: BoardStateDataset,\n",
        "                       val_board_state_dataset: BoardStateDataset,\n",
        "                       othello_rule_to_board_state_test_dataset: dict[OthelloRule, list[BoardStateDataset]],\n",
        "                       final: bool,\n",
        "                       fake_board_state_transform: FakeBoardStateTransform | None = None,\n",
        "                       ):\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
        "    fake_board_state_transform_name = fake_board_state_transform.name if fake_board_state_transform is not None else None\n",
        "    experiment_name = get_experiment_name_finetuned(weak_model_size, weak_rule, strong_model_size, strong_rule, index=0, final=final, linear_probe=True, fake_probe=fake_board_state_transform_name)\n",
        "    experiment_name += f\"_{timestamp}\"\n",
        "    train_linear_probe(hooked_model, layer, project_name, experiment_name, experiment_folder,\n",
        "                       train_board_state_dataset, val_board_state_dataset, othello_rule_to_board_state_test_dataset, fake_board_state_transform)\n",
        "\n",
        "# project_name = \"playground_othello\"\n",
        "# train_linear_probe_pretrained(hooked_model, layer, project_name, othello_rule, model_size, experiment_folder,\n",
        "#                    train_board_state_dataset, val_board_state_dataset, othello_rule_to_board_state_test_dataset, final=False,\n",
        "#                    fake_board_state_transform=fake_board_state_transform)\n",
        "# train_linear_probe_finetuned(hooked_model, layer, project_name, weak_model_size, weak_rule, strong_model_size, strong_rule, experiment_folder,\n",
        "#                    train_board_state_dataset, val_board_state_dataset, othello_rule_to_board_state_test_dataset, final=False.\n",
        "#                    fake_board_state_transform=fake_board_state_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvxnImAryZfm"
      },
      "outputs": [],
      "source": [
        "# Pretrain\n",
        "# linear_probe = load_model(\n",
        "#             project_name=project_name,\n",
        "#             model_size=model_size,\n",
        "#             othello_rule=othello_rule,\n",
        "#             experiment_folder=experiment_folder,\n",
        "#             device=DEVICE,\n",
        "#             index=0,\n",
        "#             final=False,\n",
        "#             linear_probe=True\n",
        "#         )\n",
        "\n",
        "# # Finetune\n",
        "linear_probe = load_finetuned_model(\n",
        "    project_name = project_name,\n",
        "    weak_model_size=weak_model_size,\n",
        "    weak_rule=weak_rule,\n",
        "    strong_model_size=strong_model_size,\n",
        "    strong_rule=othello_rule,\n",
        "    experiment_folder=experiment_folder,\n",
        "    device=DEVICE,\n",
        "    index=0,\n",
        "    final=False,\n",
        "    linear_probe=True\n",
        ")\n",
        "linear_probe.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O71UkcTTksvf"
      },
      "outputs": [],
      "source": [
        "board_seqs_square_test, board_seqs_id_test, n_skipped = get_board_seqs_square_and_id(test_dataset, 100)\n",
        "print(board_seqs_square_test.shape)\n",
        "# evaluate_probe_generalization(trainer.linear_probe, othello_gpt, board_seqs_square_test, board_seqs_id_test, args.layer, DEVICE, plot_results=True)\n",
        "fig, mean_accuracy_even_on_even, mean_accuracy_odd_on_odd, mean_accuracy_both_on_all = evaluate_probe_generalization(linear_probe,\n",
        "                                                                                                                     hooked_model,\n",
        "                                                                                                                     board_seqs_square_test,\n",
        "                                                                                                                     board_seqs_id_test,\n",
        "                                                                                                                     layer,\n",
        "                                                                                                                     DEVICE,\n",
        "                                                                                                                     plot_results=False)\n",
        "print(f\"Even probe on even data: {mean_accuracy_even_on_even:.4f}\")\n",
        "print(f\"Odd probe on odd data: {mean_accuracy_odd_on_odd.mean():.4f}\")\n",
        "print(f\"Both probe on all data: {mean_accuracy_both_on_all.mean():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYtoCnqRhqnM"
      },
      "outputs": [],
      "source": [
        "    # # Convert train_dataset to required tensor formats\n",
        "    # print(\"Converting game data to tensors...\")\n",
        "    # board_seqs_square, board_seqs_id, n_skipped = get_board_seqs_square_and_id(train_dataset, args.num_games)\n",
        "    # print(f\"Prepared {len(board_seqs_square)} games\")\n",
        "    # print(f\"Skipped {n_skipped} games\")\n",
        "    # print(f\"board_seqs_square shape: {board_seqs_square.shape}\")\n",
        "    # print(f\"board_seqs_id shape: {board_seqs_id.shape}\")\n",
        "    # print(\"Min/max in board_seqs_square:\", board_seqs_square.min(), board_seqs_square.max())\n",
        "    # print(\"Unique values:\", board_seqs_square.unique())\n",
        "    # check_opening_moves(board_seqs_square)\n",
        "    # plot_average_move_index(board_seqs_square)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NDyhs6_5ix9"
      },
      "outputs": [],
      "source": [
        "raise Exception()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HwS5QTYOjdu"
      },
      "source": [
        "# --- Run Sweeps ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU8sbUsfMa0H"
      },
      "source": [
        "### Results & Folder paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geI8k3VS7L_d"
      },
      "outputs": [],
      "source": [
        "experiment_folder = os.path.join(project_folder, 'experiments/PAPER_SWEEP_2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRkiK4AzMaab"
      },
      "outputs": [],
      "source": [
        "pretrained_rule_to_result_file = {\n",
        "    OthelloRule.STANDARD: os.path.join(experiment_folder, 'pretrain_sweep/pretrain_standard_next_to_opponent_results.pkl'),\n",
        "    OthelloRule.BIAS_CLOCK: os.path.join(experiment_folder, 'pretrain_sweep/pretrain_standard_bias_clock_results.pkl'),\n",
        "    OthelloRule.NEXT_TO_OPPONENT: os.path.join(experiment_folder, 'pretrain_sweep/pretrain_standard_next_to_opponent_results.pkl'),\n",
        "    OthelloRule.CHESS: os.path.join(experiment_folder, 'pretrain_sweep/pretrain_standard_chess_results.pkl'),\n",
        "    OthelloRule.UNTRAINED: os.path.join(experiment_folder, 'pretrain_sweep/pretrain_standard_untrained_results.pkl'),\n",
        "    OthelloRule.CONSTANT_PARAMETERS: os.path.join(experiment_folder, 'pretrain_sweep/pretrain_standard_constant_parameters_results.pkl'),\n",
        "    OthelloRule.NO_FLIPPING: os.path.join(experiment_folder, 'pretrain_sweep/pretrain_standard_no_flipping_results.pkl'),\n",
        "}\n",
        "pretrained_rule_to_result_file_linear_probe = {\n",
        "    OthelloRule.CHESS: os.path.join(experiment_folder, 'chess_linear_probe/board_prediction_pretrained.pkl'),\n",
        "}\n",
        "pretrained_rule_to_result_file_fake_linear_probe = {\n",
        "    OthelloRule.CHESS: os.path.join(experiment_folder, 'fake_pretrain_sweep_linear_probe/board_prediction_pretrained.pkl'),\n",
        "}\n",
        "pretrained_rule_to_result_file_black_white_linear_probe = {\n",
        "    OthelloRule.CHESS: os.path.join(experiment_folder, 'black_white_pretrain_sweep_linear_probe/board_prediction_pretrained.pkl'),\n",
        "}\n",
        "pretrained_rule_to_result_file_modulo_linear_probe = {\n",
        "    OthelloRule.CHESS: os.path.join(experiment_folder, 'modulo_pretrain_sweep_linear_probe/board_prediction_pretrained.pkl'),\n",
        "}\n",
        "for othello_rule in [OthelloRule.STANDARD, OthelloRule.BIAS_CLOCK, OthelloRule.NEXT_TO_OPPONENT, OthelloRule.UNTRAINED, OthelloRule.CONSTANT_PARAMETERS, OthelloRule.NO_FLIPPING]:\n",
        "    pretrained_rule_to_result_file_linear_probe[othello_rule] = os.path.join(experiment_folder, 'pretrain_sweep_linear_probe/board_prediction_pretrained.pkl')\n",
        "    pretrained_rule_to_result_file_fake_linear_probe[othello_rule] = os.path.join(experiment_folder, 'fake_pretrain_sweep_linear_probe/board_prediction_pretrained.pkl')\n",
        "    pretrained_rule_to_result_file_black_white_linear_probe[othello_rule] = os.path.join(experiment_folder, 'black_white_pretrain_sweep_linear_probe/board_prediction_pretrained.pkl')\n",
        "    pretrained_rule_to_result_file_modulo_linear_probe[othello_rule] = os.path.join(experiment_folder, 'modulo_pretrain_sweep_linear_probe/board_prediction_pretrained.pkl')\n",
        "\n",
        "\n",
        "finetuned_rule_to_result_file = {\n",
        "    OthelloRule.BIAS_CLOCK: os.path.join(experiment_folder, 'finetune_sweep_bias_clock/finetune_standard_bias_clock_results.pkl'),\n",
        "    OthelloRule.NEXT_TO_OPPONENT: os.path.join(experiment_folder, 'finetune_sweep_next_to_opponent/finetune_standard_next_to_opponent_results.pkl'),\n",
        "    OthelloRule.CHESS: os.path.join(experiment_folder, 'chess_1/finetune_standard_chess_results.pkl'),\n",
        "    OthelloRule.UNTRAINED: os.path.join(experiment_folder, 'finetune_sweep_untrained/finetune_standard_untrained_results.pkl'),\n",
        "    OthelloRule.CONSTANT_PARAMETERS: os.path.join(experiment_folder, 'finetune_sweep_constant_parameters/finetune_standard_constant_parameters_results.pkl'),\n",
        "    OthelloRule.NO_FLIPPING: os.path.join(experiment_folder, 'finetune_sweep_no_flipping/finetune_standard_no_flipping_results.pkl'),\n",
        "}\n",
        "finetued_rule_to_result_file_linear_probe = {\n",
        "    OthelloRule.BIAS_CLOCK: os.path.join(experiment_folder, 'finetune_sweep_bias_clock_linear_probe/board_accuracy_finetuned_standard_bias_clock.pkl'),\n",
        "    OthelloRule.NEXT_TO_OPPONENT: os.path.join(experiment_folder, 'finetune_sweep_next_to_opponent_linear_probe/board_accuracy_finetuned_standard_next_to_opponent.pkl'),\n",
        "    OthelloRule.CHESS: os.path.join(experiment_folder, 'chess_linear_probe/board_accuracy_finetuned_standard_chess.pkl'),\n",
        "    OthelloRule.UNTRAINED: os.path.join(experiment_folder, 'finetune_sweep_untrained_linear_probe/board_accuracy_finetuned_standard_untrained.pkl'),\n",
        "    OthelloRule.CONSTANT_PARAMETERS: os.path.join(experiment_folder, 'experiments/finetune_sweep_constant_parameters_linear_probe/board_accuracy_finetuned_standard_constant_parameters.pkl'),\n",
        "    OthelloRule.NO_FLIPPING: os.path.join(experiment_folder, 'finetune_sweep_no_flipping_linear_probe/board_accuracy_finetuned_standard_no_flipping.pkl'),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCQDRrlM0Eev"
      },
      "outputs": [],
      "source": [
        "othello_rule_to_optimization_step = {\n",
        "    OthelloRule.BIAS_CLOCK: {\n",
        "        ModelSize.MICRO: {ModelSize.NANO: 2050},\n",
        "        ModelSize.MINI: {ModelSize.NANO: 180, ModelSize.MICRO: 380},\n",
        "        ModelSize.SMALL: {ModelSize.NANO: 30, ModelSize.MICRO: 80, ModelSize.MINI: 290},\n",
        "        ModelSize.MEDIUM: {ModelSize.NANO: 20, ModelSize.MICRO: 20, ModelSize.MINI: 330, ModelSize.SMALL: 1020},\n",
        "        ModelSize.LARGE: {ModelSize.NANO: 30, ModelSize.MICRO: 30, ModelSize.MINI: 110, ModelSize.SMALL: 210, ModelSize.MEDIUM: 310},\n",
        "        ModelSize.HUGE: {ModelSize.NANO: 20, ModelSize.MICRO: 20, ModelSize.MINI: 110, ModelSize.SMALL: 380, ModelSize.MEDIUM: 240, ModelSize.LARGE: 770},\n",
        "    },\n",
        "    OthelloRule.NEXT_TO_OPPONENT: {\n",
        "        ModelSize.MICRO: {ModelSize.NANO: 0},\n",
        "        ModelSize.MINI: {ModelSize.NANO: 0, ModelSize.MICRO: 0},\n",
        "        ModelSize.SMALL: {ModelSize.NANO: 10, ModelSize.MICRO: 20, ModelSize.MINI: 280},\n",
        "        ModelSize.MEDIUM: {ModelSize.NANO: 70, ModelSize.MICRO: 70, ModelSize.MINI: 270, ModelSize.SMALL: 8750},\n",
        "        ModelSize.LARGE: {ModelSize.NANO: 100, ModelSize.MICRO: 100, ModelSize.MINI: 320, ModelSize.SMALL: 15100, ModelSize.MEDIUM: 15840},\n",
        "        ModelSize.HUGE: {ModelSize.NANO: 90, ModelSize.MICRO: 320, ModelSize.MINI: 280, ModelSize.SMALL: 11000, ModelSize.MEDIUM: 7050, ModelSize.LARGE: 20530},\n",
        "    },\n",
        "    OthelloRule.CHESS: {\n",
        "        ModelSize.HUGE: {ModelSize.NANO: 7900, ModelSize.MICRO: 8100, ModelSize.MINI: 17300, ModelSize.SMALL: 17700, ModelSize.MEDIUM: 21700, ModelSize.LARGE: 25200},\n",
        "    },\n",
        "    OthelloRule.UNTRAINED: {\n",
        "        ModelSize.MICRO: {ModelSize.NANO: 31300},\n",
        "        ModelSize.MINI: {ModelSize.NANO: 29500, ModelSize.MICRO: 28100},\n",
        "        ModelSize.SMALL: {ModelSize.NANO: 16900, ModelSize.MICRO: 16500, ModelSize.MINI: 32900},\n",
        "        ModelSize.MEDIUM: {ModelSize.NANO: 19000, ModelSize.MICRO: 19800, ModelSize.MINI: 50800, ModelSize.SMALL: 24800},\n",
        "        ModelSize.LARGE: {ModelSize.NANO: 24600, ModelSize.MICRO: 14600, ModelSize.MINI: 34400, ModelSize.SMALL: 50800, ModelSize.MEDIUM:42200},\n",
        "        ModelSize.HUGE: {ModelSize.NANO: 25700, ModelSize.MICRO: 13200, ModelSize.MINI: 26100, ModelSize.SMALL: 50800, ModelSize.MEDIUM: 50800, ModelSize.LARGE: 50800},\n",
        "    },\n",
        "    OthelloRule.CONSTANT_PARAMETERS: {\n",
        "        ModelSize.MICRO: {ModelSize.NANO: 35200},\n",
        "        ModelSize.MINI: {ModelSize.NANO: 27200, ModelSize.MICRO: 27700},\n",
        "        ModelSize.SMALL: {ModelSize.NANO: 29300, ModelSize.MICRO: 22200, ModelSize.MINI: 33900},\n",
        "        ModelSize.MEDIUM: {ModelSize.NANO: 50800, ModelSize.MICRO: 22500, ModelSize.MINI: 26600, ModelSize.SMALL: 31800},\n",
        "        ModelSize.LARGE: {ModelSize.NANO: 26600, ModelSize.MICRO: 23500, ModelSize.MINI: 24300, ModelSize.SMALL: 41800, ModelSize.MEDIUM: 22300},\n",
        "        ModelSize.HUGE: {ModelSize.NANO: 18400, ModelSize.MICRO: 50600, ModelSize.MINI: 21700, ModelSize.SMALL: 20400, ModelSize.MEDIUM: 19600, ModelSize.LARGE: 24900},\n",
        "    },\n",
        "    OthelloRule.NO_FLIPPING: {\n",
        "        ModelSize.MICRO: {ModelSize.NANO: 0},\n",
        "        ModelSize.MINI: {ModelSize.NANO: 0, ModelSize.MICRO: 0},\n",
        "        ModelSize.SMALL: {ModelSize.NANO: 0, ModelSize.MICRO: 10, ModelSize.MINI: 30},\n",
        "        ModelSize.MEDIUM: {ModelSize.NANO: 20, ModelSize.MICRO: 30, ModelSize.MINI: 20, ModelSize.SMALL: 17520},\n",
        "        ModelSize.LARGE: {ModelSize.NANO: 30, ModelSize.MICRO: 30, ModelSize.MINI: 40, ModelSize.SMALL: 14170, ModelSize.MEDIUM: 50800},\n",
        "        ModelSize.HUGE: {ModelSize.NANO: 20, ModelSize.MICRO: 30, ModelSize.MINI: 30, ModelSize.SMALL: 26500, ModelSize.MEDIUM: 41300, ModelSize.LARGE: 50800},\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwmzHhr-PmVk"
      },
      "outputs": [],
      "source": [
        "othello_rule_to_optimization_step_2 = {\n",
        "    OthelloRule.BIAS_CLOCK: {\n",
        "        ModelSize.MICRO: {ModelSize.NANO: 540},\n",
        "        ModelSize.MINI: {ModelSize.NANO: 190, ModelSize.MICRO: 270},\n",
        "        ModelSize.SMALL: {ModelSize.NANO: 30, ModelSize.MICRO: 170, ModelSize.MINI: 220},\n",
        "        ModelSize.MEDIUM: {ModelSize.NANO: 20, ModelSize.MICRO: 420, ModelSize.MINI: 390, ModelSize.SMALL: 1060},\n",
        "        ModelSize.LARGE: {ModelSize.NANO: 30, ModelSize.MICRO: 150, ModelSize.MINI: 210, ModelSize.SMALL: 280, ModelSize.MEDIUM: 380},\n",
        "        ModelSize.HUGE: {ModelSize.NANO: 50, ModelSize.MICRO: 180, ModelSize.MINI: 200, ModelSize.SMALL: 330, ModelSize.MEDIUM: 310, ModelSize.LARGE: 2880},\n",
        "    },\n",
        "    OthelloRule.NEXT_TO_OPPONENT: {\n",
        "        ModelSize.MICRO: {ModelSize.NANO: 0},\n",
        "        ModelSize.MINI: {ModelSize.NANO: 0, ModelSize.MICRO: 0},\n",
        "        ModelSize.SMALL: {ModelSize.NANO: 20, ModelSize.MICRO: 40, ModelSize.MINI: 400},\n",
        "        ModelSize.MEDIUM: {ModelSize.NANO: 90, ModelSize.MICRO: 240, ModelSize.MINI: 370, ModelSize.SMALL: 18750},\n",
        "        ModelSize.LARGE: {ModelSize.NANO: 110, ModelSize.MICRO: 110, ModelSize.MINI: 1850, ModelSize.SMALL: 13450, ModelSize.MEDIUM: 19970},\n",
        "        ModelSize.HUGE: {ModelSize.NANO: 100, ModelSize.MICRO: 180, ModelSize.MINI: 1930, ModelSize.SMALL: 11750, ModelSize.MEDIUM: 50800, ModelSize.LARGE: 50800},\n",
        "    },\n",
        "    OthelloRule.CHESS: {\n",
        "        ModelSize.HUGE: {ModelSize.NANO: 0, ModelSize.MICRO: 0, ModelSize.MINI: 0, ModelSize.SMALL: 0, ModelSize.MEDIUM: 0, ModelSize.LARGE: 0},\n",
        "    },\n",
        "    OthelloRule.UNTRAINED: {\n",
        "        ModelSize.MICRO: {ModelSize.NANO: 17800},\n",
        "        ModelSize.MINI: {ModelSize.NANO: 12040, ModelSize.MICRO: 16700},\n",
        "        ModelSize.SMALL: {ModelSize.NANO: 10680, ModelSize.MICRO: 11970, ModelSize.MINI: 10570},\n",
        "        ModelSize.MEDIUM: {ModelSize.NANO: 6330, ModelSize.MICRO: 7700, ModelSize.MINI: 9400, ModelSize.SMALL: 15800},\n",
        "        ModelSize.LARGE: {ModelSize.NANO: 1800, ModelSize.MICRO: 7620, ModelSize.MINI: 20720, ModelSize.SMALL: 20000, ModelSize.MEDIUM: 18100},\n",
        "        ModelSize.HUGE: {ModelSize.NANO: 180, ModelSize.MICRO: 3300, ModelSize.MINI: 12400, ModelSize.SMALL: 15900, ModelSize.MEDIUM: 15100, ModelSize.LARGE: 12900},\n",
        "    },\n",
        "    OthelloRule.CONSTANT_PARAMETERS: {\n",
        "        ModelSize.MICRO: {ModelSize.NANO: 18800},\n",
        "        ModelSize.MINI: {ModelSize.NANO: 22330, ModelSize.MICRO: 14900},\n",
        "        ModelSize.SMALL: {ModelSize.NANO: 12950, ModelSize.MICRO: 14860, ModelSize.MINI: 0},\n",
        "        ModelSize.MEDIUM: {ModelSize.NANO: 9930, ModelSize.MICRO: 19920, ModelSize.MINI: 22000, ModelSize.SMALL: 19000},\n",
        "        ModelSize.LARGE: {ModelSize.NANO: 12570, ModelSize.MICRO: 13520, ModelSize.MINI: 10680, ModelSize.SMALL: 13680, ModelSize.MEDIUM: 18100},\n",
        "        ModelSize.HUGE: {ModelSize.NANO: 6220, ModelSize.MICRO: 5570, ModelSize.MINI: 6570, ModelSize.SMALL: 16300, ModelSize.MEDIUM: 15100, ModelSize.LARGE: 9300},\n",
        "    },\n",
        "    OthelloRule.NO_FLIPPING: {\n",
        "        ModelSize.MICRO: {ModelSize.NANO: 90},\n",
        "        ModelSize.MINI: {ModelSize.NANO: 0, ModelSize.MICRO: 10},\n",
        "        ModelSize.SMALL: {ModelSize.NANO: 0, ModelSize.MICRO: 20, ModelSize.MINI: 4550},\n",
        "        ModelSize.MEDIUM: {ModelSize.NANO: 30, ModelSize.MICRO: 20, ModelSize.MINI: 1990, ModelSize.SMALL: 14850},\n",
        "        ModelSize.LARGE: {ModelSize.NANO: 30, ModelSize.MICRO: 50, ModelSize.MINI: 0, ModelSize.SMALL: 2810, ModelSize.MEDIUM: 50800},\n",
        "        ModelSize.HUGE: {ModelSize.NANO: 50, ModelSize.MICRO: 40, ModelSize.MINI: 3680, ModelSize.SMALL: 16970, ModelSize.MEDIUM: 50800, ModelSize.LARGE: 50400},\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wrqa5MuQCAq"
      },
      "outputs": [],
      "source": [
        "othello_rule_to_optimization_step_3 = {\n",
        "    OthelloRule.BIAS_CLOCK: {\n",
        "        ModelSize.MICRO: {ModelSize.NANO: 330},\n",
        "        ModelSize.MINI: {ModelSize.NANO: 450, ModelSize.MICRO: 570},\n",
        "        ModelSize.SMALL: {ModelSize.NANO: 30, ModelSize.MICRO: 170, ModelSize.MINI: 260},\n",
        "        ModelSize.MEDIUM: {ModelSize.NANO: 0, ModelSize.MICRO: 0, ModelSize.MINI: 0, ModelSize.SMALL: 0},\n",
        "        ModelSize.LARGE: {ModelSize.NANO: 0, ModelSize.MICRO: 0, ModelSize.MINI: 0, ModelSize.SMALL: 0, ModelSize.MEDIUM: 0},\n",
        "        ModelSize.HUGE: {ModelSize.NANO: 0, ModelSize.MICRO: 0, ModelSize.MINI: 0, ModelSize.SMALL: 0, ModelSize.MEDIUM: 0, ModelSize.LARGE: 0},\n",
        "    },\n",
        "    OthelloRule.NEXT_TO_OPPONENT: {\n",
        "        ModelSize.MICRO: {ModelSize.NANO: 0},\n",
        "        ModelSize.MINI: {ModelSize.NANO: 0, ModelSize.MICRO: 0},\n",
        "        ModelSize.SMALL: {ModelSize.NANO: 0, ModelSize.MICRO: 0, ModelSize.MINI: 0},\n",
        "        ModelSize.MEDIUM: {ModelSize.NANO: 0, ModelSize.MICRO: 0, ModelSize.MINI: 0, ModelSize.SMALL: 0},\n",
        "        ModelSize.LARGE: {ModelSize.NANO: 0, ModelSize.MICRO: 0, ModelSize.MINI: 0, ModelSize.SMALL: 0, ModelSize.MEDIUM: 0},\n",
        "        ModelSize.HUGE: {ModelSize.NANO: 0, ModelSize.MICRO: 0, ModelSize.MINI: 0, ModelSize.SMALL: 0, ModelSize.MEDIUM: 0, ModelSize.LARGE: 0},\n",
        "    },\n",
        "    OthelloRule.CHESS: {\n",
        "        ModelSize.HUGE: {ModelSize.NANO: 0, ModelSize.MICRO: 0, ModelSize.MINI: 0, ModelSize.SMALL: 0, ModelSize.MEDIUM: 0, ModelSize.LARGE: 0},\n",
        "    },\n",
        "    OthelloRule.UNTRAINED: {\n",
        "        ModelSize.MICRO: {ModelSize.NANO: 0},\n",
        "        ModelSize.MINI: {ModelSize.NANO: 0, ModelSize.MICRO: 0},\n",
        "        ModelSize.SMALL: {ModelSize.NANO: 0, ModelSize.MICRO: 0, ModelSize.MINI: 0},\n",
        "        ModelSize.MEDIUM: {ModelSize.NANO: 0, ModelSize.MICRO: 0, ModelSize.MINI: 0, ModelSize.SMALL: 0},\n",
        "        ModelSize.LARGE: {ModelSize.NANO: 0, ModelSize.MICRO: 0, ModelSize.MINI: 0, ModelSize.SMALL: 0, ModelSize.MEDIUM: 0},\n",
        "        ModelSize.HUGE: {ModelSize.NANO: 0, ModelSize.MICRO: 0, ModelSize.MINI: 0, ModelSize.SMALL: 0, ModelSize.MEDIUM: 0, ModelSize.LARGE: 0},\n",
        "    },\n",
        "    OthelloRule.CONSTANT_PARAMETERS: {\n",
        "        ModelSize.MICRO: {ModelSize.NANO: 0},\n",
        "        ModelSize.MINI: {ModelSize.NANO: 0, ModelSize.MICRO: 0},\n",
        "        ModelSize.SMALL: {ModelSize.NANO: 0, ModelSize.MICRO: 0, ModelSize.MINI: 0},\n",
        "        ModelSize.MEDIUM: {ModelSize.NANO: 0, ModelSize.MICRO: 0, ModelSize.MINI: 0, ModelSize.SMALL: 0},\n",
        "        ModelSize.LARGE: {ModelSize.NANO: 0, ModelSize.MICRO: 0, ModelSize.MINI: 0, ModelSize.SMALL: 0, ModelSize.MEDIUM: 0},\n",
        "        ModelSize.HUGE: {ModelSize.NANO: 0, ModelSize.MICRO: 0, ModelSize.MINI: 0, ModelSize.SMALL: 0, ModelSize.MEDIUM: 0, ModelSize.LARGE: 0},\n",
        "    },\n",
        "    OthelloRule.NO_FLIPPING: {\n",
        "        ModelSize.MICRO: {ModelSize.NANO: 0},\n",
        "        ModelSize.MINI: {ModelSize.NANO: 0, ModelSize.MICRO: 0},\n",
        "        ModelSize.SMALL: {ModelSize.NANO: 0, ModelSize.MICRO: 0, ModelSize.MINI: 0},\n",
        "        ModelSize.MEDIUM: {ModelSize.NANO: 0, ModelSize.MICRO: 0, ModelSize.MINI: 0, ModelSize.SMALL: 0},\n",
        "        ModelSize.LARGE: {ModelSize.NANO: 0, ModelSize.MICRO: 0, ModelSize.MINI: 0, ModelSize.SMALL: 0, ModelSize.MEDIUM: 0},\n",
        "        ModelSize.HUGE: {ModelSize.NANO: 0, ModelSize.MICRO: 0, ModelSize.MINI: 0, ModelSize.SMALL: 0, ModelSize.MEDIUM: 0, ModelSize.LARGE: 0},\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhNnFw8ULiIl"
      },
      "outputs": [],
      "source": [
        "RULE_TO_MARKER = {\n",
        "    OthelloRule.CONSTANT_PARAMETERS: 's',\n",
        "    OthelloRule.UNTRAINED: 'D',\n",
        "    OthelloRule.CHESS: '*',\n",
        "    OthelloRule.NO_FLIPPING: 'o',\n",
        "    OthelloRule.NEXT_TO_OPPONENT: 'P',\n",
        "    OthelloRule.BIAS_CLOCK: '^',\n",
        "}\n",
        "THRESHOLD = 0.02\n",
        "\n",
        "def wsg_score_to_alpha_color(wsg_score: float) -> tuple[float, str]:\n",
        "    alpha = min(0.9, 0.5 + abs(wsg_score))\n",
        "    if wsg_score > THRESHOLD:\n",
        "        color = 'green'\n",
        "    elif wsg_score < - THRESHOLD:\n",
        "        color = 'red'\n",
        "    else:\n",
        "        color = 'black'\n",
        "        alpha = 0.7\n",
        "\n",
        "    return alpha, color"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUmIifXOJpRc"
      },
      "outputs": [],
      "source": [
        "#####################################################################\n",
        "# https://www.dmcdougall.co.uk/publication-ready-the-first-time-beautiful-reproducible-plots-with-matplotlib\n",
        "factor = 1.2\n",
        "plt.rcParams.update({\n",
        "    \"font.size\":         14*factor,\n",
        "    \"axes.titlesize\":    16*factor,\n",
        "    \"axes.labelsize\":    14*factor,\n",
        "    \"xtick.labelsize\":   12*factor,\n",
        "    \"ytick.labelsize\":   12*factor,\n",
        "    \"legend.fontsize\":   12*factor,\n",
        "    \"lines.linewidth\":   2.0*factor,\n",
        "    \"lines.markersize\":  8.0*factor,\n",
        "    \"xtick.major.width\": 1.0*factor,\n",
        "    \"ytick.major.width\": 1.0*factor,\n",
        "    \"xtick.major.size\":  6*factor,\n",
        "    \"ytick.major.size\":  6*factor,\n",
        "})\n",
        "#####################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD6EzWo5OmJf"
      },
      "source": [
        "### plot_pretrain_sweep.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abvKPKeyOnH3"
      },
      "outputs": [],
      "source": [
        "def compute_loss_pretrain_models(\n",
        "    data_folder: str,\n",
        "    experiment_folder: str,\n",
        "    project_name_pretrain: str,\n",
        "    othello_rule_to_test: dict[OthelloRule, CharDataset],\n",
        "    index: int,\n",
        "    device: t.device,\n",
        "    final: bool = True,\n",
        "    overwrite: bool = False,\n",
        ") -> dict[str, float]:\n",
        "    weak_rule = get_othello_rule(Goal.WEAK_GOAL)\n",
        "    strong_rule = get_othello_rule(Goal.STRONG_GOAL)\n",
        "\n",
        "    results_filename = f\"pretrain_{weak_rule.value}_{strong_rule.value}_results.pkl\"\n",
        "    results_path = os.path.join(experiment_folder, project_name_pretrain, results_filename)\n",
        "\n",
        "    if not overwrite and os.path.exists(results_path):\n",
        "        print(f\"Loading existing results from {results_path}\")\n",
        "        try:\n",
        "            with open(results_path, 'rb') as f:\n",
        "                results = pickle.load(f)\n",
        "            print(f\"Loaded {len(results)} existing results\")\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading existing results: {e}\")\n",
        "            print(\"Computing new results...\")\n",
        "\n",
        "    weak_test_dataset = othello_rule_to_test[weak_rule]\n",
        "    strong_test_dataset = othello_rule_to_test[strong_rule]\n",
        "    batch_size = 64\n",
        "    n_games = 1000\n",
        "\n",
        "    # Evaluate models\n",
        "    results = []\n",
        "    for model_size in [\n",
        "        ModelSize.NANO,\n",
        "        ModelSize.MICRO,\n",
        "        ModelSize.MINI,\n",
        "        ModelSize.SMALL,\n",
        "        ModelSize.MEDIUM,\n",
        "        ModelSize.LARGE,\n",
        "        ModelSize.HUGE,\n",
        "    ]:\n",
        "        for goal in [Goal.WEAK_GOAL, Goal.STRONG_GOAL]:\n",
        "            othello_rule = get_othello_rule(goal)\n",
        "            model = load_model_pretrained(\n",
        "                project_name_pretrain, model_size, othello_rule, experiment_folder, device, index, final\n",
        "            )\n",
        "            if not model:\n",
        "                print(\"Missing: \", index, model_size, goal)\n",
        "                continue\n",
        "\n",
        "            # Evaluate\n",
        "            n_parameters = count_parameters(model)\n",
        "\n",
        "            res = test_hooked_transformer(model, weak_rule, weak_test_dataset, batch_size, n_games, device)\n",
        "            avg_weak_loss = res['ce_loss']\n",
        "            weak_illegal_move_percentage = res['illegal_move_percentage']\n",
        "            res = test_hooked_transformer(model, strong_rule, strong_test_dataset, batch_size, n_games, device)\n",
        "            avg_strong_loss = res['ce_loss']\n",
        "            strong_illegal_move_percentage = res['illegal_move_percentage']\n",
        "\n",
        "            results.append(\n",
        "                {\n",
        "                    \"goal\": goal,\n",
        "                    \"othello_rule\": othello_rule,\n",
        "                    \"model_size\": model_size,\n",
        "                    \"n_parameters\": n_parameters,\n",
        "                    \"avg_weak_loss\": avg_weak_loss,\n",
        "                    \"weak_illegal_move_percentage\": weak_illegal_move_percentage,\n",
        "                    \"avg_strong_loss\": avg_strong_loss,\n",
        "                    \"strong_illegal_move_percentage\": strong_illegal_move_percentage,\n",
        "                }\n",
        "            )\n",
        "\n",
        "            # Clean memory\n",
        "            model.cpu()\n",
        "            del model\n",
        "            t.cuda.empty_cache()\n",
        "\n",
        "    if len(results) == 0:\n",
        "        print(\"No models found.\")\n",
        "\n",
        "    os.makedirs(os.path.dirname(results_path), exist_ok=True)\n",
        "    try:\n",
        "        with open(results_path, 'wb') as f:\n",
        "            pickle.dump(results, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        print(f\"Results saved to {results_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving results: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def othello_rule_to_minimum_CE_loss(othello_rule: OthelloRule):\n",
        "    if othello_rule in [OthelloRule.STANDARD, OthelloRule.CHESS, OthelloRule.UNTRAINED, OthelloRule.CONSTANT_PARAMETERS, OthelloRule.NO_FLIPPING]:\n",
        "        return 2.086631\n",
        "    elif othello_rule == OthelloRule.BIAS_CLOCK:\n",
        "        return 0.772031\n",
        "    elif othello_rule == OthelloRule.NEXT_TO_OPPONENT:\n",
        "        return 2.190385\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown OthelloRule: {othello_rule}\")\n",
        "\n",
        "\n",
        "def plot_pretrain_sweep(results: dict, save_path: str | None = None):\n",
        "    weak_rule = get_othello_rule(Goal.WEAK_GOAL)\n",
        "    minimal_loss_weak = othello_rule_to_minimum_CE_loss(weak_rule)\n",
        "    strong_rule = get_othello_rule(Goal.STRONG_GOAL)\n",
        "    minimal_loss_strong = othello_rule_to_minimum_CE_loss(strong_rule)\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
        "\n",
        "    def plot_metric(ax, df_subset, y_metric, title, ylabel, ylim, horizontal_line=None):\n",
        "        df_weak_trained = df_subset[df_subset[\"goal\"] == Goal.WEAK_GOAL]\n",
        "        df_strong_trained = df_subset[df_subset[\"goal\"] == Goal.STRONG_GOAL]\n",
        "\n",
        "        sns.lineplot(data=df_weak_trained, x=\"n_parameters\", y=y_metric,\n",
        "                    label=f\"Trained on {weak_rule.value}\", marker=\"s\", linestyle=\"-\", ax=ax)\n",
        "        sns.lineplot(data=df_strong_trained, x=\"n_parameters\", y=y_metric,\n",
        "                    label=f\"Trained on {strong_rule.value}\", marker=\"d\", linestyle=\"-\", ax=ax)\n",
        "\n",
        "        if horizontal_line:\n",
        "            ax.axhline(y=horizontal_line, color=\"gray\", linestyle=\"--\",\n",
        "                      label=f\"Min Achievable\")\n",
        "        ax.set_xscale(\"log\")\n",
        "        ax.set_ylim(ylim)\n",
        "        ax.set_xlabel(\"Number of Parameters (log scale)\")\n",
        "        ax.set_ylabel(ylabel)\n",
        "        ax.set_title(title)\n",
        "        ax.legend()\n",
        "        ax.grid(True, which=\"both\", ls=\"--\")\n",
        "\n",
        "    # Top row: CE Loss\n",
        "    plot_metric(axes[0,0], df, \"avg_weak_loss\",\n",
        "               f\"CE-Loss vs. Params (Evaluated on {weak_rule.value} test set)\",\n",
        "               \"CE-Loss\", (0, 6), minimal_loss_weak)\n",
        "\n",
        "    plot_metric(axes[0,1], df, \"avg_strong_loss\",\n",
        "               f\"CE-Loss vs. Params (Evaluated on {strong_rule.value} test set)\",\n",
        "               \"CE-Loss\", (0, 6), minimal_loss_strong)\n",
        "\n",
        "    # Bottom row: Illegal Move Percentage\n",
        "    plot_metric(axes[1,0], df, \"weak_illegal_move_percentage\",\n",
        "               f\"Illegal Move % vs. Params (Evaluated on {weak_rule.value} test set)\",\n",
        "               \"Illegal Move %\", (0, 100))\n",
        "\n",
        "    plot_metric(axes[1,1], df, \"strong_illegal_move_percentage\",\n",
        "               f\"Illegal Move % vs. Params (Evaluated on {strong_rule.value} test set)\",\n",
        "               \"Illegal Move %\", (0, 100))\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        save_dir = os.path.dirname(save_path)\n",
        "        if save_dir and not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir, exist_ok=True)\n",
        "        save_path = os.path.join(save_path, f\"pretrain_{weak_rule.value}_{strong_rule.value}.png\")\n",
        "        try:\n",
        "            plt.savefig(save_path, bbox_inches=\"tight\", dpi=300)\n",
        "            print(f\"Plot saved to {save_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving plot to {save_path}: {e}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "project_name_pretrain = '0_pretrain_sweep'\n",
        "results = compute_loss_pretrain_models(\n",
        "    data_folder=data_folder,\n",
        "    experiment_folder=experiment_folder,\n",
        "    project_name_pretrain=project_name_pretrain,\n",
        "    othello_rule_to_test=othello_rule_to_test,\n",
        "    index=0,\n",
        "    device=DEVICE,\n",
        "    final=False,\n",
        "    overwrite=False,\n",
        ")\n",
        "\n",
        "save_path = os.path.join(experiment_folder, project_name_pretrain)\n",
        "plot_pretrain_sweep(results, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf5iSO5I-ABA"
      },
      "outputs": [],
      "source": [
        "def plot_rule_performance_on_weak_task(\n",
        "    pretrained_rule_to_result_file: dict[OthelloRule, str],\n",
        "    pretrained_rule_to_result_file_linear_probe: dict[OthelloRule, str],\n",
        "    finetuned_rule_to_result_file: dict[tuple[OthelloRule, OthelloRule], str],\n",
        "    save_path: str | None = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Plots performance on weak task (standard Othello) for each othello_rule.\n",
        "\n",
        "    x-axis: model size (number of parameters, log-scale)\n",
        "    y-axis: CE-loss on weak rule (standard Othello)\n",
        "\n",
        "    One line for each othello_rule.\n",
        "    \"\"\"\n",
        "    # Load the data using the existing function\n",
        "    model_size_to_n_parameters, rule_to_model_size_to_CE_loss, rule_to_model_size_to_LP_accuracy, rule_to_model_size_pair_to_wsg_score = load_wsg_precomputed_data(\n",
        "        pretrained_rule_to_result_file,\n",
        "        pretrained_rule_to_result_file_linear_probe,\n",
        "        finetuned_rule_to_result_file,\n",
        "    )\n",
        "\n",
        "    # Default colors\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f']\n",
        "    rules = list(rule_to_model_size_to_CE_loss.keys())\n",
        "    rule_to_color = {rule: colors[i % len(colors)] for i, rule in enumerate(rules)}\n",
        "\n",
        "    # Create the plot\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "    # Plot line for each rule\n",
        "    for rule, model_size_to_loss in rule_to_model_size_to_CE_loss.items():\n",
        "        if not model_size_to_loss:  # Skip if no data\n",
        "            continue\n",
        "\n",
        "        # Sort by model size for proper line plotting\n",
        "        sorted_items = sorted(model_size_to_loss.items(), key=lambda x: model_size_to_n_parameters.get(x[0], 0))\n",
        "\n",
        "        model_sizes = [item[0] for item in sorted_items]\n",
        "        losses = [item[1] for item in sorted_items]\n",
        "        n_parameters = [model_size_to_n_parameters[size] for size in model_sizes if size in model_size_to_n_parameters]\n",
        "\n",
        "        if len(n_parameters) == len(losses):  # Only plot if we have parameter counts for all model sizes\n",
        "            ax.plot(n_parameters, losses,\n",
        "                   marker='o',\n",
        "                   label=rule.name,\n",
        "                   color=rule_to_color.get(rule, '#1f77b4'),\n",
        "                   linewidth=2,\n",
        "                   markersize=6)\n",
        "        else:\n",
        "            print(f\"Warning: Missing parameter data for some model sizes in rule {rule.name}\")\n",
        "\n",
        "    # Add horizontal line for minimum achievable weak loss\n",
        "    ax.axhline(y=2.086631, color='red', linestyle='--', linewidth=1, label='Min Achievable Weak Loss')\n",
        "\n",
        "    # Set axes properties\n",
        "    ax.set_xscale('log')\n",
        "    ax.set_xlabel('Model size (Number of Parameters, log scale)')\n",
        "    ax.set_ylabel('CE-loss on weak rule (standard Othello)')\n",
        "    ax.set_title('Performance on Weak Task by Othello Rule')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Create legend with mixed fonts (monospace for rules, regular for reference line)\n",
        "    legend = ax.legend(prop={\"size\": 12})\n",
        "    # Set monospace only for rule labels (all except the last one which is the reference line)\n",
        "    for i, text in enumerate(legend.get_texts()[:-1]):  # All except last\n",
        "        text.set_fontfamily(\"monospace\")\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save if path provided\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "save_path = os.path.join(experiment_folder, 'rule_performance_weak_task')\n",
        "plot_rule_performance_on_weak_task(\n",
        "    pretrained_rule_to_result_file=pretrained_rule_to_result_file,\n",
        "    pretrained_rule_to_result_file_linear_probe=pretrained_rule_to_result_file_linear_probe,\n",
        "    finetuned_rule_to_result_file=finetuned_rule_to_result_file,\n",
        "    save_path=save_path,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIgr9l_8BS-p"
      },
      "source": [
        "### finetune_sweep.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVevaqm5BTU3"
      },
      "outputs": [],
      "source": [
        "def finetune_sweep(\n",
        "    project_name_finetune: str,\n",
        "    project_name_pretrain: str,\n",
        "    experiment_folder: str,\n",
        "    strong_model_sizes_filter: list[ModelSize] | None = None,\n",
        "):\n",
        "    weak_rule = get_othello_rule(Goal.WEAK_GOAL)\n",
        "    strong_rule = get_othello_rule(Goal.STRONG_GOAL)\n",
        "\n",
        "    weak_strong_pairs = get_weak_strong_pairs()\n",
        "    for weak_size, strong_size in weak_strong_pairs:\n",
        "        if strong_model_sizes_filter and strong_size not in strong_model_sizes_filter:\n",
        "            continue\n",
        "        print(weak_size, strong_size)\n",
        "\n",
        "        weak_model = load_model_pretrained(\n",
        "            project_name=project_name_pretrain,\n",
        "            model_size=weak_size,\n",
        "            othello_rule=weak_rule,\n",
        "            experiment_folder=experiment_folder,\n",
        "            device=DEVICE,\n",
        "            index=0,\n",
        "            final=False,\n",
        "        )\n",
        "        strong_model = load_model_pretrained(\n",
        "            project_name=project_name_pretrain,\n",
        "            model_size=strong_size,\n",
        "            othello_rule=strong_rule,\n",
        "            experiment_folder=experiment_folder,\n",
        "            device=DEVICE,\n",
        "            index=0,\n",
        "            final=False,\n",
        "        )\n",
        "        if not weak_model or not strong_model:\n",
        "            print(\"Missing: \", weak_size, strong_size)\n",
        "            continue\n",
        "\n",
        "        finetune(weak_model, weak_size, weak_rule,\n",
        "                 strong_model, strong_size, strong_rule,\n",
        "                 weak_finetune_dataset, val_dataset, othello_rule_to_test,\n",
        "                 experiment_folder, project_name_finetune)\n",
        "\n",
        "\n",
        "project_name_finetune = 'finetune_sweep_untrained' # 'finetune_sweep_bias_clock', 'finetune_sweep_no_flipping', 'finetune_sweep_next_to_opponent', 'playground_othello'\n",
        "project_name_pretrain = '0_pretrain_sweep'\n",
        "# finetune_sweep(project_name_finetune, project_name_pretrain, experiment_folder, strong_model_sizes_filter=[ModelSize.NANO, ModelSize.MICRO, ModelSize.MINI, ModelSize.SMALL, ModelSize.MEDIUM])\n",
        "finetune_sweep(project_name_finetune, project_name_pretrain, experiment_folder, strong_model_sizes_filter=[ModelSize.LARGE])\n",
        "# finetune_sweep(project_name_finetune, project_name_pretrain, experiment_folder, strong_model_sizes_filter=[ModelSize.HUGE])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqeD1NyB0KSs"
      },
      "source": [
        "### plot_finetune_sweep.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgzmeJ5PBYVy"
      },
      "outputs": [],
      "source": [
        "def compute_loss_finetune_models(\n",
        "  data_folder: str,\n",
        "  experiment_folder: str,\n",
        "  project_name_pretrain: str,\n",
        "  project_name_finetune: str,\n",
        "  results_path: str,\n",
        "  othello_rule_to_test: dict[OthelloRule, CharDataset],\n",
        "  index: int,\n",
        "  device: t.device,\n",
        "  final: bool = True,\n",
        "  overwrite: bool = False,\n",
        "  strong_rule: OthelloRule | None = None,\n",
        ") -> tuple[dict[ModelSize, int], dict[tuple[ModelSize, ModelSize], tuple[float, float]]]:\n",
        "    \"\"\"\n",
        "    Returns two dict:\n",
        "        model_to_size: model_size -> n_parameters\n",
        "        weak_strong_pair_to_wsg: (weak_model_size, strong_model_size) -> (wsg_pretrained, wsg_finetuned)\n",
        "    \"\"\"\n",
        "    weak_rule = get_othello_rule(Goal.WEAK_GOAL)\n",
        "    if strong_rule is None:\n",
        "        strong_rule = get_othello_rule(Goal.STRONG_GOAL)\n",
        "\n",
        "    # results_prefix = f\"finetune_{weak_rule.value}_{strong_rule.value}_results\"\n",
        "    # results_filename = results_prefix + \".pkl\"\n",
        "    # results_path = os.path.join(experiment_folder, project_name_finetune, results_filename)\n",
        "\n",
        "    if not overwrite and os.path.exists(results_path):\n",
        "        print(f\"Loading existing results from {results_path}\")\n",
        "        try:\n",
        "            with open(results_path, 'rb') as f:\n",
        "                model_to_size, weak_strong_pair_to_wsg = pickle.load(f)\n",
        "            print(f\"Loaded existing results\")\n",
        "            return model_to_size, weak_strong_pair_to_wsg\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading existing results: {e}\")\n",
        "            print(\"Computing new results...\")\n",
        "\n",
        "    weak_test_dataset = othello_rule_to_test[weak_rule]\n",
        "    strong_test_dataset = othello_rule_to_test[strong_rule]\n",
        "    batch_size = 64\n",
        "    n_games = 1000\n",
        "    model_to_size = {}\n",
        "    weak_strong_pair_to_wsg = {}\n",
        "\n",
        "    # Evaluate models\n",
        "    weak_strong_pairs = get_weak_strong_pairs()\n",
        "    for weak_size, strong_size in weak_strong_pairs:\n",
        "        print(weak_size, strong_size)\n",
        "        weak_model_weak_rule = load_model_pretrained(\n",
        "            project_name_pretrain, weak_size, weak_rule, experiment_folder, device, index, final=final\n",
        "        )\n",
        "        strong_model_weak_rule = load_model_pretrained(\n",
        "            project_name_pretrain, strong_size, weak_rule, experiment_folder, device, index, final=final\n",
        "        )\n",
        "        strong_model_strong_rule = load_model_pretrained(\n",
        "            project_name_pretrain, strong_size, strong_rule, experiment_folder, device, index, final=final\n",
        "        )\n",
        "        finetuned_model = load_finetuned_model(\n",
        "            project_name_finetune,\n",
        "            weak_size,\n",
        "            weak_rule,\n",
        "            strong_size,\n",
        "            strong_rule,\n",
        "            experiment_folder,\n",
        "            device,\n",
        "            index,\n",
        "            final=False,\n",
        "        )\n",
        "        if not weak_model_weak_rule or not strong_model_strong_rule or not strong_model_weak_rule or not finetuned_model:\n",
        "            print(\"Missing: \", index, weak_size, strong_size)\n",
        "            continue\n",
        "\n",
        "        if not weak_size in model_to_size:\n",
        "            model_to_size[weak_size] = count_parameters(weak_model_weak_rule)\n",
        "        if not strong_size in model_to_size:\n",
        "            model_to_size[strong_size] = count_parameters(strong_model_weak_rule)\n",
        "\n",
        "        # Evaluate\n",
        "        n_parameters = count_parameters(finetuned_model)\n",
        "        loss_weak_model_weak_rule = test_hooked_transformer(weak_model_weak_rule, weak_rule, weak_test_dataset, batch_size, n_games, device)['ce_loss']\n",
        "        loss_strong_model_weak_rule = test_hooked_transformer(strong_model_weak_rule, weak_rule, weak_test_dataset, batch_size, n_games, device)['ce_loss']\n",
        "        loss_strong_model_strong_rule = test_hooked_transformer(strong_model_strong_rule, weak_rule, weak_test_dataset, batch_size, n_games, device)['ce_loss']\n",
        "        loss_finetuned_model = test_hooked_transformer(finetuned_model, weak_rule, weak_test_dataset, batch_size, n_games, device)['ce_loss']\n",
        "\n",
        "        wsg_pretrained = (loss_weak_model_weak_rule - loss_strong_model_strong_rule) / (loss_weak_model_weak_rule - loss_strong_model_weak_rule)\n",
        "        wsg_finetuned = (loss_weak_model_weak_rule - loss_finetuned_model) / (loss_weak_model_weak_rule - loss_strong_model_weak_rule)\n",
        "        weak_strong_pair_to_wsg[(weak_size, strong_size)] = (wsg_pretrained, wsg_finetuned)\n",
        "\n",
        "    results = (model_to_size, weak_strong_pair_to_wsg)\n",
        "    os.makedirs(os.path.dirname(results_path), exist_ok=True)\n",
        "    try:\n",
        "        with open(results_path, 'wb') as f:\n",
        "            pickle.dump(results, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        print(f\"Results saved to {results_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving results: {e}\")\n",
        "\n",
        "    return model_to_size, weak_strong_pair_to_wsg\n",
        "\n",
        "\n",
        "def plot_finetune_sweep(model_to_size: dict[ModelSize, int],\n",
        "                     weak_strong_pair_to_wsg: dict[tuple[ModelSize, ModelSize], tuple[float, float]],\n",
        "                     image_path: str,\n",
        "                     title: str,\n",
        "                     overwrite: bool = False,\n",
        "                     plot_legend: bool = True\n",
        "                     ):\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
        "\n",
        "    # Get unique weak models and sort them by size for consistent plotting order\n",
        "    weak_models = sorted(set(pair[0] for pair in weak_strong_pair_to_wsg.keys()),\n",
        "                         key=lambda x: model_to_size[x])\n",
        "\n",
        "    # Create a color map to assign a unique color to each weak model series\n",
        "    colors = plt.cm.viridis(np.linspace(0, 1, len(weak_models)))\n",
        "\n",
        "    # Collect all y-values (both pretrain and finetune) to determine a shared y-axis range\n",
        "    all_y_values = []\n",
        "    for (pre, fine) in weak_strong_pair_to_wsg.values():\n",
        "        all_y_values.extend([pre, fine])\n",
        "\n",
        "    # Define the y-axis range with a 10% margin for better visualization\n",
        "    y_min, y_max = min(all_y_values), max(all_y_values)\n",
        "    y_margin = (y_max - y_min) * 0.1\n",
        "    y_range = (y_min - y_margin, y_max + y_margin)\n",
        "\n",
        "    # Iterate through each weak model to plot its corresponding data series\n",
        "    for i, weak_model in enumerate(weak_models):\n",
        "        strong_params = []\n",
        "        wsg_pretrain = []\n",
        "        wsg_finetune = []\n",
        "\n",
        "        # Gather data points corresponding to the current weak model\n",
        "        for (w, s), (pre, fine) in weak_strong_pair_to_wsg.items():\n",
        "            if w == weak_model:\n",
        "                strong_params.append(model_to_size[s])\n",
        "                wsg_pretrain.append(pre)\n",
        "                wsg_finetune.append(fine)\n",
        "\n",
        "        # Sort the data by the strong model's size to ensure lines are drawn correctly\n",
        "        sorted_data = sorted(zip(strong_params, wsg_pretrain, wsg_finetune))\n",
        "        if sorted_data:\n",
        "            x, y_pre, y_fine = zip(*sorted_data)\n",
        "\n",
        "            # Plot finetuned data with a solid line and a label for the weak model\n",
        "            ax.plot(x, y_fine, 's-', color=colors[i], label=f'{weak_model.value}')\n",
        "            # Plot pretrained data with a dotted line (no label to avoid clutter)\n",
        "            ax.plot(x, y_pre, 'o:', color=colors[i], alpha=0.5)\n",
        "\n",
        "    # --- Plot Formatting ---\n",
        "    ax.set_xscale('log')\n",
        "    # Use a symmetric log scale for the y-axis to handle positive and negative values\n",
        "    ax.set_yscale('symlog', linthresh=1.0, linscale=2.0)\n",
        "    ax.set_xlabel('Strong Model Parameters')\n",
        "    ax.set_ylabel('PGR') # Renamed y-axis label\n",
        "    # ax.set_title(title, fontdict={'family': 'monospace'})\n",
        "    ax.set_ylim(y_range)\n",
        "\n",
        "    # Add a horizontal line at y=0 for reference\n",
        "    ax.axhline(y=0, color='red', linewidth=0.8, alpha=1.0)\n",
        "\n",
        "    # Add shaded regions to distinguish the logarithmic parts of the y-axis\n",
        "    ax.axhspan(1, y_range[1], alpha=0.15, color='gray', zorder=0)\n",
        "    ax.axhspan(y_range[0], -1, alpha=0.15, color='gray', zorder=0)\n",
        "\n",
        "    # --- Custom Y-axis Ticks for Symlog Scale ---\n",
        "    linear_ticks = [-0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8]\n",
        "    log_ticks_pos = [2, 5, 10, 20, 50, 100] if y_range[1] > 1 else []\n",
        "    log_ticks_neg = [-2, -5, -10, -20, -50, -100] if y_range[0] < -1 else []\n",
        "\n",
        "    all_ticks = (\n",
        "        [t for t in log_ticks_neg if t >= y_range[0]] +\n",
        "        [-1] + linear_ticks + [1] +\n",
        "        [t for t in log_ticks_pos if t <= y_range[1]]\n",
        "    )\n",
        "    # Format tick labels for clarity\n",
        "    tick_labels = [f'{tick:.1f}' if -1 < tick < 1 else f'{tick}' for tick in all_ticks]\n",
        "    ax.set_yticks(all_ticks)\n",
        "    ax.set_yticklabels(tick_labels)\n",
        "\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # --- Custom Legend ---\n",
        "    if plot_legend:\n",
        "        # Get the handles and labels from the model plots (e.g., for weak model sizes)\n",
        "        handles, labels = ax.get_legend_handles_labels()\n",
        "\n",
        "        # Create custom legend entries for the line styles (Finetuned vs. Pretrained)\n",
        "        linestyle_handles = [\n",
        "            Line2D([0], [0], color='gray', linestyle='-', lw=2, label='Finetuned'),\n",
        "            Line2D([0], [0], color='gray', linestyle=':', lw=2, label='Pretrained')\n",
        "        ]\n",
        "\n",
        "        # Combine handles and draw the legend with a title and custom fonts\n",
        "        ax.legend(\n",
        "            handles=handles + linestyle_handles,\n",
        "            title=\"Weak Model Size\",\n",
        "            # Sets the font for the labels (e.g., 'Weak: 70M')\n",
        "            prop={'family': 'monospace'},\n",
        "            # Sets the font for the title ('Weak Model Size')\n",
        "            title_fontproperties={'family': 'monospace'}\n",
        "        )\n",
        "\n",
        "    # Adjust layout to prevent labels from overlapping\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(bottom=0.15) # Adjust bottom to make space for the text\n",
        "\n",
        "    # Save the plot if a path is provided\n",
        "    if image_path:\n",
        "        # Ensure the directory exists\n",
        "        # os.makedirs(image_path, exist_ok=True)\n",
        "        if overwrite and os.path.exists(image_path):\n",
        "            print(f\"Overwriting existing file: {image_path}\")\n",
        "            os.remove(image_path)\n",
        "        # Save the figure with a tight bounding box to minimize whitespace\n",
        "        print(f\"Saving plot to {image_path}\")\n",
        "        plt.savefig(image_path, bbox_inches=\"tight\", pad_inches=0.05, dpi=300)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "experiment_folder = os.path.join(project_folder, 'experiments/PAPER_SWEEP_2')\n",
        "othello_rule = OthelloRule.CONSTANT_PARAMETERS\n",
        "project_name_pretrain = '0_pretrain_sweep'\n",
        "project_name_finetune = 'finetune_sweep_' + str(othello_rule)[12:].lower()\n",
        "print(project_name_finetune)\n",
        "results_path = finetuned_rule_to_result_file[othello_rule]\n",
        "print(results_path)\n",
        "image_path = results_path[:-4] + \".png\"\n",
        "model_to_size, weak_strong_pair_to_wsg = compute_loss_finetune_models(\n",
        "  data_folder=data_folder,\n",
        "  experiment_folder=experiment_folder,\n",
        "  project_name_pretrain=project_name_pretrain,\n",
        "  project_name_finetune=project_name_finetune,\n",
        "  results_path=results_path,\n",
        "  othello_rule_to_test=othello_rule_to_test,\n",
        "  index=0,\n",
        "  device=DEVICE,\n",
        "  final=False,\n",
        "  overwrite=True,\n",
        "  strong_rule=othello_rule,\n",
        ")\n",
        "plot_finetune_sweep(model_to_size, weak_strong_pair_to_wsg, image_path, str(othello_rule)[12:].lower(), overwrite=True, plot_legend=True)\n",
        "\n",
        "experiment_folder = os.path.join(project_folder, 'experiments/PAPER_SWEEP_3')\n",
        "\n",
        "pretrained_rule_to_result_file = {\n",
        "    OthelloRule.STANDARD: os.path.join(experiment_folder, 'pretrain_sweep/pretrain_standard_next_to_opponent_results.pkl'),\n",
        "    OthelloRule.BIAS_CLOCK: os.path.join(experiment_folder, 'pretrain_sweep/pretrain_standard_bias_clock_results.pkl'),\n",
        "    OthelloRule.NEXT_TO_OPPONENT: os.path.join(experiment_folder, 'pretrain_sweep/pretrain_standard_next_to_opponent_results.pkl'),\n",
        "    OthelloRule.CHESS: os.path.join(experiment_folder, 'pretrain_sweep/pretrain_standard_chess_results.pkl'),\n",
        "    OthelloRule.UNTRAINED: os.path.join(experiment_folder, 'pretrain_sweep/pretrain_standard_untrained_results.pkl'),\n",
        "    OthelloRule.CONSTANT_PARAMETERS: os.path.join(experiment_folder, 'pretrain_sweep/pretrain_standard_constant_parameters_results.pkl'),\n",
        "    OthelloRule.NO_FLIPPING: os.path.join(experiment_folder, 'pretrain_sweep/pretrain_standard_no_flipping_results.pkl'),\n",
        "}\n",
        "pretrained_rule_to_result_file_linear_probe = {\n",
        "    OthelloRule.CHESS: os.path.join(experiment_folder, 'chess_linear_probe/board_prediction_pretrained.pkl'),\n",
        "}\n",
        "pretrained_rule_to_result_file_fake_linear_probe = {\n",
        "    OthelloRule.CHESS: os.path.join(experiment_folder, 'fake_pretrain_sweep_linear_probe/board_prediction_pretrained.pkl'),\n",
        "}\n",
        "pretrained_rule_to_result_file_black_white_linear_probe = {\n",
        "    OthelloRule.CHESS: os.path.join(experiment_folder, 'black_white_pretrain_sweep_linear_probe/board_prediction_pretrained.pkl'),\n",
        "}\n",
        "pretrained_rule_to_result_file_modulo_linear_probe = {\n",
        "    OthelloRule.CHESS: os.path.join(experiment_folder, 'modulo_pretrain_sweep_linear_probe/board_prediction_pretrained.pkl'),\n",
        "}\n",
        "for othello_rule in [OthelloRule.STANDARD, OthelloRule.BIAS_CLOCK, OthelloRule.NEXT_TO_OPPONENT, OthelloRule.UNTRAINED, OthelloRule.CONSTANT_PARAMETERS, OthelloRule.NO_FLIPPING]:\n",
        "    pretrained_rule_to_result_file_linear_probe[othello_rule] = os.path.join(experiment_folder, 'pretrain_sweep_linear_probe/board_prediction_pretrained.pkl')\n",
        "    pretrained_rule_to_result_file_fake_linear_probe[othello_rule] = os.path.join(experiment_folder, 'fake_pretrain_sweep_linear_probe/board_prediction_pretrained.pkl')\n",
        "    pretrained_rule_to_result_file_black_white_linear_probe[othello_rule] = os.path.join(experiment_folder, 'black_white_pretrain_sweep_linear_probe/board_prediction_pretrained.pkl')\n",
        "    pretrained_rule_to_result_file_modulo_linear_probe[othello_rule] = os.path.join(experiment_folder, 'modulo_pretrain_sweep_linear_probe/board_prediction_pretrained.pkl')\n",
        "\n",
        "\n",
        "finetuned_rule_to_result_file = {\n",
        "    OthelloRule.BIAS_CLOCK: os.path.join(experiment_folder, 'finetune_sweep_bias_clock/finetune_standard_bias_clock_results.pkl'),\n",
        "    OthelloRule.NEXT_TO_OPPONENT: os.path.join(experiment_folder, 'finetune_sweep_next_to_opponent/finetune_standard_next_to_opponent_results.pkl'),\n",
        "    OthelloRule.CHESS: os.path.join(experiment_folder, 'chess_1/finetune_standard_chess_results.pkl'),\n",
        "    OthelloRule.UNTRAINED: os.path.join(experiment_folder, 'finetune_sweep_untrained/finetune_standard_untrained_results.pkl'),\n",
        "    OthelloRule.CONSTANT_PARAMETERS: os.path.join(experiment_folder, 'finetune_sweep_constant_parameters/finetune_standard_constant_parameters_results.pkl'),\n",
        "    OthelloRule.NO_FLIPPING: os.path.join(experiment_folder, 'finetune_sweep_no_flipping/finetune_standard_no_flipping_results.pkl'),\n",
        "}\n",
        "finetued_rule_to_result_file_linear_probe = {\n",
        "    OthelloRule.BIAS_CLOCK: os.path.join(experiment_folder, 'finetune_sweep_bias_clock_linear_probe/board_accuracy_finetuned_standard_bias_clock.pkl'),\n",
        "    OthelloRule.NEXT_TO_OPPONENT: os.path.join(experiment_folder, 'finetune_sweep_next_to_opponent_linear_probe/board_accuracy_finetuned_standard_next_to_opponent.pkl'),\n",
        "    OthelloRule.CHESS: os.path.join(experiment_folder, 'chess_linear_probe/board_accuracy_finetuned_standard_chess.pkl'),\n",
        "    OthelloRule.UNTRAINED: os.path.join(experiment_folder, 'finetune_sweep_untrained_linear_probe/board_accuracy_finetuned_standard_untrained.pkl'),\n",
        "    OthelloRule.CONSTANT_PARAMETERS: os.path.join(experiment_folder, 'experiments/finetune_sweep_constant_parameters_linear_probe/board_accuracy_finetuned_standard_constant_parameters.pkl'),\n",
        "    OthelloRule.NO_FLIPPING: os.path.join(experiment_folder, 'finetune_sweep_no_flipping_linear_probe/board_accuracy_finetuned_standard_no_flipping.pkl'),\n",
        "}\n",
        "\n",
        "othello_rule = OthelloRule.CONSTANT_PARAMETERS\n",
        "project_name_pretrain = '0_pretrain_sweep'\n",
        "project_name_finetune = 'finetune_sweep_' + str(othello_rule)[12:].lower()\n",
        "print(project_name_finetune)\n",
        "results_path = finetuned_rule_to_result_file[othello_rule]\n",
        "print(results_path)\n",
        "image_path = results_path[:-4] + \".png\"\n",
        "model_to_size, weak_strong_pair_to_wsg = compute_loss_finetune_models(\n",
        "  data_folder=data_folder,\n",
        "  experiment_folder=experiment_folder,\n",
        "  project_name_pretrain=project_name_pretrain,\n",
        "  project_name_finetune=project_name_finetune,\n",
        "  results_path=results_path,\n",
        "  othello_rule_to_test=othello_rule_to_test,\n",
        "  index=0,\n",
        "  device=DEVICE,\n",
        "  final=False,\n",
        "  overwrite=True,\n",
        "  strong_rule=othello_rule,\n",
        ")\n",
        "plot_finetune_sweep(model_to_size, weak_strong_pair_to_wsg, image_path, str(othello_rule)[12:].lower(), overwrite=True, plot_legend=True)\n",
        "\n",
        "othello_rule = OthelloRule.NO_FLIPPING\n",
        "project_name_pretrain = '0_pretrain_sweep'\n",
        "project_name_finetune = 'finetune_sweep_' + str(othello_rule)[12:].lower()\n",
        "print(project_name_finetune)\n",
        "results_path = finetuned_rule_to_result_file[othello_rule]\n",
        "print(results_path)\n",
        "image_path = results_path[:-4] + \".png\"\n",
        "model_to_size, weak_strong_pair_to_wsg = compute_loss_finetune_models(\n",
        "  data_folder=data_folder,\n",
        "  experiment_folder=experiment_folder,\n",
        "  project_name_pretrain=project_name_pretrain,\n",
        "  project_name_finetune=project_name_finetune,\n",
        "  results_path=results_path,\n",
        "  othello_rule_to_test=othello_rule_to_test,\n",
        "  index=0,\n",
        "  device=DEVICE,\n",
        "  final=False,\n",
        "  overwrite=True,\n",
        "  strong_rule=othello_rule,\n",
        ")\n",
        "plot_finetune_sweep(model_to_size, weak_strong_pair_to_wsg, image_path, str(othello_rule)[12:].lower(), overwrite=True, plot_legend=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKZASgVNweuS"
      },
      "outputs": [],
      "source": [
        "experiment_folder = os.path.join(project_folder, 'experiments/PAPER_SWEEP_3')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9hvwWKsGku4"
      },
      "source": [
        "### linear_probe_sweep.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dI4nJNtIGk-0"
      },
      "outputs": [],
      "source": [
        "def linear_probe_pretrained_sweep(\n",
        "    pretrained_project_name: str,\n",
        "    project_name: str,\n",
        "    experiment_folder: str,\n",
        "    othello_rule: OthelloRule,\n",
        "    model_size_filter: list[ModelSize],\n",
        "    train_board_state_dataset: BoardStateDataset,\n",
        "    val_board_state_dataset: BoardStateDataset,\n",
        "    othello_rule_to_board_state_test_dataset: dict[OthelloRule, list[BoardStateDataset]],\n",
        "    fake_board_state_transform: FakeBoardStateTransform | None = None,\n",
        "):\n",
        "    print(\"fake_board_state_transform: \", fake_board_state_transform)\n",
        "    for model_size in model_size_filter:\n",
        "        print(model_size)\n",
        "        hooked_model = load_model_pretrained(\n",
        "            project_name=pretrained_project_name,\n",
        "            model_size=model_size,\n",
        "            othello_rule=othello_rule,\n",
        "            experiment_folder=experiment_folder,\n",
        "            device=DEVICE,\n",
        "            index=0,\n",
        "            final=False,\n",
        "        )\n",
        "        if hooked_model is None:\n",
        "            print(f\"Failed to load model for {model_size}, skipping...\")\n",
        "            continue\n",
        "        layer = int(hooked_model.cfg.n_layers / 4 * 3)\n",
        "        print(layer)\n",
        "        train_linear_probe_pretrained(hooked_model, layer, project_name, othello_rule, model_size, experiment_folder,\n",
        "                  train_board_state_dataset, val_board_state_dataset, othello_rule_to_board_state_test_dataset, final=False,\n",
        "                  fake_board_state_transform=fake_board_state_transform)\n",
        "\n",
        "\n",
        "linear_probe_pretrained_sweep(\n",
        "    \"0_pretrain_sweep\",\n",
        "    # \"modulo_pretrain_sweep_1_linear_probe\",  # \"pretrain_sweep_1_linear_probe\", \"chess_1_linear_probe\"\n",
        "    \"pretrain_sweep_linear_probe\",\n",
        "    experiment_folder,\n",
        "    OthelloRule.NEXT_TO_OPPONENT,\n",
        "    [ModelSize.NANO, ModelSize.MICRO, ModelSize.MINI, ModelSize.SMALL, ModelSize.MEDIUM, ModelSize.LARGE, ModelSize.HUGE],\n",
        "    train_board_state_dataset,\n",
        "    val_board_state_dataset,\n",
        "    othello_rule_to_board_state_test_dataset,\n",
        "    fake_board_state_transform=None,# fake_board_state_transform,\n",
        ")\n",
        "experiment_folder = os.path.join(project_folder, 'experiments/PAPER_SWEEP_3')\n",
        "linear_probe_pretrained_sweep(\n",
        "    \"0_pretrain_sweep\",\n",
        "    # \"modulo_pretrain_sweep_1_linear_probe\",  # \"pretrain_sweep_1_linear_probe\", \"chess_1_linear_probe\"\n",
        "    \"pretrain_sweep_linear_probe\",\n",
        "    experiment_folder,\n",
        "    OthelloRule.NEXT_TO_OPPONENT,\n",
        "    [ModelSize.NANO, ModelSize.MICRO, ModelSize.MINI, ModelSize.SMALL, ModelSize.MEDIUM, ModelSize.LARGE, ModelSize.HUGE],\n",
        "    train_board_state_dataset,\n",
        "    val_board_state_dataset,\n",
        "    othello_rule_to_board_state_test_dataset,\n",
        "    fake_board_state_transform=None,# fake_board_state_transform,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrl0AgyEGqcC"
      },
      "outputs": [],
      "source": [
        "def linear_probe_finetuned_sweep(\n",
        "    finetuned_project_name: str,\n",
        "    project_name: str,\n",
        "    experiment_folder: str,\n",
        "    notebook_index: int,\n",
        "    n_notebooks: int,\n",
        "    train_board_state_dataset: BoardStateDataset,\n",
        "    val_board_state_dataset: BoardStateDataset,\n",
        "    othello_rule_to_board_state_test_dataset: dict[OthelloRule, list[BoardStateDataset]],\n",
        "    final: bool,\n",
        "):\n",
        "    weak_rule = get_othello_rule(Goal.WEAK_GOAL)\n",
        "    strong_rule = get_othello_rule(Goal.STRONG_GOAL)\n",
        "\n",
        "    weak_strong_pairs = get_weak_strong_pairs()\n",
        "    total_tasks = len(weak_strong_pairs)\n",
        "    base_tasks_per_notebook = total_tasks // n_notebooks\n",
        "    extra_tasks = total_tasks % n_notebooks\n",
        "    if notebook_index < extra_tasks:  # This notebook gets one extra task\n",
        "        start_idx = notebook_index * (base_tasks_per_notebook + 1)\n",
        "        end_idx = start_idx + base_tasks_per_notebook + 1\n",
        "    else:  # This notebook gets base number of tasks\n",
        "        start_idx = extra_tasks * (base_tasks_per_notebook + 1) + (notebook_index - extra_tasks) * base_tasks_per_notebook\n",
        "        end_idx = start_idx + base_tasks_per_notebook\n",
        "\n",
        "    print(f\"Notebook {notebook_index}/{n_notebooks}: Processing tasks {start_idx} to {end_idx-1} (total: {end_idx-start_idx} tasks)\")\n",
        "    print(weak_strong_pairs[start_idx:end_idx])\n",
        "    for weak_model_size, strong_model_size in weak_strong_pairs[start_idx:end_idx]:\n",
        "        print(weak_model_size, strong_model_size)\n",
        "        hooked_model = load_finetuned_model(\n",
        "            project_name=finetuned_project_name,\n",
        "            weak_model_size=weak_model_size,\n",
        "            weak_rule=weak_rule,\n",
        "            strong_model_size=strong_model_size,\n",
        "            strong_rule=strong_rule,\n",
        "            experiment_folder=experiment_folder,\n",
        "            device=DEVICE,\n",
        "            index=0,\n",
        "            final=final,\n",
        "        )\n",
        "        if hooked_model is None:\n",
        "            print(f\"Failed to load finetuned model for {weak_model_size}-{strong_model_size}, skipping...\")\n",
        "            continue\n",
        "        layer = int(hooked_model.cfg.n_layers / 4 * 3)\n",
        "        print(layer)\n",
        "        train_linear_probe_finetuned(hooked_model, layer, project_name, weak_model_size, weak_rule, strong_model_size, strong_rule, experiment_folder,\n",
        "                     train_board_state_dataset, val_board_state_dataset, othello_rule_to_board_state_test_dataset, final)\n",
        "\n",
        "\n",
        "# linear_probe_finetuned_sweep(\n",
        "#     finetuned_project_name=\"finetune_sweep_2_no_flipping\",\n",
        "#     project_name=\"finetune_sweep_2_no_flipping_linear_probe\",\n",
        "#     # project_name=\"playground_othello\",\n",
        "#     experiment_folder=experiment_folder,\n",
        "#     notebook_index=2,\n",
        "#     n_notebooks=7,\n",
        "#     train_board_state_dataset=train_board_state_dataset,\n",
        "#     val_board_state_dataset=val_board_state_dataset,\n",
        "#     othello_rule_to_board_state_test_dataset=othello_rule_to_board_state_test_dataset,\n",
        "#     final=False\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBIyzWRluaH1"
      },
      "outputs": [],
      "source": [
        "!ls '/content/drive/MyDrive/Colab Notebooks/WSG_masterthesis/experiments/finetune_sweep_2_untrained'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orCzD9XX1Ic4"
      },
      "outputs": [],
      "source": [
        "def linear_probe_finetuned_over_time_sweep(\n",
        "    finetuned_project_name: str,\n",
        "    project_name: str,\n",
        "    experiment_folder: str,\n",
        "    weak_rule: OthelloRule,\n",
        "    weak_model_size: ModelSize,\n",
        "    strong_rule: OthelloRule,\n",
        "    strong_model_size: ModelSize,\n",
        "    steps: list[int],\n",
        "    notebook_index: int,\n",
        "    n_notebooks: int,\n",
        "    train_board_state_dataset: BoardStateDataset,\n",
        "    val_board_state_dataset: BoardStateDataset,\n",
        "    othello_rule_to_board_state_test_dataset: dict[OthelloRule, list[BoardStateDataset]],\n",
        "):\n",
        "    total_tasks = len(steps)\n",
        "    base_tasks_per_notebook = total_tasks // n_notebooks\n",
        "    extra_tasks = total_tasks % n_notebooks\n",
        "    if notebook_index < extra_tasks:  # This notebook gets one extra task\n",
        "        start_idx = notebook_index * (base_tasks_per_notebook + 1)\n",
        "        end_idx = start_idx + base_tasks_per_notebook + 1\n",
        "    else:  # This notebook gets base number of tasks\n",
        "        start_idx = extra_tasks * (base_tasks_per_notebook + 1) + (notebook_index - extra_tasks) * base_tasks_per_notebook\n",
        "        end_idx = start_idx + base_tasks_per_notebook\n",
        "\n",
        "    print(f\"Notebook {notebook_index}/{n_notebooks}: Processing tasks {start_idx} to {end_idx-1} (total: {end_idx-start_idx} tasks)\")\n",
        "    print(steps[start_idx:end_idx])\n",
        "    for step_index in steps[start_idx:end_idx]:\n",
        "        hooked_model = load_finetuned_model(\n",
        "            project_name=finetuned_project_name,\n",
        "            weak_model_size=weak_model_size,\n",
        "            weak_rule=weak_rule,\n",
        "            strong_model_size=strong_model_size,\n",
        "            strong_rule=strong_rule,\n",
        "            experiment_folder=experiment_folder,\n",
        "            device=DEVICE,\n",
        "            index=step_index,\n",
        "            final=False,\n",
        "        )\n",
        "        if hooked_model is None:\n",
        "            print(f\"Failed to load finetuned model for step {step_index}, skipping...\")\n",
        "            continue\n",
        "        layer = int(hooked_model.cfg.n_layers / 4 * 3)\n",
        "        print(layer)\n",
        "        train_linear_probe_finetuned(hooked_model, layer, project_name, weak_model_size, weak_rule, strong_model_size, strong_rule, experiment_folder,\n",
        "                     train_board_state_dataset, val_board_state_dataset, othello_rule_to_board_state_test_dataset, False)\n",
        "\n",
        "# steps = [10 * n for n in range(0, 40)] + [400 + 60 * n for n in range(20)]\n",
        "# linear_probe_finetuned_over_time_sweep(\n",
        "#     finetuned_project_name=\"finetune_many_checkpoints\",\n",
        "#     project_name=\"finetune_many_checkpoints_linear_probe\",\n",
        "#     experiment_folder=experiment_folder,\n",
        "#     weak_rule=OthelloRule.STANDARD,\n",
        "#     weak_model_size=ModelSize.MINI,\n",
        "#     strong_rule=OthelloRule.BIAS_CLOCK,\n",
        "#     strong_model_size=ModelSize.HUGE,\n",
        "#     steps,\n",
        "#     notebook_index=0,\n",
        "#     n_notebooks=10,\n",
        "#     train_board_state_dataset=train_board_state_dataset,\n",
        "#     val_board_state_dataset=val_board_state_dataset,\n",
        "#     othello_rule_to_board_state_test_dataset=othello_rule_to_board_state_test_dataset,\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZEfASMX0Gjy"
      },
      "source": [
        "### plot_linear_probe_sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7sTVscj0F-8"
      },
      "outputs": [],
      "source": [
        "def compute_board_prediction_pretrained(\n",
        "    experiment_folder: str,\n",
        "    project_name_linear_probe: str,\n",
        "    project_name_pretrain: str,\n",
        "    othello_rule_to_board_state_test_dataset: dict[OthelloRule, BoardStateDataset],\n",
        "    list_othello_rules: list[OthelloRule],\n",
        "    index: int,\n",
        "    device: t.device,\n",
        "    final: bool = False,\n",
        "    fake_board_state_transform: FakeBoardStateTransform | None = None,\n",
        "    overwrite: bool = False,\n",
        ") -> list[dict]:\n",
        "    weak_rule = get_othello_rule(Goal.WEAK_GOAL)\n",
        "    test_board_state_dataset = othello_rule_to_board_state_test_dataset[weak_rule]\n",
        "\n",
        "    if fake_board_state_transform:\n",
        "        results_filename = f\"board_prediction_pretrained_{fake_board_state_transform.name}.pkl\"\n",
        "    else:\n",
        "        results_filename = \"board_prediction_pretrained_baseline.pkl\"\n",
        "    results_path = os.path.join(experiment_folder, project_name_linear_probe, results_filename)\n",
        "\n",
        "    if not overwrite and os.path.exists(results_path):\n",
        "        print(f\"Loading existing results from {results_path}\")\n",
        "        try:\n",
        "            with open(results_path, 'rb') as f:\n",
        "                results = pickle.load(f)\n",
        "            print(f\"Loaded {len(results)} existing results\")\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading existing results: {e}\")\n",
        "            print(\"Computing new results...\")\n",
        "\n",
        "    results = []\n",
        "    if fake_board_state_transform:\n",
        "        fake_probe = fake_board_state_transform.name\n",
        "    else:\n",
        "        fake_probe = None\n",
        "    print(\"fake_board_state_transform: \", fake_board_state_transform)\n",
        "    print(\"fake_probe: \", fake_probe)\n",
        "\n",
        "    for model_size in [\n",
        "        ModelSize.NANO,\n",
        "        ModelSize.MICRO,\n",
        "        ModelSize.MINI,\n",
        "        ModelSize.SMALL,\n",
        "        ModelSize.MEDIUM,\n",
        "        ModelSize.LARGE,\n",
        "        ModelSize.HUGE,\n",
        "    ]:\n",
        "        for othello_rule in list_othello_rules:\n",
        "\n",
        "            # Load the base model\n",
        "            model = load_model_pretrained(\n",
        "                project_name_pretrain, model_size, othello_rule, experiment_folder, device, index, final=final\n",
        "            )\n",
        "            if not model:\n",
        "                print(f\"Missing model: {index}, {model_size}, {othello_rule}\")\n",
        "                continue\n",
        "\n",
        "            # Load the linear probe\n",
        "            linear_probe = load_model_pretrained(\n",
        "                project_name_linear_probe, model_size, othello_rule, experiment_folder, device, index, final=final, linear_probe=True, fake_probe=fake_probe,\n",
        "            )\n",
        "            if linear_probe is None:\n",
        "                print(f\"Missing linear probe: {index}, {model_size}, {othello_rule}\")\n",
        "                model.cpu()\n",
        "                del model\n",
        "                t.cuda.empty_cache()\n",
        "                continue\n",
        "\n",
        "            # Evaluate probe generalization\n",
        "            layer = int(model.cfg.n_layers / 4 * 3)\n",
        "            _, _, _, mean_accuracy_both_on_all = evaluate_probe_generalization(\n",
        "                probe=linear_probe,\n",
        "                model=model,\n",
        "                board_seqs_square_test=test_board_state_dataset.board_seqs_square,\n",
        "                board_seqs_id_test=test_board_state_dataset.board_seqs_id,\n",
        "                layer=layer,\n",
        "                device=device,\n",
        "                othello_rule=weak_rule,\n",
        "                plot_results=False,\n",
        "                fake_board_state_transform=fake_board_state_transform,\n",
        "            )\n",
        "            print(\"mean_accuracy_both_on_all: \", mean_accuracy_both_on_all)\n",
        "\n",
        "            n_parameters = count_parameters(model)\n",
        "\n",
        "            results.append({\n",
        "                \"othello_rule\": othello_rule,\n",
        "                \"model_size\": model_size,\n",
        "                \"n_parameters\": n_parameters,\n",
        "                \"mean_accuracy\": mean_accuracy_both_on_all.item(),\n",
        "            })\n",
        "\n",
        "            # Clean memory\n",
        "            model.cpu()\n",
        "            linear_probe.cpu()\n",
        "            del model, linear_probe\n",
        "            t.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "    if len(results) == 0:\n",
        "        print(\"No models found.\")\n",
        "\n",
        "    os.makedirs(os.path.dirname(results_path), exist_ok=True)\n",
        "    try:\n",
        "        with open(results_path, 'wb') as f:\n",
        "            pickle.dump(results, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        print(f\"Results saved to {results_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving results: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def plot_board_prediction_pretrained(results: list[dict], save_path: str | None = None, list_othello_rules: list[OthelloRule] = None, fake_board_state_transform: FakeBoardStateTransform | None = None):\n",
        "    if list_othello_rules is None:\n",
        "        print(\"Please provide a list of othello rules to plot\")\n",
        "        return\n",
        "    df = pd.DataFrame(results)\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "    unique_rules = df['othello_rule'].unique()\n",
        "    colors = plt.cm.Set1(np.linspace(0, 1, len(unique_rules)))\n",
        "\n",
        "    # Plot each rule with a different color\n",
        "    for i, rule in enumerate(unique_rules):\n",
        "        rule_data = df[df['othello_rule'] == rule]\n",
        "\n",
        "        sns.lineplot(\n",
        "            data=rule_data,\n",
        "            x=\"n_parameters\",\n",
        "            y=\"mean_accuracy\",\n",
        "            marker=\"o\",\n",
        "            linestyle=\"-\",\n",
        "            ax=ax,\n",
        "            color=colors[i],\n",
        "            label=f\"Trained on {rule.value}\"\n",
        "        )\n",
        "\n",
        "    # Formatting\n",
        "    ax.set_xscale(\"log\")\n",
        "    ax.set_ylim((0, 1))\n",
        "    ax.set_xlabel(\"Number of Parameters (log scale)\")\n",
        "    ax.set_ylabel(\"Mean Board State Prediction Accuracy\")\n",
        "    ax.set_title(\"Board State Prediction Accuracy by Othello Rule\")\n",
        "    ax.grid(True, which=\"both\", ls=\"--\", alpha=0.3)\n",
        "    ax.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        save_dir = os.path.dirname(save_path)\n",
        "        if save_dir and not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir, exist_ok=True)\n",
        "        if fake_board_state_transform:\n",
        "            plot_filename = f\"board_prediction_pretrained_{fake_board_state_transform.name}.png\"\n",
        "        else:\n",
        "            plot_filename = \"board_prediction_pretrained_baseline.png\"\n",
        "        save_path = os.path.join(save_path, plot_filename)\n",
        "        try:\n",
        "            plt.savefig(save_path, bbox_inches=\"tight\", dpi=300)\n",
        "            print(f\"Plot saved to {save_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving plot to {save_path}: {e}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# ## fake_board_state_name = 'placed_before'\n",
        "# # fake_board_state_name = 'black_white'\n",
        "# fake_board_state_name = 'modulo'\n",
        "# fake_board_state_transform = load_fake_board_state_transform(experiment_folder, fake_board_state_name, DEVICE, overwrite=False)\n",
        "# print(fake_board_state_transform)\n",
        "\n",
        "exp_folders = [os.path.join(project_folder, 'experiments/PAPER_SWEEP_2'), os.path.join(project_folder, 'experiments/PAPER_SWEEP_3')]\n",
        "board_state_transforms = [None,\n",
        "                          load_fake_board_state_transform(experiment_folder, 'placed_before', DEVICE, overwrite=False),\n",
        "                          load_fake_board_state_transform(experiment_folder, 'black_white', DEVICE, overwrite=False),\n",
        "                          load_fake_board_state_transform(experiment_folder, 'modulo', DEVICE, overwrite=False)]\n",
        "# for exp_f in exp_folders:\n",
        "for exp_f in [os.path.join(project_folder, 'experiments/PAPER_SWEEP_3')]:\n",
        "    for fake_state in board_state_transforms:\n",
        "    # for fake_state in [None]:\n",
        "        experiment_folder = exp_f\n",
        "        try:\n",
        "            # Example usage:\n",
        "            project_name_pretrain = '0_pretrain_sweep'\n",
        "            project_name_linear_probe = 'pretrain_sweep_linear_probe'\n",
        "            if fake_state:\n",
        "                fake_board_state_transform = fake_state.to(DEVICE)\n",
        "            else:\n",
        "                fake_board_state_transform = None\n",
        "\n",
        "            list_othello_rules = [OthelloRule.STANDARD, OthelloRule.NO_FLIPPING, OthelloRule.CONSTANT_PARAMETERS, OthelloRule.UNTRAINED, OthelloRule.BIAS_CLOCK, OthelloRule.NEXT_TO_OPPONENT, OthelloRule.CHESS]\n",
        "            # list_othello_rules = [OthelloRule.STANDARD]\n",
        "            results = compute_board_prediction_pretrained(\n",
        "                experiment_folder=experiment_folder,\n",
        "                project_name_linear_probe=project_name_linear_probe,\n",
        "                project_name_pretrain=project_name_pretrain,\n",
        "                othello_rule_to_board_state_test_dataset=othello_rule_to_board_state_test_dataset,\n",
        "                list_othello_rules=list_othello_rules,\n",
        "                index=0,\n",
        "                device=DEVICE,\n",
        "                final=False,\n",
        "                fake_board_state_transform=fake_board_state_transform,\n",
        "                overwrite=True,\n",
        "            )\n",
        "\n",
        "            save_path = os.path.join(experiment_folder, project_name_linear_probe)\n",
        "            plot_board_prediction_pretrained(results, save_path, list_othello_rules=list_othello_rules, fake_board_state_transform=fake_board_state_transform)\n",
        "        except:\n",
        "            pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zk42Fvpp9LCl"
      },
      "outputs": [],
      "source": [
        "def compute_board_prediction_finetuned(\n",
        "    experiment_folder: str,\n",
        "    project_name_pretrain: str,\n",
        "    project_name_finetune: str,\n",
        "    project_name_pretrained_linear_probe: str,\n",
        "    project_name_finetuned_linear_probe: str,\n",
        "    othello_rule_to_test: dict[OthelloRule, CharDataset],\n",
        "    index: int,\n",
        "    device: t.device,\n",
        "    overwrite: bool = False,\n",
        ") -> tuple[dict[ModelSize, int], dict[tuple[ModelSize, ModelSize], tuple[float, float, float]]]:\n",
        "    \"\"\"\n",
        "    Returns two dict:\n",
        "        model_to_size: model_size -> n_parameters\n",
        "        weak_strong_pair_to_wsg: (weak_model_size, strong_model_size) -> (wsg_pretrained, wsg_finetuned, wsg_finetuned_final)\n",
        "    \"\"\"\n",
        "    weak_rule = get_othello_rule(Goal.WEAK_GOAL)\n",
        "    strong_rule = get_othello_rule(Goal.STRONG_GOAL)\n",
        "\n",
        "    results_filename = f\"board_prediction_finetuned_{weak_rule.value}_{strong_rule.value}.pkl\"\n",
        "    results_path = os.path.join(experiment_folder, project_name_pretrain, results_filename)\n",
        "\n",
        "    if not overwrite and os.path.exists(results_path):\n",
        "        print(f\"Loading existing results from {results_path}\")\n",
        "        try:\n",
        "            with open(results_path, 'rb') as f:\n",
        "                results = pickle.load(f)\n",
        "            print(f\"Loaded {len(results)} existing results\")\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading existing results: {e}\")\n",
        "            print(\"Computing new results...\")\n",
        "\n",
        "\n",
        "    weak_test_dataset = othello_rule_to_test[weak_rule]\n",
        "    test_board_state_dataset = BoardStateDataset(weak_test_dataset, n_games=1000)\n",
        "\n",
        "    model_to_size = {}\n",
        "    weak_strong_pair_to_wsg = {}\n",
        "\n",
        "    # Cache for accuracies: (model_key) -> accuracy\n",
        "    accuracy_cache = {}\n",
        "\n",
        "    def get_cached_accuracy(model_key, model, probe, layer):\n",
        "        \"\"\"Get accuracy from cache or compute and cache it.\"\"\"\n",
        "        if model_key in accuracy_cache:\n",
        "            print(f\"  Using cached accuracy for {model_key}\")\n",
        "            return accuracy_cache[model_key]\n",
        "\n",
        "        print(f\"Computing accuracy for {model_key}\")\n",
        "        _, _, _, accuracy = evaluate_probe_generalization(\n",
        "            probe, model,\n",
        "            test_board_state_dataset.board_seqs_square,\n",
        "            test_board_state_dataset.board_seqs_id,\n",
        "            layer,\n",
        "            device,\n",
        "            plot_results=False,\n",
        "        )\n",
        "        print(\"accuracy: \", accuracy)\n",
        "        accuracy_cache[model_key] = accuracy\n",
        "        return accuracy\n",
        "\n",
        "    weak_strong_pairs = get_weak_strong_pairs()\n",
        "    for weak_size, strong_size in weak_strong_pairs:\n",
        "        print(\"----------\", weak_size, strong_size, len(weak_strong_pair_to_wsg), \"----------\")\n",
        "\n",
        "        # Load models\n",
        "        weak_model_weak_rule = load_model_pretrained(\n",
        "            project_name_pretrain, weak_size, weak_rule, experiment_folder, device, index, final=False\n",
        "        )\n",
        "        strong_model_weak_rule = load_model_pretrained(\n",
        "            project_name_pretrain, strong_size, weak_rule, experiment_folder, device, index, final=False\n",
        "        )\n",
        "        strong_model_strong_rule = load_model_pretrained(\n",
        "            project_name_pretrain, strong_size, strong_rule, experiment_folder, device, index, final=False\n",
        "        )\n",
        "        finetuned_model = load_finetuned_model(\n",
        "            project_name_finetune, weak_size, weak_rule, strong_size, strong_rule,\n",
        "            experiment_folder, device, index, final=False\n",
        "        )\n",
        "        finetuned_model_final = load_finetuned_model(\n",
        "            project_name_finetune, weak_size, weak_rule, strong_size, strong_rule,\n",
        "            experiment_folder, device, index, final=True\n",
        "        )\n",
        "\n",
        "        # Load linear probes\n",
        "        weak_probe = load_model(\n",
        "            project_name_pretrained_linear_probe, weak_size, weak_rule, experiment_folder, device, index,\n",
        "            final=False, linear_probe=True\n",
        "        )\n",
        "        strong_weak_probe = load_model(\n",
        "            project_name_pretrained_linear_probe, strong_size, weak_rule, experiment_folder, device, index,\n",
        "            final=False, linear_probe=True\n",
        "        )\n",
        "        strong_strong_probe = load_model(\n",
        "            project_name_pretrained_linear_probe, strong_size, strong_rule, experiment_folder, device, index,\n",
        "            final=False, linear_probe=True\n",
        "        )\n",
        "\n",
        "        # Load finetuned linear probes using load_finetuned_model\n",
        "        finetuned_probe = load_finetuned_model(\n",
        "            project_name=project_name_finetuned_linear_probe,\n",
        "            weak_model_size=weak_size,\n",
        "            weak_rule=weak_rule,\n",
        "            strong_model_size=strong_size,\n",
        "            strong_rule=strong_rule,\n",
        "            experiment_folder=experiment_folder,\n",
        "            device=device,\n",
        "            index=index,\n",
        "            final=False,\n",
        "            linear_probe=True\n",
        "        )\n",
        "        finetuned_probe_final = load_finetuned_model(\n",
        "            project_name=project_name_finetuned_linear_probe,\n",
        "            weak_model_size=weak_size,\n",
        "            weak_rule=weak_rule,\n",
        "            strong_model_size=strong_size,\n",
        "            strong_rule=strong_rule,\n",
        "            experiment_folder=experiment_folder,\n",
        "            device=device,\n",
        "            index=index,\n",
        "            final=True,\n",
        "            linear_probe=True\n",
        "        )\n",
        "\n",
        "        # Check if all required models/probes are available\n",
        "        models_and_probes = [weak_model_weak_rule, strong_model_weak_rule, strong_model_strong_rule,\n",
        "                   finetuned_model, finetuned_model_final, weak_probe, strong_weak_probe,\n",
        "                   strong_strong_probe, finetuned_probe, finetuned_probe_final]\n",
        "        if any(item is None for item in models_and_probes):\n",
        "            print(f\"Missing models/probes: {index}, {weak_size}, {strong_size}\")\n",
        "            continue\n",
        "\n",
        "        # Store model sizes\n",
        "        if weak_size not in model_to_size:\n",
        "            model_to_size[weak_size] = count_parameters(weak_model_weak_rule)\n",
        "        if strong_size not in model_to_size:\n",
        "            model_to_size[strong_size] = count_parameters(strong_model_weak_rule)\n",
        "\n",
        "        # Calculate layers\n",
        "        weak_layer = int(weak_model_weak_rule.cfg.n_layers / 4 * 3)\n",
        "        strong_layer = int(strong_model_weak_rule.cfg.n_layers / 4 * 3)\n",
        "        finetuned_layer = int(finetuned_model.cfg.n_layers / 4 * 3)\n",
        "\n",
        "        # Evaluate board prediction accuracies with caching\n",
        "        acc_weak_model = get_cached_accuracy(\n",
        "            f\"pretrain_{weak_size}_{weak_rule}\", weak_model_weak_rule, weak_probe, weak_layer\n",
        "        )\n",
        "        acc_strong_model_weak_rule = get_cached_accuracy(\n",
        "            f\"pretrain_{strong_size}_{weak_rule}\", strong_model_weak_rule, strong_weak_probe, strong_layer\n",
        "        )\n",
        "        acc_strong_model_strong_rule = get_cached_accuracy(\n",
        "            f\"pretrain_{strong_size}_{strong_rule}\", strong_model_strong_rule, strong_strong_probe, strong_layer\n",
        "        )\n",
        "        acc_finetuned_model = get_cached_accuracy(\n",
        "            f\"finetune_{weak_size}_{strong_size}_{weak_rule}_to_{strong_rule}_non_final\",\n",
        "            finetuned_model, finetuned_probe, finetuned_layer\n",
        "        )\n",
        "        acc_finetuned_model_final = get_cached_accuracy(\n",
        "            f\"finetune_{weak_size}_{strong_size}_{weak_rule}_to_{strong_rule}_final\",\n",
        "            finetuned_model_final, finetuned_probe_final, finetuned_layer\n",
        "        )\n",
        "\n",
        "        # Calculate WSG scores (using accuracy instead of loss, so formula is flipped)\n",
        "        wsg_pretrained = (acc_strong_model_strong_rule - acc_weak_model) / (acc_strong_model_weak_rule - acc_weak_model)\n",
        "        wsg_finetuned = (acc_finetuned_model - acc_weak_model) / (acc_strong_model_weak_rule - acc_weak_model)\n",
        "        wsg_finetuned_final = (acc_finetuned_model_final - acc_weak_model) / (acc_strong_model_weak_rule - acc_weak_model)\n",
        "\n",
        "        weak_strong_pair_to_wsg[(weak_size, strong_size)] = (\n",
        "            wsg_pretrained.item(), wsg_finetuned.item(), wsg_finetuned_final.item()\n",
        "        )\n",
        "\n",
        "        # Clean memory\n",
        "        for model in [weak_model_weak_rule, strong_model_weak_rule, strong_model_strong_rule,\n",
        "                     finetuned_model, finetuned_model_final]:\n",
        "            model.cpu()\n",
        "            del model\n",
        "        del weak_probe, strong_weak_probe, strong_strong_probe, finetuned_probe, finetuned_probe_final\n",
        "        t.cuda.empty_cache()\n",
        "\n",
        "    results = (model_to_size, weak_strong_pair_to_wsg)\n",
        "    os.makedirs(os.path.dirname(results_path), exist_ok=True)\n",
        "    try:\n",
        "        with open(results_path, 'wb') as f:\n",
        "            pickle.dump(results, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        print(f\"Results saved to {results_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving results: {e}\")\n",
        "\n",
        "\n",
        "    return model_to_size, weak_strong_pair_to_wsg\n",
        "\n",
        "\n",
        "def plot_board_prediction_finetuned(\n",
        "    model_to_size: dict[ModelSize, int],\n",
        "    weak_strong_pair_to_wsg: dict[tuple[ModelSize, ModelSize], tuple[float, float, float]],\n",
        "    save_path: str | None = None\n",
        "):\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    # Get unique weak models and sort by size\n",
        "    weak_models = sorted(set(pair[0] for pair in weak_strong_pair_to_wsg.keys()),\n",
        "                        key=lambda x: model_to_size[x])\n",
        "\n",
        "    # Create color map based on weak model parameters\n",
        "    colors = plt.cm.viridis(np.linspace(0, 1, len(weak_models)))\n",
        "\n",
        "    # Collect all y-values to determine shared y-axis range\n",
        "    all_y_values = []\n",
        "    for (w, s), (pre, fine, fine_final) in weak_strong_pair_to_wsg.items():\n",
        "        all_y_values.extend([pre, fine, fine_final])\n",
        "\n",
        "    y_min, y_max = min(all_y_values), max(all_y_values)\n",
        "    y_margin = (y_max - y_min) * 0.1\n",
        "    y_range = (y_min - y_margin, y_max + y_margin)\n",
        "\n",
        "    for i, weak_model in enumerate(weak_models):\n",
        "        # Get data for this weak model\n",
        "        strong_params = []\n",
        "        wsg_pretrain = []\n",
        "        wsg_finetune = []\n",
        "        wsg_finetune_final = []\n",
        "\n",
        "        for (w, s), (pre, fine, fine_final) in weak_strong_pair_to_wsg.items():\n",
        "            if w == weak_model:\n",
        "                strong_params.append(model_to_size[s])\n",
        "                wsg_pretrain.append(pre)\n",
        "                wsg_finetune.append(fine)\n",
        "                wsg_finetune_final.append(fine_final)\n",
        "\n",
        "        # Sort by strong model size\n",
        "        sorted_data = sorted(zip(strong_params, wsg_pretrain, wsg_finetune, wsg_finetune_final))\n",
        "        if sorted_data:\n",
        "            x, y_pre, y_fine, y_fine_final = zip(*sorted_data)\n",
        "\n",
        "            ax1.plot(x, y_pre, 'o-', color=colors[i], label=f'{weak_model.value}')\n",
        "            ax2.plot(x, y_fine, 'o-', color=colors[i], label=f'{weak_model.value}')\n",
        "            ax3.plot(x, y_fine_final, 'o-', color=colors[i], label=f'{weak_model.value}')\n",
        "\n",
        "    # Formatting\n",
        "    titles = ['WSG Pretrained', 'WSG Finetuned', 'WSG Finetuned Final']\n",
        "    for ax, title in zip([ax1, ax2, ax3], titles):\n",
        "        ax.set_xscale('log')\n",
        "        ax.set_yscale('symlog', linthresh=1.0, linscale=2.0)\n",
        "        ax.set_xlabel('Strong Model Parameters')\n",
        "        ax.set_ylabel('WSG Score (Board Prediction)')\n",
        "        ax.set_title(title)\n",
        "        ax.set_ylim(y_range)\n",
        "\n",
        "        # Add horizontal line at y=0\n",
        "        ax.axhline(y=0, color='red', linewidth=0.8, alpha=0.8)\n",
        "\n",
        "        # Add shading for logarithmic regions\n",
        "        ax.axhspan(1, y_range[1], alpha=0.15, color='gray', zorder=0)\n",
        "        ax.axhspan(y_range[0], -1, alpha=0.15, color='gray', zorder=0)\n",
        "\n",
        "        # Custom y-ticks\n",
        "        linear_ticks = [-0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8]\n",
        "        log_ticks_pos = [2, 5, 10, 20, 50, 100] if y_range[1] > 1 else []\n",
        "        log_ticks_neg = [-2, -5, -10, -20, -50, -100] if y_range[0] < -1 else []\n",
        "\n",
        "        all_ticks = (\n",
        "            [t for t in log_ticks_neg if t >= y_range[0]] +\n",
        "            [-1] + linear_ticks + [1] +\n",
        "            [t for t in log_ticks_pos if t <= y_range[1]]\n",
        "        )\n",
        "\n",
        "        tick_labels = []\n",
        "        for tick in all_ticks:\n",
        "            if tick == 1:\n",
        "                tick_labels.append('1')\n",
        "            elif tick == -1:\n",
        "                tick_labels.append('-1')\n",
        "            elif -1 < tick < 1:\n",
        "                tick_labels.append(f'{tick:.1f}')\n",
        "            else:\n",
        "                tick_labels.append(f'{tick}')\n",
        "\n",
        "        ax.set_yticks(all_ticks)\n",
        "        ax.set_yticklabels(tick_labels)\n",
        "\n",
        "        # Make linear region ticks smaller\n",
        "        for j, (tick, label) in enumerate(zip(all_ticks, tick_labels)):\n",
        "            if -1 < tick < 1 and tick != 0:\n",
        "                ax.get_yticklabels()[j].set_fontsize(8)\n",
        "\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.legend()\n",
        "\n",
        "    # Add text annotations\n",
        "    fig.text(0.5, -0.05, 'White background: Linear scale [-1,1] | Gray background: Logarithmic scale',\n",
        "              ha='center', fontsize=10, style='italic')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(bottom=0.1)\n",
        "\n",
        "    if save_path:\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "        plt.savefig(os.path.join(save_path, \"wsg_board_prediction.png\"), bbox_inches=\"tight\", dpi=300)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# # Example usage:\n",
        "# project_name_pretrain = 'pretrain_sweep_1'\n",
        "# project_name_finetune = 'finetune_sweep_4_next_to_opponent'\n",
        "# project_name_pretrained_linear_probe = ''pretrain_sweep_1_linear_probe'\n",
        "# project_name_finetuned_linear_probe = 'finetune_sweep_4_next_to_opponent_linear_probe'\n",
        "\n",
        "# model_to_size, weak_strong_pair_to_wsg = compute_board_prediction_finetuned(\n",
        "#     experiment_folder=experiment_folder,\n",
        "#     project_name_pretrain=project_name_pretrain,\n",
        "#     project_name_finetune=project_name_finetune,\n",
        "#     project_name_pretrained_linear_probe=project_name_pretrained_linear_probe,\n",
        "#     project_name_finetuned_linear_probe=project_name_finetuned_linear_probe,\n",
        "#     othello_rule_to_test=othello_rule_to_test,\n",
        "#     index=0,\n",
        "#     device=DEVICE,\n",
        "#     overwrite=False,\n",
        "# )\n",
        "\n",
        "# save_path = os.path.join(experiment_folder, project_name_finetune)\n",
        "# plot_board_prediction_finetuned(model_to_size, weak_strong_pair_to_wsg, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PYl7sb1MkhB"
      },
      "outputs": [],
      "source": [
        "def compute_and_save_board_accuracy_finetuned(\n",
        "    experiment_folder: str,\n",
        "    project_name_finetune: str,\n",
        "    project_name_finetuned_linear_probe: str,\n",
        "    othello_rule_to_board_state_test_dataset: dict[OthelloRule, BoardStateDataset],\n",
        "    index: int,\n",
        "    device: t.device,\n",
        "    overwrite: bool = False,\n",
        ") -> dict[tuple[ModelSize, ModelSize], float]:\n",
        "    \"\"\"\n",
        "    Computes board representation accuracies for finetuning pairs.\n",
        "    Returns:\n",
        "        dict mapping (weak_size, strong_size) -> finetuned_accuracy\n",
        "    \"\"\"\n",
        "    weak_rule = get_othello_rule(Goal.WEAK_GOAL)\n",
        "    strong_rule = get_othello_rule(Goal.STRONG_GOAL)\n",
        "\n",
        "    results_filename = f\"board_accuracy_finetuned_{weak_rule.value}_{strong_rule.value}.pkl\"\n",
        "    results_path = os.path.join(experiment_folder, project_name_finetuned_linear_probe, results_filename)\n",
        "\n",
        "    if not overwrite and os.path.exists(results_path):\n",
        "        print(f\"Loading existing board accuracy results from {results_path}\")\n",
        "        try:\n",
        "            with open(results_path, 'rb') as f:\n",
        "                results = pickle.load(f)\n",
        "            print(f\"Loaded {len(results)} existing board accuracy results\")\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading existing results: {e}\")\n",
        "            print(\"Computing new results...\")\n",
        "\n",
        "    test_board_state_dataset = othello_rule_to_board_state_test_dataset[weak_rule]\n",
        "    accuracy_results = {}\n",
        "\n",
        "    weak_strong_pairs = get_weak_strong_pairs()\n",
        "    for weak_size, strong_size in weak_strong_pairs:\n",
        "        print(\"----------\", weak_size, strong_size, len(accuracy_results), \"----------\")\n",
        "        # Load model\n",
        "        finetuned_model = load_finetuned_model(\n",
        "            project_name_finetune, weak_size, weak_rule, strong_size, strong_rule,\n",
        "            experiment_folder, device, index, final=False\n",
        "        )\n",
        "\n",
        "        # Load probe\n",
        "        finetuned_probe = load_finetuned_model(\n",
        "            project_name=project_name_finetuned_linear_probe,\n",
        "            weak_model_size=weak_size,\n",
        "            weak_rule=weak_rule,\n",
        "            strong_model_size=strong_size,\n",
        "            strong_rule=strong_rule,\n",
        "            experiment_folder=experiment_folder,\n",
        "            device=device,\n",
        "            index=index,\n",
        "            final=False,\n",
        "            linear_probe=True\n",
        "        )\n",
        "\n",
        "        # Check if all required models/probes are available\n",
        "        if finetuned_model is None or finetuned_probe is None:\n",
        "            print(f\"Missing models/probes: {index}, {weak_size}, {strong_size}\")\n",
        "            continue\n",
        "\n",
        "        # Evaluate board prediction accuracies\n",
        "        layer = int(finetuned_model.cfg.n_layers / 4 * 3)\n",
        "        _, _, _, accuracy = evaluate_probe_generalization(\n",
        "            finetuned_probe,\n",
        "            finetuned_model,\n",
        "            test_board_state_dataset.board_seqs_square,\n",
        "            test_board_state_dataset.board_seqs_id,\n",
        "            layer,\n",
        "            device,\n",
        "            weak_rule,\n",
        "            plot_results=False,\n",
        "        )\n",
        "        print(\"accuracy: \", accuracy)\n",
        "        accuracy_results[(weak_size, strong_size)] = accuracy.item()\n",
        "\n",
        "        # Clean memory\n",
        "        finetuned_model.cpu()\n",
        "        finetuned_probe.cpu()\n",
        "        del finetuned_model\n",
        "        del finetuned_probe\n",
        "        t.cuda.empty_cache()\n",
        "\n",
        "    # Save results\n",
        "    os.makedirs(os.path.dirname(results_path), exist_ok=True)\n",
        "    try:\n",
        "        with open(results_path, 'wb') as f:\n",
        "            pickle.dump(accuracy_results, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        print(f\"Board accuracy results saved to {results_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving results: {e}\")\n",
        "\n",
        "    return accuracy_results\n",
        "\n",
        "\n",
        "# # Example usage:\n",
        "# project_name_finetune = 'finetune_sweep_2_no_flipping' # 'finetune_sweep_4_next_to_opponent'\n",
        "# project_name_finetuned_linear_probe = 'finetune_sweep_2_no_flipping_linear_probe' # 'finetune_sweep_4_next_to_opponent_linear_probe'\n",
        "# accuracy_results = compute_and_save_board_accuracy_finetuned(\n",
        "#     experiment_folder=experiment_folder,\n",
        "#     project_name_finetune=project_name_finetune,\n",
        "#     project_name_finetuned_linear_probe=project_name_finetuned_linear_probe,\n",
        "#     othello_rule_to_board_state_test_dataset=othello_rule_to_board_state_test_dataset,\n",
        "#     index=0,\n",
        "#     device=DEVICE,\n",
        "#     overwrite=False,\n",
        "# )\n",
        "# print(accuracy_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxNSQsZQDKSv"
      },
      "outputs": [],
      "source": [
        "def compute_board_prediction_accuracy_over_time(\n",
        "    experiment_folder: str,\n",
        "    finetuned_project_name: str,\n",
        "    project_name_pretrain: str,\n",
        "    linear_probe_project_name: str,\n",
        "    weak_rule: OthelloRule,\n",
        "    weak_model_size: ModelSize,\n",
        "    strong_rule: OthelloRule,\n",
        "    strong_model_size: ModelSize,\n",
        "    steps: list[int],\n",
        "    othello_rule_to_board_state_test_dataset: dict[OthelloRule, BoardStateDataset],\n",
        "    device: t.device,\n",
        "    overwrite: bool = False,\n",
        ") -> list[dict]:\n",
        "    \"\"\"\n",
        "    Compute board prediction accuracies for finetuned models at different training steps.\n",
        "\n",
        "    Returns:\n",
        "        List of dictionaries with keys: 'step', 'accuracy', 'n_parameters'\n",
        "    \"\"\"\n",
        "    results_filename = f\"board_prediction_over_time.pkl\"\n",
        "    results_path = os.path.join(experiment_folder, project_name_pretrain, results_filename)\n",
        "\n",
        "    if not overwrite and os.path.exists(results_path):\n",
        "        print(f\"Loading existing results from {results_path}\")\n",
        "        try:\n",
        "            with open(results_path, 'rb') as f:\n",
        "                results = pickle.load(f)\n",
        "            print(f\"Loaded {len(results)} existing results\")\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading existing results: {e}\")\n",
        "            print(\"Computing new results...\")\n",
        "\n",
        "    weak_test_dataset = othello_rule_to_board_state_test_dataset[weak_rule]\n",
        "    results = []\n",
        "\n",
        "    for step in steps:\n",
        "        print(f\"Processing step {step}...\")\n",
        "\n",
        "        # Load corresponding linear probe\n",
        "        linear_probe = load_finetuned_model(\n",
        "            project_name=linear_probe_project_name,\n",
        "            weak_model_size=weak_model_size,\n",
        "            weak_rule=weak_rule,\n",
        "            strong_model_size=strong_model_size,\n",
        "            strong_rule=strong_rule,\n",
        "            experiment_folder=experiment_folder,\n",
        "            device=device,\n",
        "            index=step,\n",
        "            final=False,\n",
        "            linear_probe=True\n",
        "        )\n",
        "\n",
        "        if linear_probe is None:\n",
        "            print(f\"  Failed to load linear probe for step {step}, skipping...\")\n",
        "            continue\n",
        "\n",
        "        # Load finetuned model at this step\n",
        "        finetuned_model = load_finetuned_model(\n",
        "            project_name=finetuned_project_name,\n",
        "            weak_model_size=weak_model_size,\n",
        "            weak_rule=weak_rule,\n",
        "            strong_model_size=strong_model_size,\n",
        "            strong_rule=strong_rule,\n",
        "            experiment_folder=experiment_folder,\n",
        "            device=device,\n",
        "            index=step,\n",
        "            final=False,\n",
        "        )\n",
        "\n",
        "        if finetuned_model is None:\n",
        "            print(f\"  Failed to load finetuned model for step {step}, skipping...\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Evaluate probe accuracy\n",
        "            layer = int(finetuned_model.cfg.n_layers / 4 * 3)\n",
        "            _, _, _, mean_accuracy_both_on_all = evaluate_probe_generalization(\n",
        "                probe=linear_probe,\n",
        "                model=finetuned_model,\n",
        "                board_seqs_square_test=weak_test_dataset.board_seqs_square,\n",
        "                board_seqs_id_test=weak_test_dataset.board_seqs_id,\n",
        "                layer=layer,\n",
        "                device=device,\n",
        "                plot_results=False,\n",
        "                othello_rule=weak_rule,  # Added missing parameter\n",
        "            )\n",
        "\n",
        "            n_parameters = count_parameters(finetuned_model)\n",
        "\n",
        "            results.append({\n",
        "                \"step\": step,\n",
        "                \"accuracy\": mean_accuracy_both_on_all.item(),\n",
        "                \"n_parameters\": n_parameters,\n",
        "                \"weak_model_size\": weak_model_size,\n",
        "                \"strong_model_size\": strong_model_size,\n",
        "                \"weak_rule\": weak_rule,\n",
        "                \"strong_rule\": strong_rule,\n",
        "            })\n",
        "\n",
        "            print(f\"  Step {step}: Accuracy = {mean_accuracy_both_on_all.item():.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Error evaluating step {step}: {e}\")\n",
        "\n",
        "        finally:\n",
        "            # Clean memory\n",
        "            if 'finetuned_model' in locals():\n",
        "                finetuned_model.cpu()\n",
        "                del finetuned_model\n",
        "            if 'linear_probe' in locals():\n",
        "                del linear_probe\n",
        "            t.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "    print(f\"Completed processing {len(results)}/{len(steps)} steps\")\n",
        "\n",
        "    # Save results (fixed the incorrect assignment)\n",
        "    os.makedirs(os.path.dirname(results_path), exist_ok=True)\n",
        "    try:\n",
        "        with open(results_path, 'wb') as f:\n",
        "            pickle.dump(results, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        print(f\"Results saved to {results_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving results: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def plot_board_prediction_accuracy_over_time(\n",
        "    results: list[dict],\n",
        "    save_path: str | None = None,\n",
        "    title_suffix: str = \"\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Plot board prediction accuracy over training steps.\n",
        "\n",
        "    Args:\n",
        "        results: List of dictionaries from compute_board_prediction_accuracy_over_time\n",
        "        save_path: Optional path to save the plot\n",
        "        title_suffix: Optional suffix to add to plot title\n",
        "    \"\"\"\n",
        "    if not results:\n",
        "        print(\"No results to plot\")\n",
        "        return\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "    # Create the plot\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "\n",
        "    # Plot accuracy over steps\n",
        "    sns.lineplot(data=df, x=\"step\", y=\"accuracy\", marker=\"o\", linestyle=\"-\", ax=ax)\n",
        "\n",
        "    # Formatting\n",
        "    ax.set_xlabel(\"Finetuning step\")\n",
        "    ax.set_ylabel('Linear Probe Accuracy [empty/mine/yours]')\n",
        "    ax.set_ylim((0.7, 1))\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Create title from the data\n",
        "    if results:\n",
        "        sample = results[0]\n",
        "        weak_size = sample['weak_model_size'].value\n",
        "        strong_size = sample['strong_model_size'].value\n",
        "        weak_rule = sample['weak_rule'].value\n",
        "        strong_rule = sample['strong_rule'].value\n",
        "        title = f\"Linear probe accuracy during finetuning\\n{weak_size}→{strong_size}, {weak_rule}→{strong_rule}\"\n",
        "        if title_suffix:\n",
        "            title += f\" {title_suffix}\"\n",
        "        ax.set_title(title)\n",
        "\n",
        "    # Add some statistics as text\n",
        "    # if len(results) > 1:\n",
        "    #     initial_acc = results[0]['accuracy']\n",
        "    #     final_acc = results[-1]['accuracy']\n",
        "    #     max_acc = max(r['accuracy'] for r in results)\n",
        "    #     min_acc = min(r['accuracy'] for r in results)\n",
        "\n",
        "    #     stats_text = f\"Initial: {initial_acc:.3f}\\nFinal: {final_acc:.3f}\\nMax: {max_acc:.3f}\\nMin: {min_acc:.3f}\"\n",
        "    #     ax.text(0.02, 0.98, stats_text, transform=ax.transAxes,\n",
        "    #             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plot\n",
        "    if save_path:\n",
        "        save_dir = os.path.dirname(save_path)\n",
        "        if save_dir and not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir, exist_ok=True)\n",
        "        plot_file_path = os.path.join(save_path, \"board_prediction_accuracy_over_time.png\")\n",
        "        try:\n",
        "            plt.savefig(plot_file_path, bbox_inches=\"tight\", dpi=300)\n",
        "            print(f\"Plot saved to {plot_file_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving plot to {plot_file_path}: {e}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# # Example usage:\n",
        "# steps = [10 * n for n in range(0, 40)] + [400 + 60 * n for n in range(20)]\n",
        "# # steps = [10]\n",
        "\n",
        "# results = compute_board_prediction_accuracy_over_time(\n",
        "#     experiment_folder=experiment_folder,\n",
        "#     finetuned_project_name=\"finetune_many_checkpoints\",\n",
        "#     project_name_pretrain=\"pretrain_1\",\n",
        "#     linear_probe_project_name=\"finetune_many_checkpoints_linear_probe\",\n",
        "#     weak_rule=OthelloRule.STANDARD,\n",
        "#     weak_model_size=ModelSize.MINI,\n",
        "#     strong_rule=OthelloRule.BIAS_CLOCK,\n",
        "#     strong_model_size=ModelSize.HUGE,\n",
        "#     steps=steps,\n",
        "#     othello_rule_to_board_state_test_dataset=othello_rule_to_board_state_test_dataset,\n",
        "#     device=DEVICE,\n",
        "#     overwrite=False,\n",
        "# )\n",
        "\n",
        "# save_path = os.path.join(experiment_folder, \"finetune_many_checkpoints_linear_probe\")\n",
        "# plot_board_prediction_accuracy_over_time(results, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4L5Qx66M_1A"
      },
      "source": [
        "### plot_all_wsg_results.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IB-NYnKVNACR"
      },
      "outputs": [],
      "source": [
        "def load_wsg_precomputed_data(\n",
        "    pretrained_rule_to_result_file: dict[OthelloRule, str],\n",
        "    pretrained_rule_to_result_file_linear_probe: dict[OthelloRule, str],\n",
        "    finetuned_rule_to_result_file: dict[tuple[OthelloRule, OthelloRule], str],\n",
        "):\n",
        "    if pretrained_rule_to_result_file_linear_probe:\n",
        "        assert pretrained_rule_to_result_file.keys() == pretrained_rule_to_result_file_linear_probe.keys()\n",
        "    model_size_to_n_parameters = {}  # Top left\n",
        "    rule_to_model_size_to_CE_loss = {}  # Top right\n",
        "    rule_to_model_size_to_LP_accuracy = {}  # Bottom left\n",
        "    rule_to_model_size_pair_to_wsg_score = {}  # What to plot\n",
        "\n",
        "    def load_pickle(file_path):\n",
        "        try:\n",
        "            with open(file_path, 'rb') as f:\n",
        "                return pickle.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    # model_size_to_n_parameters, rule_to_model_size_to_CE_loss\n",
        "    for rule, path in pretrained_rule_to_result_file.items():\n",
        "        results = load_pickle(path)\n",
        "        if results:\n",
        "            rule_to_model_size_to_CE_loss[rule] = {}\n",
        "            for res in results:\n",
        "                if res['othello_rule'] == rule:\n",
        "                    rule_to_model_size_to_CE_loss[rule][res['model_size']] = res['avg_weak_loss']\n",
        "                    model_size_to_n_parameters[res['model_size']] = res['n_parameters']\n",
        "\n",
        "    # rule_to_model_size_to_LP_accuracy\n",
        "    if pretrained_rule_to_result_file_linear_probe:\n",
        "        for rule, path in pretrained_rule_to_result_file_linear_probe.items():\n",
        "            results = load_pickle(path)\n",
        "            if results:\n",
        "                rule_to_model_size_to_LP_accuracy[rule] = {}\n",
        "                for res in results:\n",
        "                    if res['othello_rule'] == rule:\n",
        "                        rule_to_model_size_to_LP_accuracy[rule][res['model_size']] = res['mean_accuracy']\n",
        "\n",
        "    # rule_to_model_size_pair_to_wsg_score\n",
        "    for rule, path in finetuned_rule_to_result_file.items():\n",
        "        results = load_pickle(path)\n",
        "        if results:\n",
        "            rule_to_model_size_pair_to_wsg_score[rule] = {}\n",
        "            for (weak_model_size, strong_model_size), res in results[1].items():  # First element of tuple is only model sizes again\n",
        "                rule_to_model_size_pair_to_wsg_score[rule][(weak_model_size, strong_model_size)] = res[1]\n",
        "\n",
        "    return model_size_to_n_parameters, rule_to_model_size_to_CE_loss, rule_to_model_size_to_LP_accuracy, rule_to_model_size_pair_to_wsg_score\n",
        "\n",
        "\n",
        "\n",
        "def plot_all_wsg_results(\n",
        "    pretrained_rule_to_result_file: dict[OthelloRule, str],\n",
        "    pretrained_rule_to_result_file_linear_probe: dict[OthelloRule, str],\n",
        "    finetuned_rule_to_result_file: dict[tuple[OthelloRule, OthelloRule], str],\n",
        "    rule_to_marker: dict[OthelloRule, str],\n",
        "    threshold: float,\n",
        "    save_path: str | None = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Plots all WSG results.\n",
        "    Top Left:\n",
        "        x-axis: n_parameters strong model\n",
        "        y-axis: n_parameters weak model\n",
        "    Top Right:\n",
        "        x-axis: strong model CE-loss (on standard rule)\n",
        "        y-axis: weak model CE-loss (on standard rule)\n",
        "    Bottom Left:\n",
        "        x-axis: linear probe accuracy strong model (on standard rule)\n",
        "        y-axis: linear probe accuracy weak model (on standard rule)\n",
        "\n",
        "    Each dot is one pair of (weak_model, strong_model).\n",
        "        color: wsg score (green > 0 and red < 0)\n",
        "        symbol: strong-model rule\n",
        "            - Circle: BIAS_CLOCK\n",
        "            - Square: NEXT_TO_OPPONENT\n",
        "            - Star: CHESS\n",
        "    \"\"\"\n",
        "    model_size_to_n_parameters, rule_to_model_size_to_CE_loss, rule_to_model_size_to_LP_accuracy, rule_to_model_size_pair_to_wsg_score = load_wsg_precomputed_data(\n",
        "        pretrained_rule_to_result_file,\n",
        "        pretrained_rule_to_result_file_linear_probe,\n",
        "        finetuned_rule_to_result_file,\n",
        "    )\n",
        "\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    # Plot data\n",
        "    print(rule_to_model_size_pair_to_wsg_score)\n",
        "    # Plot data\n",
        "    counter_positive_wsg = 0\n",
        "    counter_negative_wsg = 0\n",
        "    for strong_rule, wsg_pairs in rule_to_model_size_pair_to_wsg_score.items():\n",
        "        weak_rule = OthelloRule.STANDARD  # Assuming weak is always STANDARD\n",
        "        marker = rule_to_marker.get(strong_rule, 'o')\n",
        "\n",
        "        for (weak_size, strong_size), wsg_score in wsg_pairs.items():\n",
        "            alpha, color = wsg_score_to_alpha_color(wsg_score)\n",
        "            if wsg_score > 0:\n",
        "                counter_positive_wsg += 1\n",
        "            else:\n",
        "                counter_negative_wsg += 1\n",
        "\n",
        "            # Top Left: n_parameters\n",
        "            if weak_size in model_size_to_n_parameters and strong_size in model_size_to_n_parameters:\n",
        "                ax1.scatter(model_size_to_n_parameters[strong_size],\n",
        "                          model_size_to_n_parameters[weak_size],\n",
        "                          c=color, marker=marker, s=100, alpha=alpha,\n",
        "                          edgecolors='black', linewidth=0.5)\n",
        "\n",
        "            # Top Right: CE loss\n",
        "            if (weak_rule in rule_to_model_size_to_CE_loss and\n",
        "                strong_rule in rule_to_model_size_to_CE_loss and\n",
        "                weak_size in rule_to_model_size_to_CE_loss[weak_rule] and\n",
        "                strong_size in rule_to_model_size_to_CE_loss[strong_rule]):\n",
        "                ax2.scatter(rule_to_model_size_to_CE_loss[strong_rule][strong_size],\n",
        "                          rule_to_model_size_to_CE_loss[weak_rule][weak_size],\n",
        "                          c=color, marker=marker, s=100, alpha=alpha,\n",
        "                          edgecolors='black', linewidth=0.5)\n",
        "\n",
        "            if (weak_rule in rule_to_model_size_to_LP_accuracy and\n",
        "                strong_rule in rule_to_model_size_to_LP_accuracy and\n",
        "                weak_size in rule_to_model_size_to_LP_accuracy[weak_rule] and\n",
        "                strong_size in rule_to_model_size_to_LP_accuracy[strong_rule]):\n",
        "                ax3.scatter(rule_to_model_size_to_LP_accuracy[strong_rule][strong_size],\n",
        "                          rule_to_model_size_to_LP_accuracy[weak_rule][weak_size],\n",
        "                          c=color, marker=marker, s=100, alpha=alpha,\n",
        "                          edgecolors='black', linewidth=0.5)\n",
        "            else:\n",
        "                print(f\"Missing data for {weak_rule}→{strong_rule}, {weak_size}→{strong_size}\")\n",
        "\n",
        "    print(\"Positive WSG score: \", counter_positive_wsg, \" Negative WSG Score: \", counter_negative_wsg)\n",
        "    # Set scales FIRST (before drawing lines)\n",
        "    ax1.set_xscale('log')\n",
        "    ax1.set_yscale('log')\n",
        "\n",
        "    # Add diagonal reference lines (x=y) that extend beyond boundaries\n",
        "    # For ax1 (parameters plot)\n",
        "    ax1_xlim = ax1.get_xlim()\n",
        "    ax1_ylim = ax1.get_ylim()\n",
        "    # Extend beyond the limits\n",
        "    extend_factor = 10  # For log scale\n",
        "    ax1.plot([ax1_xlim[0]/extend_factor, ax1_xlim[1]*extend_factor],\n",
        "            [ax1_xlim[0]/extend_factor, ax1_xlim[1]*extend_factor],\n",
        "            'k--', alpha=0.5, linewidth=1)\n",
        "    ax1.set_xlim(ax1_xlim)  # Restore original limits\n",
        "    ax1.set_ylim(ax1_ylim)\n",
        "\n",
        "    # For ax2 (CE loss plot)\n",
        "    ax2_xlim = ax2.get_xlim()\n",
        "    ax2_ylim = ax2.get_ylim()\n",
        "    # Extend beyond the limits\n",
        "    extend_range = max(ax2_xlim[1] - ax2_xlim[0], ax2_ylim[1] - ax2_ylim[0])\n",
        "    ax2.plot([ax2_xlim[0] - extend_range, ax2_xlim[1] + extend_range],\n",
        "            [ax2_xlim[0] - extend_range, ax2_xlim[1] + extend_range],\n",
        "            'k--', alpha=0.5, linewidth=1)\n",
        "    ax2.set_xlim(ax2_xlim)  # Restore original limits\n",
        "    ax2.set_ylim(ax2_ylim)\n",
        "\n",
        "    # For ax3 (linear probe accuracy plot)\n",
        "    ax3_xlim = ax3.get_xlim()\n",
        "    ax3_ylim = ax3.get_ylim()\n",
        "    # Extend beyond the limits\n",
        "    extend_range = max(ax3_xlim[1] - ax3_xlim[0], ax3_ylim[1] - ax3_ylim[0])\n",
        "    ax3.plot([ax3_xlim[0] - extend_range, ax3_xlim[1] + extend_range],\n",
        "            [ax3_xlim[0] - extend_range, ax3_xlim[1] + extend_range],\n",
        "            'k--', alpha=0.5, linewidth=1)\n",
        "    ax3.set_xlim(ax3_xlim)  # Restore original limits\n",
        "    ax3.set_ylim(ax3_ylim)\n",
        "\n",
        "    # Set labels and titles\n",
        "    ax1.set_xlabel('Strong Model Parameters')\n",
        "    ax1.set_ylabel('Weak Model Parameters')\n",
        "    ax1.set_title('WSG by Model Parameters')\n",
        "    ax1.set_xscale('log')\n",
        "    ax1.set_yscale('log')\n",
        "\n",
        "    ax2.set_xlabel('Strong Model CE Loss')\n",
        "    ax2.set_ylabel('Weak Model CE Loss')\n",
        "    ax2.set_title('WSG by CE Loss')\n",
        "\n",
        "    ax3.set_xlabel('Strong Model Linear Probe Accuracy')\n",
        "    ax3.set_ylabel('Weak Model Linear Probe Accuracy')\n",
        "    ax3.set_title('WSG by Linear Probe Accuracy')\n",
        "\n",
        "    # Create legend\n",
        "    legend_elements = []\n",
        "    for rule, marker in rule_to_marker.items():\n",
        "        legend_elements.append(plt.Line2D([0], [0], marker=marker, color='black',\n",
        "                                        linestyle='None', markersize=10, label=rule.name))\n",
        "    legend_elements.append(plt.Line2D([0], [0], marker='o', color='green',\n",
        "                                    linestyle='None', markersize=10, label=f'WSG > {threshold}'))\n",
        "    legend_elements.append(plt.Line2D([0], [0], marker='o', color='red',\n",
        "                                    linestyle='None', markersize=10, label=f'WSG < -{threshold}'))\n",
        "    legend_elements.append(plt.Line2D([0], [0], marker='o', color='black',\n",
        "                                    linestyle='None', markersize=10, label=f'-{threshold} < WSG < {threshold}'))\n",
        "    legend_elements.append(plt.Line2D([0], [0], color='black', linestyle='--',\n",
        "                                linewidth=1, label='y = x'))\n",
        "\n",
        "    ax4.legend(handles=legend_elements, loc='center')\n",
        "    ax4.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# save_path = os.path.join(experiment_folder, 'wsg_all')\n",
        "# plot_all_wsg_results(\n",
        "#     pretrained_rule_to_result_file=pretrained_rule_to_result_file,\n",
        "#     pretrained_rule_to_result_file_linear_probe=pretrained_rule_to_result_file_linear_probe,\n",
        "#     # pretrained_rule_to_result_file_linear_probe=pretrained_rule_to_result_file_fake_linear_probe,\n",
        "#     # pretrained_rule_to_result_file_linear_probe=pretrained_rule_to_result_file_black_white_linear_probe,\n",
        "#     # pretrained_rule_to_result_file_linear_probe=pretrained_rule_to_result_file_modulo_linear_probe,\n",
        "#     finetuned_rule_to_result_file=finetuned_rule_to_result_file,\n",
        "#     threshold=THRESHOLD,\n",
        "#     rule_to_marker=RULE_TO_MARKER,\n",
        "#     save_path=save_path,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3xtYLFeYiAx"
      },
      "outputs": [],
      "source": [
        "def plot_wsg_vs_accuracy_difference(\n",
        "    pretrained_rule_to_result_file: dict[OthelloRule, str],\n",
        "    pretrained_rule_to_result_file_linear_probe: dict[OthelloRule, str],\n",
        "    finetuned_rule_to_result_file: dict[tuple[OthelloRule, OthelloRule], str],\n",
        "    title: str,\n",
        "    plot_x: str,\n",
        "    save_path: str | None = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Plots WSG score vs the difference in linear probe accuracy (strong - weak).\n",
        "    Calculates the accuracy of predicting the sign of the WSG score.\n",
        "    Uses the same y-axis scaling as plot_wsg_vs_metric.\n",
        "    \"\"\"\n",
        "    rule_to_marker = {\n",
        "        OthelloRule.CONSTANT_PARAMETERS: 's',\n",
        "        OthelloRule.UNTRAINED: 'D',\n",
        "        OthelloRule.CHESS: '*',\n",
        "        OthelloRule.NO_FLIPPING: 'o',\n",
        "        OthelloRule.NEXT_TO_OPPONENT: 'P',\n",
        "        OthelloRule.BIAS_CLOCK: '^',\n",
        "    }\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # Use the same data loading function\n",
        "    model_size_to_n_parameters, rule_to_model_size_to_CE_loss, rule_to_model_size_to_LP_accuracy, rule_to_model_size_pair_to_wsg_score = load_wsg_precomputed_data(\n",
        "        pretrained_rule_to_result_file,\n",
        "        pretrained_rule_to_result_file_linear_probe,\n",
        "        finetuned_rule_to_result_file,\n",
        "    )\n",
        "\n",
        "    # Collect data points for plotting and correlation analysis\n",
        "    all_x_values = []  # accuracy differences\n",
        "    all_y_values = []  # WSG scores\n",
        "    correct_sign_predictions = 0\n",
        "    threshold = 0.02\n",
        "\n",
        "    # Plot data - handle both single rules and tuple keys\n",
        "    for rule_key, wsg_pairs in rule_to_model_size_pair_to_wsg_score.items():\n",
        "        # Check if it's a tuple or single rule\n",
        "        if isinstance(rule_key, tuple):\n",
        "            weak_rule, strong_rule = rule_key\n",
        "        else:\n",
        "            strong_rule = rule_key\n",
        "            weak_rule = OthelloRule.STANDARD\n",
        "\n",
        "        marker = rule_to_marker.get(strong_rule, 'o')\n",
        "\n",
        "        for (weak_size, strong_size), wsg_score in wsg_pairs.items():\n",
        "            # Calculate accuracy difference\n",
        "            x_value = None\n",
        "            data_available = False\n",
        "            if plot_x.startswith('accuracy_difference') or plot_x == 'modulo':\n",
        "                if (weak_rule in rule_to_model_size_to_LP_accuracy and\n",
        "                    strong_rule in rule_to_model_size_to_LP_accuracy and\n",
        "                    weak_size in rule_to_model_size_to_LP_accuracy[weak_rule] and\n",
        "                    strong_size in rule_to_model_size_to_LP_accuracy[strong_rule]):\n",
        "\n",
        "                    weak_acc = rule_to_model_size_to_LP_accuracy[weak_rule][weak_size]\n",
        "                    strong_acc = rule_to_model_size_to_LP_accuracy[strong_rule][strong_size]\n",
        "                    x_value = strong_acc - weak_acc\n",
        "                    data_available = True\n",
        "\n",
        "            elif plot_x == 'ce_loss':\n",
        "                if (weak_rule in rule_to_model_size_to_CE_loss and\n",
        "                    strong_rule in rule_to_model_size_to_CE_loss and\n",
        "                    weak_size in rule_to_model_size_to_CE_loss[weak_rule] and\n",
        "                    strong_size in rule_to_model_size_to_CE_loss[strong_rule]):\n",
        "\n",
        "                    weak_loss = rule_to_model_size_to_CE_loss[weak_rule][weak_size]\n",
        "                    strong_loss = rule_to_model_size_to_CE_loss[strong_rule][strong_size]\n",
        "                    # x > 0 means strong model is better (has lower loss)\n",
        "                    x_value = weak_loss - strong_loss\n",
        "                    data_available = True\n",
        "\n",
        "            elif plot_x == 'n_parameters':\n",
        "                if (weak_size in model_size_to_n_parameters and\n",
        "                    strong_size in model_size_to_n_parameters):\n",
        "\n",
        "                    weak_params = model_size_to_n_parameters[weak_size]\n",
        "                    strong_params = model_size_to_n_parameters[strong_size]\n",
        "                    # As requested: Ratio of parameters\n",
        "                    x_value = np.log(strong_params) - np.log(weak_params)\n",
        "                    data_available = True\n",
        "\n",
        "            if data_available and x_value is not None:\n",
        "                all_x_values.append(x_value)\n",
        "                all_y_values.append(wsg_score)\n",
        "\n",
        "                # Check if the sign prediction is correct\n",
        "                is_positive = (x_value > 0) if plot_x == 'n_parameters' else (x_value > 0)\n",
        "                if (is_positive and wsg_score > 0) or (not is_positive and wsg_score <= 0):\n",
        "                    correct_sign_predictions += 1\n",
        "\n",
        "                # Determine color based on WSG score\n",
        "                alpha = min(0.9, 0.5 + abs(wsg_score))\n",
        "                if wsg_score > threshold:\n",
        "                    color = 'green'\n",
        "                elif wsg_score < -threshold:\n",
        "                    color = 'red'\n",
        "                else:\n",
        "                    color = 'black'\n",
        "                    alpha = 0.7\n",
        "\n",
        "                # Use the generic x_value variable which holds the result for any metric\n",
        "                ax.scatter(x_value, wsg_score, c=color, marker=marker, s=100,\n",
        "                          alpha=alpha, edgecolors='black', linewidth=0.5)\n",
        "            else:\n",
        "                # Debug: print what's missing\n",
        "                if weak_rule not in rule_to_model_size_to_LP_accuracy:\n",
        "                    print(f\"Missing weak rule {weak_rule} in LP data\")\n",
        "                elif strong_rule not in rule_to_model_size_to_LP_accuracy:\n",
        "                    print(f\"Missing strong rule {strong_rule} in LP data\")\n",
        "                elif weak_size not in rule_to_model_size_to_LP_accuracy.get(weak_rule, {}):\n",
        "                    print(f\"Missing weak size {weak_size} for rule {weak_rule}\")\n",
        "                elif strong_size not in rule_to_model_size_to_LP_accuracy.get(strong_rule, {}):\n",
        "                    print(f\"Missing strong size {strong_size} for rule {strong_rule}\")\n",
        "\n",
        "    # Check if we have enough data points\n",
        "    if len(all_x_values) < 2:\n",
        "        print(f\"\\nWarning: Not enough data points ({len(all_x_values)}) to calculate correlations.\")\n",
        "        print(\"Please check that the data files contain matching model sizes and rules.\")\n",
        "        plt.close()\n",
        "        return\n",
        "\n",
        "    # Convert to numpy arrays for correlation analysis\n",
        "    x_arr = np.array(all_x_values)\n",
        "    y_arr = np.array(all_y_values)\n",
        "\n",
        "    # Set up y-axis with symlog scale\n",
        "    y_min, y_max = min(all_y_values), max(all_y_values)\n",
        "    y_margin = (y_max - y_min) * 0.1\n",
        "    y_range = (y_min - y_margin, y_max + y_margin)\n",
        "\n",
        "    ax.set_yscale('symlog', linthresh=1.0, linscale=2.0)\n",
        "    # --- DYNAMIC X-AXIS LABEL ---\n",
        "    x_axis_labels = {\n",
        "        'accuracy_difference': 'Linear Probe Accuracy Difference (Strong - Weak) [empty/mine/yours]',\n",
        "        'accuracy_difference_fake': 'LP Accuracy Difference (Strong - Weak) [empty/filled]',\n",
        "        'accuracy_difference_black_white': 'LP Accuracy Difference (Strong - Weak) [empty/black/white]',\n",
        "        'modulo': 'LP Accuracy Difference (Strong - Weak) [linear*board%3]]',\n",
        "        'ce_loss': 'CE Loss Difference (Weak - Strong)',\n",
        "        'n_parameters': 'Parameter Count Log-Ratio (Log(Strong/Weak))'\n",
        "    }\n",
        "    ax.set_xlabel(x_axis_labels.get(plot_x, 'Metric Difference'))\n",
        "    ax.set_ylabel('PGR')\n",
        "    ax.set_title(title)\n",
        "    ax.set_ylim(y_range)\n",
        "\n",
        "    # Add horizontal line at y=0\n",
        "    ax.axhline(y=0, color='red', linewidth=0.8, alpha=0.8)\n",
        "\n",
        "    # Add vertical line at x=0\n",
        "    ax.axvline(x=0, color='gray', linewidth=0.5, alpha=0.5, linestyle='--')\n",
        "\n",
        "    # Add shading for logarithmic regions\n",
        "    ax.axhspan(1, y_range[1], alpha=0.15, color='gray', zorder=0)\n",
        "    ax.axhspan(y_range[0], -1, alpha=0.15, color='gray', zorder=0)\n",
        "\n",
        "    # Custom y-ticks\n",
        "    linear_ticks = [-0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8]\n",
        "    log_ticks_pos = [2, 5, 10, 20, 50, 100] if y_range[1] > 1 else []\n",
        "    log_ticks_neg = [-2, -5, -10, -20, -50, -100] if y_range[0] < -1 else []\n",
        "\n",
        "    all_ticks = (\n",
        "        [t for t in log_ticks_neg if t >= y_range[0]] +\n",
        "        [-1] + linear_ticks + [1] +\n",
        "        [t for t in log_ticks_pos if t <= y_range[1]]\n",
        "    )\n",
        "\n",
        "    tick_labels = []\n",
        "    for tick in all_ticks:\n",
        "        if tick == 1:\n",
        "            tick_labels.append('1')\n",
        "        elif tick == -1:\n",
        "            tick_labels.append('-1')\n",
        "        elif -1 < tick < 1:\n",
        "            tick_labels.append(f'{tick:.1f}')\n",
        "        else:\n",
        "            tick_labels.append(f'{tick}')\n",
        "\n",
        "    ax.set_yticks(all_ticks)\n",
        "    ax.set_yticklabels(tick_labels)\n",
        "\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Create legend\n",
        "    legend_elements = []\n",
        "\n",
        "    # Add rule markers - extract unique strong rules\n",
        "    unique_strong_rules = set()\n",
        "    for rule_key in rule_to_model_size_pair_to_wsg_score.keys():\n",
        "        if isinstance(rule_key, tuple):\n",
        "            _, strong_rule = rule_key\n",
        "        else:\n",
        "            strong_rule = rule_key\n",
        "        unique_strong_rules.add(strong_rule)\n",
        "\n",
        "    for rule, marker in rule_to_marker.items():\n",
        "        if rule in unique_strong_rules:\n",
        "            legend_elements.append(plt.Line2D([0], [0], marker=marker, color='black',\n",
        "                                              linestyle='None', markersize=10, label=rule.name))\n",
        "\n",
        "    # Add color coding\n",
        "    legend_elements.append(plt.Line2D([0], [0], marker='o', color='green',\n",
        "                                      linestyle='None', markersize=10, label=f'WSG > {threshold}'))\n",
        "    legend_elements.append(plt.Line2D([0], [0], marker='o', color='red',\n",
        "                                      linestyle='None', markersize=10, label=f'WSG < -{threshold}'))\n",
        "    legend_elements.append(plt.Line2D([0], [0], marker='o', color='black',\n",
        "                                      linestyle='None', markersize=10, label=f'-{threshold} < WSG < {threshold}'))\n",
        "\n",
        "    ax.legend(handles=legend_elements)\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(bottom=0.1)\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_x = 'accuracy_difference'\n",
        "if plot_x == 'accuracy_difference':\n",
        "    title_suffix = 'difference of LP accuracy in basis (empty/mine/yours)'\n",
        "    save_path = os.path.join(experiment_folder, 'wsg_vs_accuracy_difference')\n",
        "    pretrained_rule_to_result_file_linear_probe = pretrained_rule_to_result_file_linear_probe\n",
        "elif plot_x == 'accuracy_difference_fake':\n",
        "    title_suffix = 'difference of LP accuracy in basis (empty/filled)'\n",
        "    save_path = os.path.join(experiment_folder, 'wsg_vs_accuracy_difference_fake')\n",
        "    pretrained_rule_to_result_file_linear_probe = pretrained_rule_to_result_file_fake_linear_probe\n",
        "elif plot_x == 'accuracy_difference_black_white':\n",
        "    title_suffix = 'difference of LP accuracy in basis (empty/black/white)'\n",
        "    save_path = os.path.join(experiment_folder, 'wsg_vs_accuracy_difference_black_white')\n",
        "    pretrained_rule_to_result_file_linear_probe=pretrained_rule_to_result_file_black_white_linear_probe\n",
        "elif plot_x == 'modulo':\n",
        "    title_suffix = 'difference of LP accuracy on highly non-linear board (linear*board%3)'\n",
        "    save_path = os.path.join(experiment_folder, 'wsg_vs_accuracy_difference_modulo')\n",
        "    pretrained_rule_to_result_file_linear_probe=pretrained_rule_to_result_file_modulo_linear_probe\n",
        "elif plot_x == 'ce_loss':\n",
        "    title_suffix = 'difference CE-loss difference'\n",
        "    save_path = os.path.join(experiment_folder, 'wsg_vs_ce_loss_difference.png')\n",
        "    # Linear probe data is not needed for this plot\n",
        "    pretrained_rule_to_result_file_linear_probe = None\n",
        "elif plot_x == 'n_parameters':\n",
        "    title_suffix = 'ratio of parameter counts'\n",
        "    save_path = os.path.join(experiment_folder, 'wsg_vs_n_parameters_ratio.png')\n",
        "    # Linear probe data is not needed for this plot\n",
        "    pretrained_rule_to_result_file_linear_probe = None\n",
        "else:\n",
        "    raise Exception('Unknown plot_x')\n",
        "\n",
        "plot_wsg_vs_accuracy_difference(\n",
        "    pretrained_rule_to_result_file=pretrained_rule_to_result_file,\n",
        "    pretrained_rule_to_result_file_linear_probe=pretrained_rule_to_result_file_linear_probe,\n",
        "    finetuned_rule_to_result_file=finetuned_rule_to_result_file,\n",
        "    plot_x=plot_x,\n",
        "    title='PGR vs. ' + title_suffix,\n",
        "    save_path=save_path,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csZz91Ca4l9e"
      },
      "outputs": [],
      "source": [
        "def compute_table(\n",
        "    pretrained_rule_to_result_file: Dict[Any, str],\n",
        "    pretrained_rule_to_result_file_linear_probe: Dict[Any, str],\n",
        "    transform_to_pretrained_rule_to_result_file_linear_probe: Dict[str, Dict[Any, str]],\n",
        "    finetuned_rule_to_result_file: Dict[Any, str],\n",
        "):\n",
        "    \"\"\"\n",
        "    Loads model performance data, computes differences between weak and strong models,\n",
        "    and prints a table of correlation metrics against the weak-to-strong generalization (WSG) score.\n",
        "    \"\"\"\n",
        "    # --- Sections 1 & 2 remain unchanged ---\n",
        "\n",
        "    # --- 1. Data Loading ---\n",
        "    model_size_to_n_parameters = {}\n",
        "    rule_to_model_size_to_CE_loss = {}\n",
        "    rule_to_model_size_to_LP_accuracy = {}\n",
        "    transform_to_rule_to_model_size_to_LP_accuracy = {}\n",
        "    rule_to_model_size_pair_to_wsg_score = {}\n",
        "\n",
        "    def load_pickle(file_path):\n",
        "        try:\n",
        "            with open(file_path, 'rb') as f:\n",
        "                return pickle.load(f)\n",
        "        except Exception as e:\n",
        "            return None\n",
        "\n",
        "    for rule, path in pretrained_rule_to_result_file.items():\n",
        "        results = load_pickle(path)\n",
        "        if results:\n",
        "            rule_to_model_size_to_CE_loss[rule] = {res['model_size']: res['avg_weak_loss'] for res in results if res.get('othello_rule') == rule}\n",
        "            for res in results:\n",
        "                if res.get('othello_rule') == rule:\n",
        "                    model_size_to_n_parameters[res['model_size']] = res['n_parameters']\n",
        "\n",
        "    for rule, path in pretrained_rule_to_result_file_linear_probe.items():\n",
        "        results = load_pickle(path)\n",
        "        if results:\n",
        "            rule_to_model_size_to_LP_accuracy[rule] = {res['model_size']: res['mean_accuracy'] for res in results if res.get('othello_rule') == rule}\n",
        "\n",
        "    for transform_name, rule_to_path in transform_to_pretrained_rule_to_result_file_linear_probe.items():\n",
        "        transform_to_rule_to_model_size_to_LP_accuracy[transform_name] = {}\n",
        "        for rule, path in rule_to_path.items():\n",
        "            results = load_pickle(path)\n",
        "            if results:\n",
        "                transform_to_rule_to_model_size_to_LP_accuracy[transform_name][rule] = {res['model_size']: res['mean_accuracy'] for res in results if res.get('othello_rule') == rule}\n",
        "\n",
        "    for rule, path in finetuned_rule_to_result_file.items():\n",
        "        results = load_pickle(path)\n",
        "        if results:\n",
        "            rule_to_model_size_pair_to_wsg_score[rule] = results[1]\n",
        "\n",
        "    # --- 2. Data Aggregation and Calculation ---\n",
        "    all_data_points = []\n",
        "    for rule_key, wsg_pairs in rule_to_model_size_pair_to_wsg_score.items():\n",
        "        weak_rule, strong_rule = rule_key if isinstance(rule_key, tuple) else (OthelloRule.STANDARD, rule_key)\n",
        "\n",
        "        for (weak_size, strong_size), wsg_result_tuple in wsg_pairs.items():\n",
        "            wsg_score = wsg_result_tuple[1]\n",
        "            point = {'wsg_score': wsg_score}\n",
        "            try:\n",
        "                point[\"N_Params(S) - N_Params(W)\"] = model_size_to_n_parameters[strong_size] - model_size_to_n_parameters[weak_size]\n",
        "                point[\"CE Loss(W) - CE Loss(S)\"] = rule_to_model_size_to_CE_loss[weak_rule][weak_size] - rule_to_model_size_to_CE_loss[strong_rule][strong_size]\n",
        "                if rule_to_model_size_to_LP_accuracy:\n",
        "                    point[\"Empty/Mine/Yours: LP-Acc(S) - LP-Acc(W)\"] = rule_to_model_size_to_LP_accuracy[strong_rule][strong_size] - rule_to_model_size_to_LP_accuracy[weak_rule][weak_size]\n",
        "                for name, data in transform_to_rule_to_model_size_to_LP_accuracy.items():\n",
        "                    key = f\"{name}: LP Acc(S) - LP Acc(W)\"\n",
        "                    point[key] = data[strong_rule][strong_size] - data[weak_rule][weak_size]\n",
        "                all_data_points.append(point)\n",
        "            except KeyError:\n",
        "                continue\n",
        "\n",
        "    if not all_data_points:\n",
        "        print(\"No complete data points found to generate table. Check input files and keys.\")\n",
        "        return\n",
        "\n",
        "    y_wsg_scores = [p['wsg_score'] for p in all_data_points]\n",
        "    x_variables_data = {key: [p[key] for p in all_data_points] for key in all_data_points[0] if key != 'wsg_score'}\n",
        "\n",
        "    def calculate_metrics(x_list: list, y_list: list) -> Tuple[float, float, float]:\n",
        "        if len(x_list) < 2: return (float('nan'), float('nan'), float('nan'))\n",
        "        x_arr, y_arr = np.array(x_list), np.array(y_list)\n",
        "        pearson_r, _ = stats.pearsonr(x_arr, y_arr)\n",
        "        r_squared = pearson_r**2 if not np.isnan(pearson_r) else float('nan')\n",
        "        spearman_rho, p_value = stats.spearmanr(x_arr, y_arr)\n",
        "        correct_signs = np.sum(np.sign(x_arr) == np.sign(y_arr))\n",
        "        sign_accuracy = correct_signs / len(x_arr) if len(x_arr) > 0 else 0\n",
        "        return r_squared, spearman_rho, p_value, sign_accuracy\n",
        "\n",
        "    # --- 3. Print Table ---\n",
        "    # This section is modified to reorder columns and highlight max values.\n",
        "\n",
        "    results = []\n",
        "    for name, x_values in x_variables_data.items():\n",
        "        metrics = calculate_metrics(x_values, y_wsg_scores)\n",
        "        results.append((name, *metrics))\n",
        "\n",
        "    # Sort results by the original Sign Accuracy (index 3) to maintain the requested order\n",
        "    sorted_results = sorted(results, key=lambda item: item[4], reverse=True)\n",
        "\n",
        "    # Find the maximum value in each metric column for bolding\n",
        "    if sorted_results:\n",
        "        # The tuple is (name, r_squared, spearman_rho, sign_accuracy)\n",
        "        max_r2 = max(item[1] for item in sorted_results)\n",
        "        max_rho = max(item[2] for item in sorted_results)\n",
        "        max_acc = max(item[4] for item in sorted_results)\n",
        "    else:\n",
        "        # Set to a very low number if no results exist\n",
        "        max_r2, max_rho, max_acc = float('-inf'), float('-inf'), float('-inf')\n",
        "\n",
        "    # Define column widths for consistent padding\n",
        "    max_name_len = max(len(name) for name, _, _, _, _ in sorted_results) if sorted_results else 35\n",
        "    acc_width = 15\n",
        "    rho_width = 12\n",
        "    pval_width = 12\n",
        "    r2_width = 10\n",
        "    total_width = max_name_len + acc_width + rho_width + pval_width + r2_width + 10\n",
        "\n",
        "    print(f\"\\n📈 Correlation Metrics vs. WSG Score (N={len(y_wsg_scores)}) -- Sorted by Sign Accuracy\")\n",
        "    print(\"-\" * total_width)\n",
        "    # Print the reordered header\n",
        "    print(\n",
        "        f\"{'X-Variable':<{max_name_len}} | \"\n",
        "        f\"{'Sign Accuracy':<{acc_width}} | \"\n",
        "        f\"{'Spearman ρ':<{rho_width}} | \"\n",
        "        f\"{'p-value':<{pval_width}} | \"\n",
        "        f\"{'R²':<{r2_width}}\"\n",
        "    )\n",
        "    print(\"-\" * total_width)\n",
        "\n",
        "    # Loop over the sorted results and print each row with new formatting\n",
        "    for name, r2, rho, p_val, acc in sorted_results:\n",
        "        # Format each value, adding '**' for bolding if it is the maximum\n",
        "        acc_str = f\"**{acc:.3f}**\" if acc == max_acc else f\"{acc:.3f}\"\n",
        "        rho_str = f\"**{rho:.3f}**\" if rho == max_rho else f\"{rho:.3f}\"\n",
        "        r2_str = f\"**{r2:.3f}**\" if r2 == max_r2 else f\"{r2:.3f}\"\n",
        "        pval_str = f\"{p_val:.2e}\"\n",
        "\n",
        "        # Print the reordered, formatted row with left-justified columns\n",
        "        print(\n",
        "            f\"{name:<{max_name_len}} | \"\n",
        "            f\"{acc_str:<{acc_width}} | \"\n",
        "            f\"{rho_str:<{rho_width}} | \"\n",
        "            f\"{pval_str:<{pval_width}} | \"\n",
        "            f\"{r2_str:<{r2_width}}\"\n",
        "        )\n",
        "    print(\"-\" * total_width)\n",
        "\n",
        "\n",
        "transform_to_pretrained_rule_to_result_file_linear_probe = {\n",
        "     \"Empty/Filled\": pretrained_rule_to_result_file_fake_linear_probe,\n",
        "     \"Empty/Black/White\": pretrained_rule_to_result_file_black_white_linear_probe,\n",
        "     \"Linear*board % 3\": pretrained_rule_to_result_file_modulo_linear_probe,\n",
        "    }\n",
        "compute_table(\n",
        "    pretrained_rule_to_result_file,\n",
        "    pretrained_rule_to_result_file_linear_probe,\n",
        "    transform_to_pretrained_rule_to_result_file_linear_probe,\n",
        "    finetuned_rule_to_result_file,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0nMvQJLJzf8"
      },
      "source": [
        "### plot_wsg_vs_metric.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMhY044C5hns"
      },
      "outputs": [],
      "source": [
        "def plot_wsg_vs_metric(\n",
        "    othello_rule_to_metric: dict,\n",
        "    finetuned_rule_to_result_file: dict,\n",
        "    x_axis_name: str,\n",
        "    title: str,\n",
        "    save_path: str | None = None,\n",
        "    x_axis_log: bool = False\n",
        "):\n",
        "    \"\"\"\n",
        "    Plots WSG score vs metric for different Othello rules.\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import pickle\n",
        "    import numpy as np\n",
        "\n",
        "    rule_to_marker = {\n",
        "        OthelloRule.CONSTANT_PARAMETERS: 's',\n",
        "        OthelloRule.UNTRAINED: 'D',\n",
        "        OthelloRule.CHESS: '*',\n",
        "        OthelloRule.NO_FLIPPING: 'o',\n",
        "        OthelloRule.NEXT_TO_OPPONENT: 'P',\n",
        "        OthelloRule.BIAS_CLOCK: '^',\n",
        "    }\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # Load WSG data\n",
        "    def load_pickle(file_path):\n",
        "        try:\n",
        "            with open(file_path, 'rb') as f:\n",
        "                return pickle.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    rule_to_model_size_pair_to_wsg_score = {}\n",
        "    for rule, path in finetuned_rule_to_result_file.items():\n",
        "        results = load_pickle(path)\n",
        "        if results:\n",
        "            rule_to_model_size_pair_to_wsg_score[rule] = {}\n",
        "            for (weak_model_size, strong_model_size), res in results[1].items():\n",
        "                rule_to_model_size_pair_to_wsg_score[rule][(weak_model_size, strong_model_size)] = res[1]\n",
        "\n",
        "    # Collect all y-values to determine shared y-axis range\n",
        "    all_y_values = []\n",
        "    for rule_data in rule_to_model_size_pair_to_wsg_score.values():\n",
        "        all_y_values.extend(rule_data.values())\n",
        "\n",
        "    y_min, y_max = min(all_y_values), max(all_y_values)\n",
        "    y_margin = (y_max - y_min) * 0.1\n",
        "    y_range = (y_min - y_margin, y_max + y_margin)\n",
        "\n",
        "    # Plot data for each rule\n",
        "    threshold = 0.02\n",
        "    for rule, ms_to_ms_to_metric in othello_rule_to_metric.items():\n",
        "        if rule not in rule_to_model_size_pair_to_wsg_score:\n",
        "            continue\n",
        "\n",
        "        marker = rule_to_marker.get(rule, 'o')\n",
        "\n",
        "        x_values = []\n",
        "        y_values = []\n",
        "        for (weak_size, strong_size), wsg_data in rule_to_model_size_pair_to_wsg_score[rule].items():\n",
        "            if (strong_size in ms_to_ms_to_metric and\n",
        "                weak_size in ms_to_ms_to_metric[strong_size]):\n",
        "\n",
        "                metric_value = ms_to_ms_to_metric[strong_size][weak_size]\n",
        "                # Add small offset for zero values to handle log scale\n",
        "                if metric_value == 0:\n",
        "                    metric_value = 0.1\n",
        "                x_values.append(metric_value)\n",
        "                y_values.append(wsg_data)\n",
        "\n",
        "                alpha = min(0.9, 0.5 + abs(wsg_data))\n",
        "                if wsg_data > threshold:\n",
        "                    color = 'green'\n",
        "                elif wsg_data < - threshold:\n",
        "                    color = 'red'\n",
        "                else:\n",
        "                    color = 'black'\n",
        "                    alpha = 0.7\n",
        "\n",
        "                ax.scatter(metric_value, wsg_data, marker=marker, s=100,\n",
        "                          c=color, alpha=alpha, edgecolors='black', linewidth=0.5)\n",
        "\n",
        "        # Add legend entry (single point to avoid duplicates)\n",
        "        ax.scatter([], [], marker=marker, s=100, c='black', alpha=0.7,\n",
        "                  edgecolors='black', linewidth=0.5, label=rule.name)\n",
        "\n",
        "\n",
        "    # Apply same formatting as plot_finetune_sweep\n",
        "    if x_axis_log:\n",
        "        ax.set_xscale('log')\n",
        "    ax.set_yscale('symlog', linthresh=1.0, linscale=2.0)\n",
        "    ax.set_xlabel(x_axis_name)\n",
        "    ax.set_ylabel('PGR')\n",
        "    ax.set_title(title)\n",
        "    ax.set_ylim(y_range)\n",
        "\n",
        "    # Add horizontal line at y=0\n",
        "    ax.axhline(y=0, color='red', linewidth=0.8, alpha=0.8)\n",
        "\n",
        "    # Add shading for logarithmic regions\n",
        "    ax.axhspan(1, y_range[1], alpha=0.15, color='gray', zorder=0)\n",
        "    ax.axhspan(y_range[0], -1, alpha=0.15, color='gray', zorder=0)\n",
        "\n",
        "    # Custom y-ticks\n",
        "    linear_ticks = [-0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8]\n",
        "    log_ticks_pos = [2, 5, 10, 20, 50, 100] if y_range[1] > 1 else []\n",
        "    log_ticks_neg = [-2, -5, -10, -20, -50, -100] if y_range[0] < -1 else []\n",
        "\n",
        "    all_ticks = (\n",
        "        [t for t in log_ticks_neg if t >= y_range[0]] +\n",
        "        [-1] + linear_ticks + [1] +\n",
        "        [t for t in log_ticks_pos if t <= y_range[1]]\n",
        "    )\n",
        "\n",
        "    tick_labels = []\n",
        "    for tick in all_ticks:\n",
        "        if tick == 1:\n",
        "            tick_labels.append('1')\n",
        "        elif tick == -1:\n",
        "            tick_labels.append('-1')\n",
        "        elif -1 < tick < 1:\n",
        "            tick_labels.append(f'{tick:.1f}')\n",
        "        else:\n",
        "            tick_labels.append(f'{tick}')\n",
        "\n",
        "    ax.set_yticks(all_ticks)\n",
        "    ax.set_yticklabels(tick_labels)\n",
        "\n",
        "    # Make linear region ticks smaller\n",
        "    #for i, (tick, label) in enumerate(zip(all_ticks, tick_labels)):\n",
        "    #    if -1 < tick < 1 and tick != 0:\n",
        "    #        ax.get_yticklabels()[i].set_fontsize(8)\n",
        "\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Create combined legend with markers and colors\n",
        "    legend_elements = []\n",
        "\n",
        "    # Add rule markers\n",
        "    for rule, marker in rule_to_marker.items():\n",
        "        if rule in othello_rule_to_metric:\n",
        "            legend_elements.append(plt.Line2D([0], [0], marker=marker, color='black',\n",
        "                                            linestyle='None', markersize=10, label=rule.name))\n",
        "\n",
        "    # Add color coding\n",
        "    legend_elements.append(plt.Line2D([0], [0], marker='o', color='green',\n",
        "                                    linestyle='None', markersize=10, label=f'PGR > {threshold}'))\n",
        "    legend_elements.append(plt.Line2D([0], [0], marker='o', color='red',\n",
        "                                    linestyle='None', markersize=10, label=f'PGR < -{threshold}'))\n",
        "    legend_elements.append(plt.Line2D([0], [0], marker='o', color='black',\n",
        "                                    linestyle='None', markersize=10, label=f'-{threshold} < PGR < {threshold}'))\n",
        "\n",
        "    ax.legend(handles=legend_elements,  loc='lower left')\n",
        "\n",
        "    # Add text annotations\n",
        "    # fig.text(0.5, -0.05, 'White background: Linear scale [-1,1] | Gray background: Logarithmic scale',\n",
        "    #           ha='center', fontsize=10, style='italic')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(bottom=0.1)\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "save_path = os.path.join(experiment_folder, 'optimization_step')\n",
        "plot_wsg_vs_metric(\n",
        "    othello_rule_to_optimization_step,\n",
        "    finetuned_rule_to_result_file,\n",
        "    x_axis_name=\"Early-stop optimization step based on ground-truth labels\",\n",
        "    title=\"PGR vs. Optimzation step at which ground truth validation is minimized\",\n",
        "    save_path=save_path,\n",
        "    x_axis_log=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PP3lSuGyK_F3"
      },
      "outputs": [],
      "source": [
        "def load_board_accuracy_degradation_metric(\n",
        "    finetuned_rule_to_result_file_linear_probe: dict,\n",
        "    pretrained_rule_to_result_file_linear_probe: dict\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Computes degradation metric: pretrained_accuracy - finetuned_accuracy\n",
        "\n",
        "    Returns:\n",
        "        dict mapping OthelloRule -> ModelSize (strong) -> ModelSize (weak) -> degradation\n",
        "    \"\"\"\n",
        "    import pickle\n",
        "\n",
        "    def load_pickle(file_path):\n",
        "        try:\n",
        "            with open(file_path, 'rb') as f:\n",
        "                return pickle.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    # Load pretrained accuracies\n",
        "    pretrained_accuracies = {}\n",
        "    for rule, path in pretrained_rule_to_result_file_linear_probe.items():\n",
        "        results = load_pickle(path)\n",
        "        if results:\n",
        "            pretrained_accuracies[rule] = {}\n",
        "            for res in results:\n",
        "                if res['othello_rule'] == rule:\n",
        "                    pretrained_accuracies[rule][res['model_size']] = res['mean_accuracy']\n",
        "\n",
        "    # Load finetuned accuracies and compute degradation\n",
        "    othello_rule_to_metric = {}\n",
        "\n",
        "    for rule, path in finetuned_rule_to_result_file_linear_probe.items():\n",
        "        finetuned_results = load_pickle(path)\n",
        "        if finetuned_results and rule in pretrained_accuracies:\n",
        "            othello_rule_to_metric[rule] = {}\n",
        "\n",
        "            for (weak_size, strong_size), finetuned_accuracy in finetuned_results.items():\n",
        "                # Get pretrained accuracy for strong model\n",
        "                if strong_size in pretrained_accuracies[rule]:\n",
        "                    pretrained_accuracy = pretrained_accuracies[rule][strong_size]\n",
        "                    degradation = finetuned_accuracy - pretrained_accuracy\n",
        "\n",
        "                    if strong_size not in othello_rule_to_metric[rule]:\n",
        "                        othello_rule_to_metric[rule][strong_size] = {}\n",
        "                    othello_rule_to_metric[rule][strong_size][weak_size] = degradation\n",
        "\n",
        "    return othello_rule_to_metric\n",
        "\n",
        "\n",
        "# Generate the degradation metric\n",
        "othello_rule_to_degradation = load_board_accuracy_degradation_metric(\n",
        "    finetued_rule_to_result_file_linear_probe,  # First parameter should be linear probe\n",
        "    pretrained_rule_to_result_file_linear_probe,\n",
        ")\n",
        "\n",
        "# Create the plot\n",
        "save_path = os.path.join(experiment_folder, 'board_accuracy_degradation')\n",
        "plot_wsg_vs_metric(\n",
        "    othello_rule_to_degradation,\n",
        "    finetuned_rule_to_result_file,  # Second parameter should be WSG results\n",
        "    x_axis_name=\"Change in Linear Probe accuracy [Acc(Finetuned) - Acc(Pretrained)]\",\n",
        "    title=\"WSG vs Change in Representations during Finetuning\",\n",
        "    save_path=save_path\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deXRIAsvteuL"
      },
      "outputs": [],
      "source": [
        "from typing import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_5qEvxrXcVY"
      },
      "source": [
        "# --- Random ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzjTH5YWF5Ss"
      },
      "outputs": [],
      "source": [
        "import torch as t\n",
        "import numpy as np\n",
        "\n",
        "# Get example game and extract moves 10-11\n",
        "example_game = [to_square(i) for i in train_dataset[0][0].tolist()]\n",
        "game_ids = to_id(example_game)\n",
        "\n",
        "# Get board states for moves 10-11\n",
        "board_states = t.zeros((2, 8, 8), dtype=t.int32)\n",
        "board = OthelloBoardState()\n",
        "for i, token_id in enumerate(game_ids[:12]):  # Up to move 11\n",
        "    board.umpire(id_to_square(token_id))\n",
        "    if i >= 10:  # Save moves 10-11\n",
        "        board_states[i-10] = t.from_numpy(board.state)\n",
        "\n",
        "# Create 4 different basis transformations\n",
        "basis_states = t.zeros((2, 4, 8, 8), dtype=t.int32)\n",
        "\n",
        "for move_idx in range(2):\n",
        "    state = board_states[move_idx]\n",
        "    current_player = -1 if (10 + move_idx) % 2 == 0 else 1  # Black starts\n",
        "\n",
        "    # Basis 1: empty/mine/yours\n",
        "    mine_mask = (state == current_player)\n",
        "    yours_mask = (state == -current_player)\n",
        "    basis_states[move_idx, 0] = mine_mask.long() + 2 * yours_mask.long()\n",
        "\n",
        "    # Basis 2: empty/white/black\n",
        "    white_mask = (state == 1)\n",
        "    black_mask = (state == -1)\n",
        "    basis_states[move_idx, 1] = white_mask.long() + 2 * black_mask.long()\n",
        "\n",
        "    # Basis 3: empty/filled\n",
        "    basis_states[move_idx, 2] = (state != 0).long()\n",
        "\n",
        "    # Basis 4: non-linear transformation\n",
        "    transform = FakeBoardStateTransformModulo()\n",
        "    fake_state = transform(state.unsqueeze(0).unsqueeze(0)).squeeze()\n",
        "    basis_states[move_idx, 3] = fake_state\n",
        "\n",
        "# Flatten for plotting (2*4 = 8 boards)\n",
        "all_states = basis_states.view(8, 8, 8)\n",
        "titles = []\n",
        "for move in [10, 11]:\n",
        "    titles.extend([f\"Move {move}: Mine/Yours\", f\"Move {move}: White/Black\",\n",
        "                   f\"Move {move}: Empty/Filled\", f\"Move {move}: Non-linear\"])\n",
        "\n",
        "# Plot directly with plotly for 0,1,2 values\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Create subplot grid\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=4,\n",
        "    subplot_titles=titles,\n",
        "    vertical_spacing=0.1,\n",
        "    horizontal_spacing=0.05\n",
        ")\n",
        "\n",
        "# Custom colorscale for 0=white, 1=grey, 2=black\n",
        "colorscale = [[0, 'grey'], [0.5, 'white'], [1, 'black']]\n",
        "\n",
        "for i in range(8):\n",
        "    row = i // 4 + 1\n",
        "    col = i % 4 + 1\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Heatmap(\n",
        "            z=all_states[i].numpy(),\n",
        "            colorscale=colorscale,\n",
        "            zmin=0, zmax=2,\n",
        "            showscale=False,\n",
        "            xaxis=f'x{i+1}',\n",
        "            yaxis=f'y{i+1}'\n",
        "        ),\n",
        "        row=row, col=col\n",
        "    )\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(height=600, width=1200)\n",
        "fig.update_xaxes(tickvals=list(range(8)), ticktext=[str(i) for i in range(8)])\n",
        "fig.update_yaxes(tickvals=list(range(8)), ticktext=list(\"ABCDEFGH\"))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvHlrr_wC3d-"
      },
      "outputs": [],
      "source": [
        "def calculate_leakage_by_train_games(\n",
        "    train_data: \"CharDataset\",\n",
        "    test_data: \"CharDataset\",\n",
        "    train_subset_size: int = 10000,\n",
        "    batch_size: int = 10000,\n",
        "    ordered_moves: bool = True,\n",
        ") -> int:\n",
        "    if len(train_data.data) > train_subset_size:\n",
        "        train_subset = random.sample(train_data.data, train_subset_size)\n",
        "    else:\n",
        "        train_subset = train_data.data\n",
        "    train_games_to_check = {tuple(game) for game in train_subset}\n",
        "\n",
        "    leaked_count = 0\n",
        "    num_test_batches = (len(test_data.data) + batch_size - 1) // batch_size\n",
        "    test_iterator = tqdm(range(num_test_batches), desc=\"🔍 Checking test batches\")\n",
        "\n",
        "    for i in test_iterator:\n",
        "        if not train_games_to_check:\n",
        "            break\n",
        "\n",
        "        start_idx = i * batch_size\n",
        "        test_batch = test_data.data[start_idx : start_idx + batch_size]\n",
        "        current_test_prefixes = {\n",
        "            tuple(game[:l]) if ordered_moves else tuple(sorted(game[:l]))\n",
        "            for game in test_batch\n",
        "            for l in range(1, len(game) + 1)\n",
        "        }\n",
        "\n",
        "        found_in_batch = set()\n",
        "        for train_game in train_games_to_check:\n",
        "            for l in range(1, len(train_game) + 1):\n",
        "                train_prefix = tuple(train_game[:l]) if ordered_moves else tuple(sorted(train_game[:l]))\n",
        "                if train_prefix in current_test_prefixes:\n",
        "                    found_in_batch.add(train_game)\n",
        "                    break\n",
        "\n",
        "        if found_in_batch:\n",
        "            leaked_count += len(found_in_batch)\n",
        "            train_games_to_check -= found_in_batch\n",
        "\n",
        "        test_iterator.set_postfix({\n",
        "            \"Leaked\": leaked_count,\n",
        "            \"Remaining\": len(train_games_to_check)\n",
        "        })\n",
        "\n",
        "    return leaked_count\n",
        "\n",
        "leaked_game_count = calculate_leakage_by_train_games(\n",
        "        train_dataset,\n",
        "        test_dataset,\n",
        "        train_subset_size=10000,\n",
        "        batch_size=10000 # Small batch size for demonstration\n",
        "    )\n",
        "\n",
        "print(\"\\n--- Results ---\")\n",
        "print(f\"Total leaked training games found: {leaked_game_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Et2ytFZ_bwzZ"
      },
      "outputs": [],
      "source": [
        "print(len(train_dataset) / (100*512))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYTWm-HyEAHm"
      },
      "outputs": [],
      "source": [
        "def calculate_leakage_by_train_games(\n",
        "    train_data: \"CharDataset\",\n",
        "    test_data: \"CharDataset\",\n",
        "    train_subset_size: int = 10000,\n",
        "    batch_size: int = 10000,\n",
        "    ordered_moves: bool = True,\n",
        ") -> int:\n",
        "    train_games_to_check = random.sample(train_data.data, train_subset_size)\n",
        "    leaked_count = 0\n",
        "    num_test_batches = (len(test_data.data) + batch_size - 1) // batch_size\n",
        "    test_iterator = tqdm(range(num_test_batches), desc=\"🔍 Checking test batches\")\n",
        "\n",
        "    for i in test_iterator:\n",
        "        if not train_games_to_check:\n",
        "            break\n",
        "\n",
        "        start_idx = i * batch_size\n",
        "        test_batch = test_data.data[start_idx : start_idx + batch_size]\n",
        "        current_test_prefixes = {\n",
        "            tuple(game[:l]) if ordered_moves else tuple(sorted(game[:l]))\n",
        "            for game in test_batch\n",
        "            for l in range(1, len(game) + 1)\n",
        "        }\n",
        "\n",
        "        found_in_batch = set()\n",
        "        for train_game in train_games_to_check:\n",
        "            for l in range(1, len(train_game) + 1):\n",
        "                train_prefix = tuple(train_game[:l]) if ordered_moves else tuple(sorted(train_game[:l]))\n",
        "                if train_prefix in current_test_prefixes:\n",
        "                    found_in_batch.add(train_game)\n",
        "                    break\n",
        "\n",
        "        if found_in_batch:\n",
        "            leaked_count += len(found_in_batch)\n",
        "            train_games_to_check -= found_in_batch\n",
        "            if leaked_count % 10:\n",
        "                print(leaked_count)\n",
        "\n",
        "        test_iterator.set_postfix({\n",
        "            \"Leaked\": leaked_count,\n",
        "            \"Remaining\": len(train_games_to_check)\n",
        "        })\n",
        "\n",
        "    return leaked_count\n",
        "\n",
        "leaked_game_count = calculate_leakage_by_train_games(\n",
        "        train_dataset,\n",
        "        test_dataset,\n",
        "        train_subset_size=1,\n",
        "        batch_size=100000 # Small batch size for demonstration\n",
        "    )\n",
        "\n",
        "print(\"\\n--- Results ---\")\n",
        "print(f\"Total leaked training games found: {leaked_game_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coz4TQ87V4xc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KNmCQ_8oJVl"
      },
      "outputs": [],
      "source": [
        "hooked_model = convert_and_verify(trainer.model.module, mconf, val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cj5v_p4oFQbD"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "del hooked_model\n",
        "t.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neFWabEVFXJm"
      },
      "outputs": [],
      "source": [
        "# def load_model(checkpoint_path, model_config):\n",
        "#    model = GPT(model_config)\n",
        "#    model.load_state_dict(t.load(checkpoint_path, map_location='cpu'))\n",
        "#    model.to(DEVICE)\n",
        "#    return model\n",
        "\n",
        "# checkpoint_path = experiment_folder + 'TEST_MODEL_2025-06-24-22-48-39/ckpts/gpt_at_20250624_224839.ckpt'  # Self trained GPT\n",
        "# checkpoint_path = experiment_folder + 'TEST_MODEL_2025-06-28-15-20-17/ckpts/gpt_at_20250628_152017.ckpt'  # Biased, trained on smaller dataset\n",
        "# checkpoint_path = experiment_folder + 'TEST_MODEL_2025-06-28-15-02-06/ckpts/gpt_at_20250628_150206.ckpt'  # Hald trained standard model\n",
        "# model_2 = load_model(checkpoint_path, mconf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiF2VgZ3_IiN"
      },
      "outputs": [],
      "source": [
        "# hooked_model_2 = convert_and_verify(model_2, mconf, test_dataset)\n",
        "\n",
        "# results = test_hooked_transformer(hooked_model_2, test_dataset=val_dataset, batch_size=512, n_games=100, device=DEVICE)\n",
        "# print(\"\")\n",
        "# print(f\"Validation CE Loss: {results['ce_loss']:.6f}\")\n",
        "# print(f\"Illegal Move %: {results['illegal_move_percentage']:.6f}%\")\n",
        "# print(f\"Total Predictions: {results['total_predictions']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_gYnul8ja89"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "FZn51HV6BxlF",
        "FlBlZGS11HFE",
        "iJBpHN2keuz_",
        "YF367sxmh2xt",
        "bvRRT56NfBbB",
        "xRMwW4x7ieNT",
        "8unPxOD58V0W",
        "x7ExCwGRXzJh",
        "dSmryN0PM8H_",
        "FyqteaDW9ssL",
        "cuqz9jya8wGF",
        "nQl3qrr8SV5Z",
        "FOj-51o88zVd",
        "ZjTXE9OjZIrm",
        "agBm7yQHYXAp",
        "ThMvq0yOYnXU",
        "tc8OKY5ZhHqn",
        "y7iUk7uY8wcE",
        "vFKAJF7S7aGF",
        "Oyzic3pGk6qh",
        "Q7taVfY8YIIm",
        "s6KeGvQd-xri",
        "b8PxZcUe0zJL",
        "HdOSlYFi6-HI",
        "V820C1vGXVY1",
        "Uy2e79Ay0ops",
        "qU8sbUsfMa0H",
        "qD6EzWo5OmJf",
        "lIgr9l_8BS-p",
        "kqeD1NyB0KSs",
        "A9hvwWKsGku4",
        "L4L5Qx66M_1A",
        "B0nMvQJLJzf8",
        "l_5qEvxrXcVY"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}